{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alvaro/repos/pysuricata/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
              "                <p>\n",
              "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
              "                </p>\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ All imports loaded successfully!\n",
            "üìä Pandas version: 2.3.3\n",
            "üî¨ PySuricata version: pysuricata.api\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "ydata-profiling vs PySuricata Performance Comparison\n",
        "This notebook compares the performance and memory usage of:\n",
        "1. ydata-profiling (formerly pandas-profiling)\n",
        "2. PySuricata\n",
        "\n",
        "Using a 1M row dataset with comprehensive monitoring.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from ydata_profiling import ProfileReport\n",
        "from pysuricata import profile, ProfileConfig, ComputeOptions\n",
        "\n",
        "print(\"üì¶ All imports loaded successfully!\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üî¨ PySuricata version: {profile.__module__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ MemoryMonitor class ready!\n"
          ]
        }
      ],
      "source": [
        "# Lightweight Memory Monitoring Setup\n",
        "class MemoryMonitor:\n",
        "    \"\"\"Lightweight memory monitoring for profiling tool comparison.\n",
        "    \n",
        "    Note: Uses psutil only to avoid tracemalloc performance overhead.\n",
        "    tracemalloc.start() causes 6-7x slowdown in PySuricata due to \n",
        "    tracking every memory allocation during data processing.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.process = psutil.Process(os.getpid())\n",
        "        self.initial_memory = self.process.memory_info().rss / 1024 / 1024\n",
        "        self.memory_snapshots = []\n",
        "        self.start_time = time.perf_counter()\n",
        "        \n",
        "    def start_tracing(self):\n",
        "        \"\"\"Start lightweight memory monitoring.\"\"\"\n",
        "        print(f\"üîç Lightweight memory monitoring started\")\n",
        "        print(f\"üìä Initial memory: {self.initial_memory:.2f} MB\")\n",
        "        print(f\"‚ÑπÔ∏è  Using psutil-only monitoring (no tracemalloc overhead)\")\n",
        "        \n",
        "    def snapshot(self, step=None, description=\"\"):\n",
        "        \"\"\"Take a memory snapshot using psutil only.\"\"\"\n",
        "        current_memory = self.process.memory_info().rss / 1024 / 1024\n",
        "        \n",
        "        snapshot = {\n",
        "            'step': step,\n",
        "            'description': description,\n",
        "            'timestamp': time.perf_counter() - self.start_time,\n",
        "            'process_memory': current_memory,\n",
        "            'traced_memory': 0,  # Not available without tracemalloc\n",
        "            'peak_traced': 0,    # Not available without tracemalloc\n",
        "            'memory_growth': current_memory - self.initial_memory\n",
        "        }\n",
        "        \n",
        "        self.memory_snapshots.append(snapshot)\n",
        "        \n",
        "        print(f\"üìà {step}: {description}\")\n",
        "        print(f\"   Process memory: {current_memory:.2f} MB (+{snapshot['memory_growth']:.2f} MB)\")\n",
        "        \n",
        "        return snapshot\n",
        "        \n",
        "    def stop_tracing(self):\n",
        "        \"\"\"Stop memory monitoring and return summary.\"\"\"\n",
        "        final_memory = self.process.memory_info().rss / 1024 / 1024\n",
        "        total_time = time.perf_counter() - self.start_time\n",
        "        \n",
        "        print(f\"\\nüèÅ Memory Monitoring Summary:\")\n",
        "        print(f\"   Total time: {total_time:.2f} seconds\")\n",
        "        print(f\"   Final memory: {final_memory:.2f} MB\")\n",
        "        print(f\"   Total growth: {final_memory - self.initial_memory:.2f} MB\")\n",
        "        \n",
        "        if self.memory_snapshots:\n",
        "            max_growth = max(s['memory_growth'] for s in self.memory_snapshots)\n",
        "            print(f\"   Peak growth: {max_growth:.2f} MB\")\n",
        "            \n",
        "            if max_growth < 200:\n",
        "                print(\"   ‚úÖ Memory usage is bounded and efficient!\")\n",
        "            else:\n",
        "                print(\"   ‚ö†Ô∏è  High memory growth detected!\")\n",
        "        \n",
        "        return self.memory_snapshots\n",
        "\n",
        "print(\"‚úÖ Lightweight MemoryMonitor class ready!\")\n",
        "print(\"‚ÑπÔ∏è  Note: Using psutil-only monitoring to avoid tracemalloc performance overhead\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Loading dataset: 1M_rows_test_uncompressed.csv\n",
            "üìä File size: 142.14 MB\n",
            "‚úÖ Dataset loaded successfully!\n",
            "üìä Shape: 1,000,000 rows √ó 13 columns\n",
            "‚è±Ô∏è  Load time: 0.80 seconds\n",
            "üíæ Memory usage: 368.82 MB\n",
            "\n",
            "üìã Column Types:\n",
            "   object: 6 columns\n",
            "   float64: 4 columns\n",
            "   int64: 3 columns\n",
            "\n",
            "üéØ Ready for profiling comparison!\n",
            "   Dataset: 1,000,000 rows √ó 13 columns\n",
            "   Memory footprint: 368.82 MB\n"
          ]
        }
      ],
      "source": [
        "# Data Loading and Preparation\n",
        "def load_dataset():\n",
        "    \"\"\"Load the 1M row dataset for comparison.\"\"\"\n",
        "    \n",
        "    csv_path = Path(\"1M_rows_test_uncompressed.csv\")\n",
        "    \n",
        "    if not csv_path.exists():\n",
        "        print(\"‚ùå CSV file not found: 1M_rows_test_uncompressed.csv\")\n",
        "        print(\"   Please ensure the file exists in the examples directory\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üìÅ Loading dataset: {csv_path.name}\")\n",
        "    print(f\"üìä File size: {csv_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    \n",
        "    # Load with pandas\n",
        "    start_time = time.perf_counter()\n",
        "    df = pd.read_csv(csv_path)\n",
        "    load_time = time.perf_counter() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\")\n",
        "    print(f\"‚è±Ô∏è  Load time: {load_time:.2f} seconds\")\n",
        "    print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"\\nüìã Column Types:\")\n",
        "    dtype_counts = df.dtypes.value_counts()\n",
        "    for dtype, count in dtype_counts.items():\n",
        "        print(f\"   {dtype}: {count} columns\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the dataset\n",
        "df = load_dataset()\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"\\nüéØ Ready for profiling comparison!\")\n",
        "    print(f\"   Dataset: {df.shape[0]:,} rows √ó {df.shape[1]:,} columns\")\n",
        "    print(f\"   Memory footprint: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Starting ydata-profiling benchmark...\n",
            "==================================================\n",
            "üîç Memory monitoring started\n",
            "üìä Initial memory: 853.61 MB\n",
            "üìà Start: Initial state\n",
            "   Process memory: 853.61 MB (+0.00 MB)\n",
            "   Traced memory: 0.00 MB (peak: 0.00 MB)\n",
            "‚öôÔ∏è  Configuring ydata-profiling for full analysis...\n",
            "üìà Config: Configuration complete\n",
            "   Process memory: 853.61 MB (+0.00 MB)\n",
            "   Traced memory: 0.00 MB (peak: 0.00 MB)\n",
            "üìä Generating comprehensive ydata-profiling report...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Summarize dataset:  28%|‚ñà‚ñà‚ñä       | 5/18 [00:15<00:30,  2.32s/it, Describe variable: int_col]      /Users/alvaro/repos/pysuricata/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
            "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
            "/Users/alvaro/repos/pysuricata/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:173: RuntimeWarning: invalid value encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
            "Summarize dataset:  39%|‚ñà‚ñà‚ñà‚ñâ      | 7/18 [00:22<00:30,  2.81s/it, Describe variable: int_col]/Users/alvaro/repos/pysuricata/.venv/lib/python3.13/site-packages/numpy/lib/_function_base_impl.py:4620: RuntimeWarning: invalid value encountered in subtract\n",
            "  diff_b_a = subtract(b, a)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.49s/it]2<00:10,  1.25s/it, Describe variable: int_col]\n",
            "Summarize dataset:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [00:39<00:09,  1.96s/it, Calculate auto correlation]    /Users/alvaro/repos/pysuricata/.venv/lib/python3.13/site-packages/ydata_profiling/model/correlations.py:87: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
            "To hide this warning, disable the calculation\n",
            "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
            "If this is problematic for your use case, please report this as an issue:\n",
            "https://github.com/ydataai/ydata-profiling/issues\n",
            "(include the error message: 'cannot specify integer `bins` when input data contains infinity')\n",
            "  warnings.warn(\n",
            "Summarize dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 77/77 [00:51<00:00,  1.50it/s, Completed]                           \n",
            "Generate report structure: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Processing: Report generation in progress\n",
            "   Process memory: 2220.58 MB (+1366.97 MB)\n",
            "   Traced memory: 407.40 MB (peak: 1077.70 MB)\n",
            "üíæ Saving HTML report...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Render HTML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Export report to file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 359.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Complete: Report generation complete\n",
            "   Process memory: 2186.44 MB (+1332.83 MB)\n",
            "   Traced memory: 416.32 MB (peak: 1077.70 MB)\n",
            "\n",
            "üèÅ Memory Monitoring Summary:\n",
            "   Total time: 58.31 seconds\n",
            "   Final memory: 2186.78 MB\n",
            "   Total growth: 1333.17 MB\n",
            "   Peak growth: 1366.97 MB\n",
            "   ‚ö†Ô∏è  High memory growth detected!\n",
            "\n",
            "üìä ydata-profiling Results:\n",
            "   Processing time: 58.29 seconds\n",
            "   Report size: 5.54 MB\n",
            "   Memory growth: 1332.83 MB\n",
            "   Peak memory: 1366.97 MB\n",
            "   Processing speed: 17,154 rows/second\n",
            "\n",
            "‚úÖ ydata-profiling report saved: ydata_profiling_report_1M.html\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ydata-profiling Benchmark\n",
        "def benchmark_ydata_profiling(df):\n",
        "    \"\"\"Benchmark ydata-profiling with comprehensive analysis.\"\"\"\n",
        "    \n",
        "    if df is None:\n",
        "        print(\"‚ùå No dataset available for profiling\")\n",
        "        return None\n",
        "    \n",
        "    print(\"üî¨ Starting ydata-profiling benchmark...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize memory monitor\n",
        "    monitor = MemoryMonitor()\n",
        "    monitor.start_tracing()\n",
        "    monitor.snapshot(\"Start\", \"Initial state\")\n",
        "    \n",
        "    try:\n",
        "        # Configure ydata-profiling for comprehensive analysis\n",
        "        print(\"‚öôÔ∏è  Configuring ydata-profiling for full analysis...\")\n",
        "        \n",
        "        # Start profiling with comprehensive settings\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        monitor.snapshot(\"Config\", \"Configuration complete\")\n",
        "        \n",
        "        # Generate comprehensive report\n",
        "        print(\"üìä Generating comprehensive ydata-profiling report...\")\n",
        "        report = ProfileReport(\n",
        "            df,\n",
        "            title=\"ydata-profiling Report - 1M Rows\",\n",
        "            minimal=False,  # Full analysis, not minimal\n",
        "            explorative=True,  # Enable explorative features\n",
        "            progress_bar=True,  # Show progress\n",
        "            lazy=False,  # Process immediately\n",
        "            # Enable all analysis features\n",
        "            correlations={\n",
        "                \"pearson\": {\"calculate\": True},\n",
        "                \"spearman\": {\"calculate\": True},\n",
        "                \"kendall\": {\"calculate\": True},\n",
        "                \"phi_k\": {\"calculate\": True},\n",
        "                \"cramers\": {\"calculate\": True}\n",
        "            },\n",
        "            interactions={\n",
        "                \"continuous\": True,\n",
        "                \"targets\": [],\n",
        "                \"continuous_columns\": []\n",
        "            },\n",
        "            missing_diagrams={\n",
        "                \"bar\": True,\n",
        "                \"matrix\": True,\n",
        "                \"heatmap\": True,\n",
        "                \"dendrogram\": True\n",
        "            },\n",
        "            duplicates={\n",
        "                \"head\": 10\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        monitor.snapshot(\"Processing\", \"Report generation in progress\")\n",
        "        \n",
        "        # Generate HTML report\n",
        "        print(\"üíæ Saving HTML report...\")\n",
        "        html_file = \"ydata_profiling_report_1M.html\"\n",
        "        report.to_file(html_file)\n",
        "        \n",
        "        end_time = time.perf_counter()\n",
        "        processing_time = end_time - start_time\n",
        "        \n",
        "        monitor.snapshot(\"Complete\", \"Report generation complete\")\n",
        "        \n",
        "        # Get final memory stats\n",
        "        memory_snapshots = monitor.stop_tracing()\n",
        "        \n",
        "        # Get report file size\n",
        "        report_size = Path(html_file).stat().st_size / 1024 / 1024\n",
        "        \n",
        "        print(f\"\\nüìä ydata-profiling Results:\")\n",
        "        print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
        "        print(f\"   Report size: {report_size:.2f} MB\")\n",
        "        print(f\"   Memory growth: {memory_snapshots[-1]['memory_growth']:.2f} MB\")\n",
        "        print(f\"   Peak memory: {max(s['memory_growth'] for s in memory_snapshots):.2f} MB\")\n",
        "        \n",
        "        if processing_time > 0:\n",
        "            rows_per_second = len(df) / processing_time\n",
        "            print(f\"   Processing speed: {rows_per_second:,.0f} rows/second\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ ydata-profiling report saved: {html_file}\")\n",
        "        \n",
        "        return {\n",
        "            'tool': 'ydata-profiling',\n",
        "            'processing_time': processing_time,\n",
        "            'report_size': report_size,\n",
        "            'memory_growth': memory_snapshots[-1]['memory_growth'],\n",
        "            'peak_memory': max(s['memory_growth'] for s in memory_snapshots),\n",
        "            'memory_snapshots': memory_snapshots,\n",
        "            'report': report,\n",
        "            'html_file': html_file\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during ydata-profiling: {e}\")\n",
        "        print(f\"   Error type: {type(e).__name__}\")\n",
        "        monitor.stop_tracing()\n",
        "        raise\n",
        "\n",
        "# Run ydata-profiling benchmark\n",
        "ydata_results = benchmark_ydata_profiling(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üî¨ Starting PySuricata benchmark...\n",
            "==================================================\n",
            "üîç Memory monitoring started\n",
            "üìä Initial memory: 2187.83 MB\n",
            "üìà Start: Initial state\n",
            "   Process memory: 2187.83 MB (+0.00 MB)\n",
            "   Traced memory: 0.00 MB (peak: 0.00 MB)\n",
            "‚öôÔ∏è  Configuring PySuricata...\n",
            "üìà Config: Configuration complete\n",
            "   Process memory: 2187.97 MB (+0.14 MB)\n",
            "   Traced memory: 0.00 MB (peak: 0.00 MB)\n",
            "üìä Generating PySuricata report...\n"
          ]
        }
      ],
      "source": [
        "# PySuricata Benchmark\n",
        "def benchmark_pysuricata(df):\n",
        "    \"\"\"Benchmark PySuricata with comprehensive analysis.\"\"\"\n",
        "    \n",
        "    if df is None:\n",
        "        print(\"‚ùå No dataset available for profiling\")\n",
        "        return None\n",
        "    \n",
        "    print(\"\\nüî¨ Starting PySuricata benchmark...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize memory monitor\n",
        "    monitor = MemoryMonitor()\n",
        "    monitor.start_tracing()\n",
        "    monitor.snapshot(\"Start\", \"Initial state\")\n",
        "    \n",
        "    try:\n",
        "        # Configure PySuricata for comprehensive analysis\n",
        "        print(\"‚öôÔ∏è  Configuring PySuricata...\")\n",
        "        \n",
        "        compute_options = ComputeOptions(\n",
        "            chunk_size=50_000,  # Process in smaller chunks for 1M rows\n",
        "            numeric_sample_size=5_000,  # Smaller sample size for 1M rows\n",
        "            max_uniques=1000,  # KMV sketch size\n",
        "            top_k=20,  # Top-k values to track\n",
        "            log_every_n_chunks=5,  # Log every 5 chunks\n",
        "            random_seed=42\n",
        "        )\n",
        "        \n",
        "        profile_config = ProfileConfig(compute=compute_options)\n",
        "        \n",
        "        monitor.snapshot(\"Config\", \"Configuration complete\")\n",
        "        \n",
        "        # Start profiling\n",
        "        start_time = time.perf_counter()\n",
        "        \n",
        "        print(\"üìä Generating PySuricata report...\")\n",
        "        report = profile(df, config=profile_config)\n",
        "        \n",
        "        monitor.snapshot(\"Processing\", \"Report generation in progress\")\n",
        "        \n",
        "        # Save HTML report\n",
        "        print(\"üíæ Saving HTML report...\")\n",
        "        html_file = \"pysuricata_report_1M.html\"\n",
        "        report.save_html(html_file)\n",
        "        \n",
        "        end_time = time.perf_counter()\n",
        "        processing_time = end_time - start_time\n",
        "        \n",
        "        monitor.snapshot(\"Complete\", \"Report generation complete\")\n",
        "        \n",
        "        # Get final memory stats\n",
        "        memory_snapshots = monitor.stop_tracing()\n",
        "        \n",
        "        # Get report file size\n",
        "        report_size = Path(html_file).stat().st_size / 1024 / 1024\n",
        "        \n",
        "        print(f\"\\nüìä PySuricata Results:\")\n",
        "        print(f\"   Processing time: {processing_time:.2f} seconds\")\n",
        "        print(f\"   Report size: {report_size:.2f} MB\")\n",
        "        print(f\"   Memory growth: {memory_snapshots[-1]['memory_growth']:.2f} MB\")\n",
        "        print(f\"   Peak memory: {max(s['memory_growth'] for s in memory_snapshots):.2f} MB\")\n",
        "        \n",
        "        if processing_time > 0:\n",
        "            rows_per_second = len(df) / processing_time\n",
        "            print(f\"   Processing speed: {rows_per_second:,.0f} rows/second\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ PySuricata report saved: {html_file}\")\n",
        "        \n",
        "        return {\n",
        "            'tool': 'pysuricata',\n",
        "            'processing_time': processing_time,\n",
        "            'report_size': report_size,\n",
        "            'memory_growth': memory_snapshots[-1]['memory_growth'],\n",
        "            'peak_memory': max(s['memory_growth'] for s in memory_snapshots),\n",
        "            'memory_snapshots': memory_snapshots,\n",
        "            'report': report,\n",
        "            'html_file': html_file\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during PySuricata profiling: {e}\")\n",
        "        print(f\"   Error type: {type(e).__name__}\")\n",
        "        monitor.stop_tracing()\n",
        "        raise\n",
        "\n",
        "# Run PySuricata benchmark\n",
        "pysuricata_results = benchmark_pysuricata(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üèÜ PERFORMANCE COMPARISON\n",
            "============================================================\n",
            "\n",
            "üìä Side-by-Side Comparison:\n",
            "                     Metric ydata-profiling PySuricata\n",
            "  Processing Time (seconds)           60.71     349.31\n",
            "           Report Size (MB)            5.54       1.35\n",
            "         Memory Growth (MB)         1462.70   -1132.16\n",
            "           Peak Memory (MB)         1462.70       0.02\n",
            "Processing Speed (rows/sec)          16,473      2,863\n",
            "\n",
            "‚ö° Performance Ratios:\n",
            "   Time ratio (ydata/pysuricata): 0.17x\n",
            "   Memory ratio (ydata/pysuricata): -1.29x\n",
            "   Size ratio (ydata/pysuricata): 4.09x\n",
            "\n",
            "üèÖ Winners:\n",
            "   ‚ö° Speed: ydata-profiling (60.71s vs 349.31s)\n",
            "   üíæ Memory: PySuricata (-1132.16MB vs 1462.70MB)\n",
            "   üìÑ Size: PySuricata (1.35MB vs 5.54MB)\n"
          ]
        }
      ],
      "source": [
        "# Performance Comparison\n",
        "def compare_performance(ydata_results, pysuricata_results):\n",
        "    \"\"\"Compare performance metrics between the two tools.\"\"\"\n",
        "    \n",
        "    if ydata_results is None or pysuricata_results is None:\n",
        "        print(\"‚ùå Cannot compare - one or both benchmarks failed\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\nüèÜ PERFORMANCE COMPARISON\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create comparison table\n",
        "    comparison_data = {\n",
        "        'Metric': [\n",
        "            'Processing Time (seconds)',\n",
        "            'Report Size (MB)',\n",
        "            'Memory Growth (MB)',\n",
        "            'Peak Memory (MB)',\n",
        "            'Processing Speed (rows/sec)'\n",
        "        ],\n",
        "        'ydata-profiling': [\n",
        "            f\"{ydata_results['processing_time']:.2f}\",\n",
        "            f\"{ydata_results['report_size']:.2f}\",\n",
        "            f\"{ydata_results['memory_growth']:.2f}\",\n",
        "            f\"{ydata_results['peak_memory']:.2f}\",\n",
        "            f\"{len(df) / ydata_results['processing_time']:,.0f}\" if ydata_results['processing_time'] > 0 else \"N/A\"\n",
        "        ],\n",
        "        'PySuricata': [\n",
        "            f\"{pysuricata_results['processing_time']:.2f}\",\n",
        "            f\"{pysuricata_results['report_size']:.2f}\",\n",
        "            f\"{pysuricata_results['memory_growth']:.2f}\",\n",
        "            f\"{pysuricata_results['peak_memory']:.2f}\",\n",
        "            f\"{len(df) / pysuricata_results['processing_time']:,.0f}\" if pysuricata_results['processing_time'] > 0 else \"N/A\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"\\nüìä Side-by-Side Comparison:\")\n",
        "    print(comparison_df.to_string(index=False))\n",
        "    \n",
        "    # Calculate speedup/slowdown ratios\n",
        "    print(\"\\n‚ö° Performance Ratios:\")\n",
        "    \n",
        "    time_ratio = ydata_results['processing_time'] / pysuricata_results['processing_time']\n",
        "    memory_ratio = ydata_results['memory_growth'] / pysuricata_results['memory_growth']\n",
        "    size_ratio = ydata_results['report_size'] / pysuricata_results['report_size']\n",
        "    \n",
        "    print(f\"   Time ratio (ydata/pysuricata): {time_ratio:.2f}x\")\n",
        "    print(f\"   Memory ratio (ydata/pysuricata): {memory_ratio:.2f}x\")\n",
        "    print(f\"   Size ratio (ydata/pysuricata): {size_ratio:.2f}x\")\n",
        "    \n",
        "    # Determine winners\n",
        "    print(\"\\nüèÖ Winners:\")\n",
        "    \n",
        "    if ydata_results['processing_time'] < pysuricata_results['processing_time']:\n",
        "        print(f\"   ‚ö° Speed: ydata-profiling ({ydata_results['processing_time']:.2f}s vs {pysuricata_results['processing_time']:.2f}s)\")\n",
        "    else:\n",
        "        print(f\"   ‚ö° Speed: PySuricata ({pysuricata_results['processing_time']:.2f}s vs {ydata_results['processing_time']:.2f}s)\")\n",
        "    \n",
        "    if ydata_results['memory_growth'] < pysuricata_results['memory_growth']:\n",
        "        print(f\"   üíæ Memory: ydata-profiling ({ydata_results['memory_growth']:.2f}MB vs {pysuricata_results['memory_growth']:.2f}MB)\")\n",
        "    else:\n",
        "        print(f\"   üíæ Memory: PySuricata ({pysuricata_results['memory_growth']:.2f}MB vs {ydata_results['memory_growth']:.2f}MB)\")\n",
        "    \n",
        "    if ydata_results['report_size'] < pysuricata_results['report_size']:\n",
        "        print(f\"   üìÑ Size: ydata-profiling ({ydata_results['report_size']:.2f}MB vs {pysuricata_results['report_size']:.2f}MB)\")\n",
        "    else:\n",
        "        print(f\"   üìÑ Size: PySuricata ({pysuricata_results['report_size']:.2f}MB vs {ydata_results['report_size']:.2f}MB)\")\n",
        "    \n",
        "    return comparison_df\n",
        "\n",
        "# Run comparison\n",
        "comparison_df = compare_performance(ydata_results, pysuricata_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Creating memory usage visualization...\n",
            "‚úÖ Memory visualization created successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/sg/5k66qx793vb5czcbn73g84200000gn/T/ipykernel_61275/1490450988.py:100: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# Memory Usage Visualization\n",
        "def visualize_memory_usage(ydata_results, pysuricata_results):\n",
        "    \"\"\"Visualize memory usage patterns for both tools.\"\"\"\n",
        "    \n",
        "    if ydata_results is None or pysuricata_results is None:\n",
        "        print(\"‚ùå Cannot visualize - one or both benchmarks failed\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        print(\"\\nüìä Creating memory usage visualization...\")\n",
        "        \n",
        "        # Prepare data\n",
        "        ydata_snapshots = ydata_results['memory_snapshots']\n",
        "        pysuricata_snapshots = pysuricata_results['memory_snapshots']\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Plot 1: Memory growth over time\n",
        "        if len(ydata_snapshots) > 1:\n",
        "            ydata_times = [s['timestamp'] for s in ydata_snapshots]\n",
        "            ydata_growth = [s['memory_growth'] for s in ydata_snapshots]\n",
        "            ax1.plot(ydata_times, ydata_growth, 'b-o', label='ydata-profiling', linewidth=2, markersize=6)\n",
        "        \n",
        "        if len(pysuricata_snapshots) > 1:\n",
        "            pysuricata_times = [s['timestamp'] for s in pysuricata_snapshots]\n",
        "            pysuricata_growth = [s['memory_growth'] for s in pysuricata_snapshots]\n",
        "            ax1.plot(pysuricata_times, pysuricata_growth, 'r-o', label='PySuricata', linewidth=2, markersize=6)\n",
        "        \n",
        "        ax1.set_xlabel('Time (seconds)')\n",
        "        ax1.set_ylabel('Memory Growth (MB)')\n",
        "        ax1.set_title('Memory Growth Over Time')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 2: Process memory comparison\n",
        "        if len(ydata_snapshots) > 1:\n",
        "            ydata_process = [s['process_memory'] for s in ydata_snapshots]\n",
        "            ax2.plot(ydata_times, ydata_process, 'b-o', label='ydata-profiling', linewidth=2, markersize=6)\n",
        "        \n",
        "        if len(pysuricata_snapshots) > 1:\n",
        "            pysuricata_process = [s['process_memory'] for s in pysuricata_snapshots]\n",
        "            ax2.plot(pysuricata_times, pysuricata_process, 'r-o', label='PySuricata', linewidth=2, markersize=6)\n",
        "        \n",
        "        ax2.set_xlabel('Time (seconds)')\n",
        "        ax2.set_ylabel('Process Memory (MB)')\n",
        "        ax2.set_title('Process Memory Usage')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 3: Bar chart comparison\n",
        "        metrics = ['Processing Time', 'Memory Growth', 'Report Size']\n",
        "        ydata_values = [\n",
        "            ydata_results['processing_time'],\n",
        "            ydata_results['memory_growth'],\n",
        "            ydata_results['report_size']\n",
        "        ]\n",
        "        pysuricata_values = [\n",
        "            pysuricata_results['processing_time'],\n",
        "            pysuricata_results['memory_growth'],\n",
        "            pysuricata_results['report_size']\n",
        "        ]\n",
        "        \n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "        \n",
        "        ax3.bar(x - width/2, ydata_values, width, label='ydata-profiling', alpha=0.8)\n",
        "        ax3.bar(x + width/2, pysuricata_values, width, label='PySuricata', alpha=0.8)\n",
        "        \n",
        "        ax3.set_xlabel('Metrics')\n",
        "        ax3.set_ylabel('Values')\n",
        "        ax3.set_title('Performance Metrics Comparison')\n",
        "        ax3.set_xticks(x)\n",
        "        ax3.set_xticklabels(metrics, rotation=45)\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Plot 4: Speed comparison\n",
        "        tools = ['ydata-profiling', 'PySuricata']\n",
        "        speeds = [\n",
        "            len(df) / ydata_results['processing_time'] if ydata_results['processing_time'] > 0 else 0,\n",
        "            len(df) / pysuricata_results['processing_time'] if pysuricata_results['processing_time'] > 0 else 0\n",
        "        ]\n",
        "        \n",
        "        colors = ['blue', 'red']\n",
        "        bars = ax4.bar(tools, speeds, color=colors, alpha=0.7)\n",
        "        ax4.set_ylabel('Rows per Second')\n",
        "        ax4.set_title('Processing Speed Comparison')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bar, speed in zip(bars, speeds):\n",
        "            height = bar.get_height()\n",
        "            ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
        "                    f'{speed:,.0f}', ha='center', va='bottom')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"‚úÖ Memory visualization created successfully!\")\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"‚ùå Matplotlib not available - skipping visualization\")\n",
        "        print(\"   Install matplotlib to see memory usage graphs: pip install matplotlib\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating visualization: {e}\")\n",
        "\n",
        "# Create visualization\n",
        "visualize_memory_usage(ydata_results, pysuricata_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ BENCHMARK COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "üìä Dataset Processed:\n",
            "   Rows: 1,000,000\n",
            "   Columns: 13\n",
            "   Memory footprint: 368.82 MB\n",
            "\n",
            "üìÑ Generated Reports:\n",
            "   1. ydata-profiling: ydata_profiling_report_1M.html (5.54 MB)\n",
            "   2. PySuricata: pysuricata_report_1M.html (1.35 MB)\n",
            "\n",
            "‚ö° Performance Summary:\n",
            "   ydata-profiling: 60.71s, 1462.70MB growth\n",
            "   PySuricata: 349.31s, -1132.16MB growth\n",
            "\n",
            "üèÜ Overall Assessment:\n",
            "   ‚ö° Speed: ydata-profiling wins\n",
            "   üíæ Memory: PySuricata wins\n",
            "   üìÑ Size: PySuricata wins\n",
            "\n",
            "üéØ Final Score:\n",
            "   ydata-profiling: 1/3\n",
            "   PySuricata: 2/3\n",
            "\n",
            "üèÖ Winner: PySuricata!\n",
            "\n",
            "üåê To view the reports:\n",
            "   Open the HTML files in your browser:\n",
            "   ‚Ä¢ ydata_profiling_report_1M.html\n",
            "   ‚Ä¢ pysuricata_report_1M.html\n",
            "\n",
            "üìà Key Insights:\n",
            "   ‚Ä¢ Both tools successfully processed 1,000,000 rows\n",
            "   ‚Ä¢ Memory usage patterns differ significantly\n",
            "   ‚Ä¢ Report sizes vary based on analysis depth\n",
            "   ‚Ä¢ Choose based on your specific needs:\n",
            "     - ydata-profiling: Comprehensive analysis, larger reports\n",
            "     - PySuricata: Lightweight, memory-efficient\n"
          ]
        }
      ],
      "source": [
        "# Final Summary and Report Access\n",
        "def final_summary(ydata_results, pysuricata_results):\n",
        "    \"\"\"Provide final summary and report access information.\"\"\"\n",
        "    \n",
        "    if ydata_results is None or pysuricata_results is None:\n",
        "        print(\"‚ùå Cannot provide summary - one or both benchmarks failed\")\n",
        "        return\n",
        "    \n",
        "    print(\"\\nüéâ BENCHMARK COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Processed:\")\n",
        "    print(f\"   Rows: {len(df):,}\")\n",
        "    print(f\"   Columns: {len(df.columns):,}\")\n",
        "    print(f\"   Memory footprint: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nüìÑ Generated Reports:\")\n",
        "    print(f\"   1. ydata-profiling: {ydata_results['html_file']} ({ydata_results['report_size']:.2f} MB)\")\n",
        "    print(f\"   2. PySuricata: {pysuricata_results['html_file']} ({pysuricata_results['report_size']:.2f} MB)\")\n",
        "    \n",
        "    print(f\"\\n‚ö° Performance Summary:\")\n",
        "    print(f\"   ydata-profiling: {ydata_results['processing_time']:.2f}s, {ydata_results['memory_growth']:.2f}MB growth\")\n",
        "    print(f\"   PySuricata: {pysuricata_results['processing_time']:.2f}s, {pysuricata_results['memory_growth']:.2f}MB growth\")\n",
        "    \n",
        "    # Overall winner\n",
        "    print(f\"\\nüèÜ Overall Assessment:\")\n",
        "    \n",
        "    ydata_score = 0\n",
        "    pysuricata_score = 0\n",
        "    \n",
        "    # Speed comparison\n",
        "    if ydata_results['processing_time'] < pysuricata_results['processing_time']:\n",
        "        ydata_score += 1\n",
        "        print(f\"   ‚ö° Speed: ydata-profiling wins\")\n",
        "    else:\n",
        "        pysuricata_score += 1\n",
        "        print(f\"   ‚ö° Speed: PySuricata wins\")\n",
        "    \n",
        "    # Memory comparison\n",
        "    if ydata_results['memory_growth'] < pysuricata_results['memory_growth']:\n",
        "        ydata_score += 1\n",
        "        print(f\"   üíæ Memory: ydata-profiling wins\")\n",
        "    else:\n",
        "        pysuricata_score += 1\n",
        "        print(f\"   üíæ Memory: PySuricata wins\")\n",
        "    \n",
        "    # Size comparison\n",
        "    if ydata_results['report_size'] < pysuricata_results['report_size']:\n",
        "        ydata_score += 1\n",
        "        print(f\"   üìÑ Size: ydata-profiling wins\")\n",
        "    else:\n",
        "        pysuricata_score += 1\n",
        "        print(f\"   üìÑ Size: PySuricata wins\")\n",
        "    \n",
        "    print(f\"\\nüéØ Final Score:\")\n",
        "    print(f\"   ydata-profiling: {ydata_score}/3\")\n",
        "    print(f\"   PySuricata: {pysuricata_score}/3\")\n",
        "    \n",
        "    if ydata_score > pysuricata_score:\n",
        "        print(f\"\\nüèÖ Winner: ydata-profiling!\")\n",
        "    elif pysuricata_score > ydata_score:\n",
        "        print(f\"\\nüèÖ Winner: PySuricata!\")\n",
        "    else:\n",
        "        print(f\"\\nü§ù Tie! Both tools have their strengths.\")\n",
        "    \n",
        "    print(f\"\\nüåê To view the reports:\")\n",
        "    print(f\"   Open the HTML files in your browser:\")\n",
        "    print(f\"   ‚Ä¢ {ydata_results['html_file']}\")\n",
        "    print(f\"   ‚Ä¢ {pysuricata_results['html_file']}\")\n",
        "    \n",
        "    print(f\"\\nüìà Key Insights:\")\n",
        "    print(f\"   ‚Ä¢ Both tools successfully processed {len(df):,} rows\")\n",
        "    print(f\"   ‚Ä¢ Memory usage patterns differ significantly\")\n",
        "    print(f\"   ‚Ä¢ Report sizes vary based on analysis depth\")\n",
        "    print(f\"   ‚Ä¢ Choose based on your specific needs:\")\n",
        "    print(f\"     - ydata-profiling: Comprehensive analysis, larger reports\")\n",
        "    print(f\"     - PySuricata: Lightweight, memory-efficient\")\n",
        "\n",
        "# Display final summary\n",
        "final_summary(ydata_results, pysuricata_results)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
