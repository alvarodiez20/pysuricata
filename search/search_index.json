{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pysuricata","title":"PySuricata","text":"<p>Lightweight, high-performance exploratory data analysis for Python.</p> <p>Generate comprehensive, self-contained HTML reports for pandas and polars DataFrames using proven streaming algorithms that work with datasets of any size.</p> <ul> <li> <p> Get Started Fast</p> <p>Install and create your first report in 60 seconds.</p> <p> Quick Start</p> </li> <li> <p> Why PySuricata?</p> <p>Learn how PySuricata compares to alternatives.</p> <p> Competitive Advantages</p> </li> <li> <p> User Guide</p> <p>Comprehensive guides for all use cases.</p> <p> Usage Documentation</p> </li> <li> <p> API Reference</p> <p>Complete API documentation with examples.</p> <p> API Docs</p> </li> </ul>"},{"location":"#features","title":"Features","text":"Memory EfficientFast &amp; AccurateFramework FlexiblePortable Reports <p>True streaming architecture processes datasets larger than RAM with bounded memory using O(1) or O(log n) algorithms per column.</p> <pre><code># Profile 10GB dataset in 50MB memory\ndef read_chunks():\n    for i in range(100):\n        yield pd.read_parquet(f\"part-{i}.parquet\")\n\nreport = profile(read_chunks())\n</code></pre> <p>Proven algorithms with mathematical guarantees: Welford/P\u00e9bay for exact moments, KMV/Misra-Gries for approximate analytics.</p> <p>15x faster than pandas-profiling on 1GB datasets.</p> <p>Native support for pandas and polars DataFrames. Works with CSV, Parquet, SQL, or any data source.</p> <pre><code># Pandas\nreport = profile(pd_df)\n\n# Polars\nreport = profile(pl_df)\n\n# Polars Lazy\nreport = profile(lf)\n</code></pre> <p>Self-contained HTML with inline CSS/JS/images. Share via email, cloud storage, or static hosting. No dependencies required.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load data\ndf = pd.read_csv(\"data.csv\")\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <p>That's it! Open <code>report.html</code> to see:</p> <ul> <li>Dataset overview (rows, columns, memory, missing, duplicates)</li> <li>Per-variable analysis (numeric, categorical, datetime, boolean)</li> <li>Correlations between numeric variables  </li> <li>Missing values patterns</li> <li>Data quality metrics</li> </ul>"},{"location":"#what-youll-find-in-reports","title":"What You'll Find in Reports","text":""},{"location":"#numeric-variables","title":"Numeric Variables","text":"<ul> <li>Central tendency: mean, median</li> <li>Dispersion: variance, std, IQR, MAD</li> <li>Shape: skewness, kurtosis</li> <li>Quantiles and histograms</li> <li>Outlier detection (IQR, z-score, MAD)</li> <li>Correlations with other columns</li> </ul>"},{"location":"#categorical-variables","title":"Categorical Variables","text":"<ul> <li>Top values and frequencies</li> <li>Distinct count (exact or approximate)</li> <li>Entropy and Gini impurity</li> <li>String length statistics</li> <li>Case/trim variants</li> </ul>"},{"location":"#datetime-variables","title":"DateTime Variables","text":"<ul> <li>Temporal range and coverage</li> <li>Hour/day-of-week/month distributions</li> <li>Monotonicity detection</li> <li>Timeline visualizations</li> </ul>"},{"location":"#boolean-variables","title":"Boolean Variables","text":"<ul> <li>True/false ratios</li> <li>Entropy calculation</li> <li>Imbalance detection</li> <li>Balance scores</li> </ul>"},{"location":"#key-advantages","title":"Key Advantages","text":"<p>\u2705 Memory efficient - Process GB/TB datasets in bounded memory \u2705 Fast - Single-pass O(n) algorithms \u2705 Accurate - Exact moments, provable approximation bounds \u2705 Portable - Self-contained HTML reports \u2705 Minimal dependencies - Just pandas/polars \u2705 Reproducible - Seeded sampling for deterministic results \u2705 Customizable - Extensive configuration options  </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pysuricata\n</code></pre> <p>Optional polars support:</p> <pre><code>pip install pysuricata polars\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>Never used PySuricata?</p> <p>Start with the Quick Start Guide</p> </li> <li> <p>Coming from pandas-profiling?</p> <p>See Why PySuricata?</p> </li> <li> <p>Need specific examples?</p> <p>Check the Examples Gallery</p> </li> <li> <p>Want to understand the math?</p> <p>Explore Statistical Methods</p> </li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Issue Tracker</li> <li>\u2b50 Star on GitHub</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! See the Contributing Guide to get started.</p>"},{"location":"#license","title":"License","text":"<p>MIT License. See LICENSE for details.</p> <p>Ready to profile your data? Install now \u2192</p>"},{"location":"about-suricatas/","title":"About Suricatas \ud83e\udda6","text":""},{"location":"about-suricatas/#meet-the-inspiration-behind-pysuricata","title":"Meet the Inspiration Behind PySuricata!","text":"<p>Suricatas (also known as meerkats) are small, incredibly intelligent mongoose species native to the Kalahari Desert in southern Africa. These fascinating creatures embody everything we aspire to achieve in data analysis software!</p> <p>A suricata standing guard - always vigilant, always analyzing!</p>"},{"location":"about-suricatas/#why-suricatas-are-perfect-for-data-analysis","title":"Why Suricatas Are Perfect for Data Analysis \ud83d\udd0d","text":""},{"location":"about-suricatas/#1-exceptional-watchfulness","title":"1. Exceptional Watchfulness \ud83d\udc40","text":"<p>Suricatas are famous for their sentinel behavior - they take turns standing on their hind legs, scanning the horizon for threats and opportunities. One member of the mob always stands guard while others forage.</p> <p>\ud83c\udfaf Connection to PySuricata: - Like a vigilant suricata, PySuricata constantly monitors your data - Scans every column for anomalies, patterns, and insights - Never misses a detail - from outliers to missing values to correlations - Always on lookout for data quality issues</p>"},{"location":"about-suricatas/#2-cooperative-intelligence","title":"2. Cooperative Intelligence \ud83e\udd1d","text":"<p>Suricatas live in highly organized groups called \"mobs\" or \"clans\" of 20-50 individuals. They work together seamlessly, each member contributing to the group's success.</p> <p>\ud83c\udfaf Connection to PySuricata: - Parallel processing: Like suricatas working together, PySuricata's accumulators can be merged - Distributed computing: Split data across multiple \"sentinels\" (workers) - Collaborative algorithms: Welford and P\u00e9bay's merge formulas enable teamwork - Streaming coordination: Process data in chunks, like suricatas coordinating foraging</p>"},{"location":"about-suricatas/#3-survival-in-harsh-environments","title":"3. Survival in Harsh Environments \ud83c\udfdc\ufe0f","text":"<p>Suricatas thrive in one of the world's harshest environments - the Kalahari Desert - where resources are scarce and conditions are extreme.</p> <p>\ud83c\udfaf Connection to PySuricata: - Memory efficiency: Survive with limited RAM, just like suricatas survive with limited water - Bounded resources: Process TB datasets with constant memory (like finding food in the desert) - Adaptive strategies: Streaming algorithms adapt to any dataset size - Resilient design: Handle missing values, outliers, and edge cases gracefully</p>"},{"location":"about-suricatas/#4-lightning-fast-reactions","title":"4. Lightning-Fast Reactions \u26a1","text":"<p>Suricatas can detect predators in milliseconds and alert the entire group instantly. Their reflexes are incredible!</p> <p>\ud83c\udfaf Connection to PySuricata: - Single-pass algorithms: Process data in O(n) time, no wasted effort - Streaming speed: Analyze data as it arrives, no waiting - Instant insights: Generate comprehensive reports in seconds - Quick detection: Find anomalies, correlations, and patterns immediately</p>"},{"location":"about-suricatas/#5-pattern-recognition-masters","title":"5. Pattern Recognition Masters \ud83e\udde0","text":"<p>Suricatas have exceptional memory and can recognize hundreds of individuals, remember danger zones, and identify safe foraging areas.</p> <p>\ud83c\udfaf Connection to PySuricata: - Pattern detection: Identify temporal patterns (hour/day/month distributions) - Anomaly recognition: Spot outliers using IQR, MAD, z-scores - Correlation discovery: Find relationships between variables - Smart memory: Remember important statistics while forgetting unnecessary details</p>"},{"location":"about-suricatas/#6-digging-for-hidden-treasures","title":"6. Digging for Hidden Treasures \ud83d\udd28","text":"<p>Suricatas spend hours digging through sand and soil to find insects, grubs, and other food sources. They're persistent excavators!</p> <p>\ud83c\udfaf Connection to PySuricata: - Deep analysis: Dig through your data to find hidden insights - Comprehensive profiling: Examine every variable type thoroughly - Missing value detection: Uncover data quality issues buried in your dataset - Statistical excavation: Extract mean, variance, skewness, kurtosis, and more</p>"},{"location":"about-suricatas/#7-teaching-and-learning","title":"7. Teaching and Learning \ud83d\udcda","text":"<p>Suricatas teach their young how to handle dangerous prey (like scorpions) by gradually introducing them to challenges. They're excellent educators!</p> <p>\ud83c\udfaf Connection to PySuricata: - Educational reports: Clear visualizations help users understand their data - Mathematical transparency: Every statistic includes the formula and interpretation - Progressive complexity: From simple counts to advanced algorithms - Documentation: Comprehensive guides teach users about statistical methods</p>"},{"location":"about-suricatas/#amazing-suricata-facts","title":"Amazing Suricata Facts! \ud83c\udf1f","text":""},{"location":"about-suricatas/#physical-characteristics","title":"Physical Characteristics","text":"<ul> <li>Size: 25-35 cm tall (about 1 foot)</li> <li>Weight: 600-900 grams (just over 1 pound)</li> <li>Speed: Can run up to 32 km/h (20 mph)</li> <li>Lifespan: 12-14 years in the wild</li> </ul>"},{"location":"about-suricatas/#incredible-abilities","title":"Incredible Abilities","text":"<ul> <li>360\u00b0 vision: Dark patches around eyes reduce glare (like sunglasses!)</li> <li>Immune to venom: Can survive scorpion and snake bites</li> <li>Complex communication: Use over 20 different vocalizations</li> <li>Social structure: Highly organized with defined roles</li> <li>Cooperative breeding: Entire mob helps raise pups</li> </ul>"},{"location":"about-suricatas/#behavioral-traits","title":"Behavioral Traits","text":"<ul> <li>Sentinel duty: Rotate guard positions every hour</li> <li>Teaching behavior: Actively instruct young on survival skills</li> <li>Playful nature: Engage in wrestling, chasing, and games</li> <li>Sun worship: Start each day basking in the sun \u2600\ufe0f</li> <li>Burrow architects: Build elaborate tunnel systems with multiple entrances</li> </ul>"},{"location":"about-suricatas/#the-perfect-mascot-for-data-analysis","title":"The Perfect Mascot for Data Analysis \ud83d\udcca","text":"<p>Just as suricatas stand tall on the African plains, scanning the horizon for important information, PySuricata stands ready to analyze your data, no matter how large or complex.</p>"},{"location":"about-suricatas/#suricata-principles-in-pysuricata","title":"Suricata Principles in PySuricata:","text":"Suricata Behavior PySuricata Feature \ud83d\udc40 Vigilant watching Comprehensive profiling of all columns \ud83e\udd1d Cooperative work Mergeable accumulators for parallel processing \ud83c\udfdc\ufe0f Desert survival Streaming algorithms for bounded memory \u26a1 Quick reactions Single-pass O(n) algorithms \ud83e\udde0 Pattern recognition Correlation detection, temporal analysis \ud83d\udd28 Persistent digging Deep statistical analysis \ud83c\udfaf Precision Mathematically proven algorithms \ud83d\udcda Teaching Educational documentation with formulas"},{"location":"about-suricatas/#join-the-mob","title":"Join the Mob! \ud83e\udda6\ud83e\udda6\ud83e\udda6","text":"<p>When you use PySuricata, you're joining a \"mob\" of data analysts who value: - Efficiency over waste - Cooperation over competition - Vigilance over negligence - Intelligence over brute force - Community over isolation</p>"},{"location":"about-suricatas/#fun-suricata-inspired-features","title":"Fun Suricata-Inspired Features","text":"<p>Throughout PySuricata, you'll notice references to these remarkable animals:</p> <ul> <li>\ud83e\udda6 Logo: Features a suricata standing watch</li> <li>\ud83c\udfdc\ufe0f Theme: Nature/green colors inspired by their habitat</li> <li>\ud83d\udcca Sentinel behavior: Always monitoring your data</li> <li>\ud83e\udd1d Mob mentality: Accumulators work together</li> <li>\u26a1 Quick reflexes: Fast streaming algorithms</li> </ul>"},{"location":"about-suricatas/#learn-more-about-suricatas","title":"Learn More About Suricatas","text":"<p>Fascinated by these amazing creatures? Learn more:</p> <ul> <li>Meerkat - Wikipedia</li> <li>Meerkat Manor - Documentary series</li> <li>San Diego Zoo: Meerkats</li> <li>National Geographic: Meerkats</li> </ul>"},{"location":"about-suricatas/#contributing-to-pysuricata","title":"Contributing to PySuricata","text":"<p>Want to help the PySuricata mob grow? Check out our Contributing Guide!</p> <p>Just like suricatas teaching their young, we welcome contributors who want to learn and share knowledge about: - Statistical algorithms - Streaming computation - Data analysis - Software engineering</p> <p>Remember: Every time you profile a dataset with PySuricata, you're channeling the vigilance, cooperation, and intelligence of a suricata mob! \ud83e\udda6\ud83d\udcca\u2728</p> <p>\"In the Kalahari of big data, be a suricata - vigilant, efficient, and always ready to dig for insights!\"</p> <p>Ready to analyze like a suricata?</p> Get Started \u2192"},{"location":"advanced/","title":"Advanced Features","text":"<p>Advanced techniques for power users.</p>"},{"location":"advanced/#custom-markdown-descriptions","title":"Custom Markdown Descriptions","text":"<p>Add rich descriptions to reports:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.render.description = \"\"\"\n# Q4 2024 Analysis\n\n**Dataset**: Customer transactions  \n**Period**: Oct-Dec 2024  \n**Source**: production.transactions\n\n## Key Findings\n\n- Revenue up 15% YoY\n- Average transaction: $87.50\n- Peak hour: 2pm EST\n\"\"\"\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"advanced/#streaming-from-multiple-sources","title":"Streaming from Multiple Sources","text":"<p>Combine data from multiple sources:</p> <pre><code>def multi_source_generator():\n    # Source 1: CSV files\n    for i in range(10):\n        yield pd.read_csv(f\"batch_{i}.csv\")\n\n    # Source 2: Parquet files\n    for i in range(5):\n        yield pd.read_parquet(f\"archive_{i}.parquet\")\n\n    # Source 3: Database\n    for chunk in pd.read_sql(\"SELECT * FROM logs\", conn, chunksize=100_000):\n        yield chunk\n\nreport = profile(multi_source_generator())\n</code></pre>"},{"location":"advanced/#parallel-processing-with-dask","title":"Parallel Processing with Dask","text":"<pre><code>import dask.dataframe as dd\nfrom pysuricata import profile\n\n# Load with Dask\nddf = dd.read_csv(\"large_*.csv\")\n\n# Convert to generator\ndef dask_generator():\n    for partition in ddf.partitions:\n        yield partition.compute()\n\nreport = profile(dask_generator())\n</code></pre>"},{"location":"advanced/#custom-sampling-strategy","title":"Custom Sampling Strategy","text":"<pre><code># Sample every Nth row for very large datasets\ndef sampled_generator(n=10):\n    for chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n        yield chunk[::n]  # Every 10th row\n\nreport = profile(sampled_generator())\n</code></pre>"},{"location":"advanced/#merging-accumulator-states-distributed","title":"Merging Accumulator States (Distributed)","text":"<pre><code>from pysuricata.accumulators import NumericAccumulator\n\n# Worker 1\nacc1 = NumericAccumulator(\"amount\")\nacc1.update(data_partition_1)\n\n# Worker 2\nacc2 = NumericAccumulator(\"amount\")\nacc2.update(data_partition_2)\n\n# Merge on coordinator\nacc1.merge(acc2)\nfinal_stats = acc1.finalize()\n</code></pre>"},{"location":"advanced/#conditional-profiling","title":"Conditional Profiling","text":"<p>Profile only rows meeting criteria:</p> <pre><code>def filtered_generator():\n    for chunk in pd.read_csv(\"data.csv\", chunksize=100_000):\n        # Only active users\n        yield chunk[chunk[\"status\"] == \"active\"]\n\nreport = profile(filtered_generator())\n</code></pre>"},{"location":"advanced/#see-also","title":"See Also","text":"<ul> <li>Configuration - All parameters</li> <li>Performance Tips - Optimization</li> <li>Examples - More use cases</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"api/","title":"High-level API","text":"<p>The unified API exposes two entry points that cover most workflows:</p> <ul> <li><code>profile(data, config=None) -&gt; Report</code>: compute + HTML</li> <li><code>summarize(data, config=None) -&gt; Mapping[str, Any]</code>: stats-only</li> </ul> <p>Import from the package root:</p> <pre><code>from pysuricata import profile, summarize, ReportConfig\n</code></pre>"},{"location":"api/#inputs","title":"Inputs","text":"<ul> <li>In-memory <code>pandas.DataFrame</code></li> <li><code>polars.DataFrame</code> or <code>LazyFrame</code></li> <li>Iterable/generator yielding pandas DataFrame chunks (you control chunking)</li> </ul>"},{"location":"api/#report-object","title":"Report object","text":"<pre><code>from pysuricata import profile, ReportConfig\n\nrep = profile(df, config=ReportConfig())\nrep.save_html(\"report.html\")\nrep.save_json(\"report.json\")\n\n# In notebooks, the report displays inline\nrep\n</code></pre>"},{"location":"api/#quick-render","title":"Quick render","text":"<pre><code>rep = profile(df)\nrep.save_html(\"report.html\")\n</code></pre>"},{"location":"api/#stats-only-cidata-quality","title":"Stats-only (CI/data-quality)","text":"<pre><code>stats = summarize(df)  # compute-only fast path (skips HTML)\n\n# Example: assert no column has &gt; 10% missing\nbad = [\n    (name, col[\"missing\"]) for name, col in stats[\"columns\"].items()\n    if col.get(\"missing\", 0) / max(1, col.get(\"count\", 0)) &gt; 0.10\n]\nassert not bad, f\"Columns too missing: {bad}\"\n</code></pre>"},{"location":"api/#configuration","title":"Configuration","text":"<p>The top-level <code>ReportConfig</code> wraps compute and render options:</p> <pre><code>from pysuricata import ReportConfig\n\ncfg = ReportConfig()\ncfg.compute.chunk_size = 250_000\ncfg.compute.columns = [\"a\", \"b\", \"c\"]\ncfg.compute.numeric_sample_size = 50_000\ncfg.compute.max_uniques = 4096\ncfg.compute.top_k = 100\ncfg.compute.random_seed = 42  # deterministic sampling\n\nrep = profile(df, config=cfg)\n</code></pre>"},{"location":"api/#load-and-chunk-outside","title":"Load and chunk outside","text":"<p>You can read data with any library and either pass a single DataFrame or an iterable of DataFrames you manage:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndef chunk_iter():\n    for i in range(10):\n        yield pd.read_parquet(f\"/data/part-{i}.parquet\")\n\nrep = profile((ch for ch in chunk_iter()))\n</code></pre>"},{"location":"api/#common-use-cases","title":"Common use cases","text":"<ul> <li> <p>Small DataFrame (in-memory):   <pre><code>import pandas as pd\nfrom pysuricata import profile\ndf = pd.DataFrame({\"x\": [1,2,3], \"y\": [\"a\",\"b\",\"a\"]})\nrep = profile(df)\n</code></pre></p> </li> <li> <p>Large dataset (streaming in-memory):   <pre><code>from pysuricata import ReportConfig, profile\ncfg = ReportConfig(); cfg.compute.chunk_size = 250_000\nrep = profile((ch for ch in chunk_iter()), config=cfg)\nrep.save_html(\"report.html\")\n</code></pre></p> </li> <li> <p>Column selection:   <pre><code>from pysuricata import ReportConfig, summarize\ncfg = ReportConfig()\ncfg.compute.columns = [\"id\", \"amount\", \"ts\"]\nstats = summarize(df[[\"id\", \"amount\", \"ts\"]])\n</code></pre></p> </li> <li> <p>CI check: enforce low duplicates and missingness:   <pre><code>stats = summarize(df)\nds = stats[\"dataset\"]\nassert ds[\"duplicate_rows_pct_est\"] &lt; 1.0\nassert ds[\"missing_cells_pct\"] &lt; 5.0\n</code></pre></p> </li> </ul>"},{"location":"api/#notes-and-limits","title":"Notes and limits","text":"<ul> <li>Current engine consumes pandas or polars DataFrames (or iterables of pandas frames). Polars eager/LazyFrames are processed natively.</li> <li>Render options are minimal; the HTML template is self-contained (light theme).</li> </ul>"},{"location":"api/#determinism","title":"Determinism","text":"<p>Set <code>cfg.compute.random_seed</code> to make reservoir sampling and other RNG use deterministic. This stabilizes histogram shapes in tests and CI.</p>"},{"location":"architecture/","title":"Architecture &amp; Internals","text":"<p>This document explains how <code>pysuricata</code> profiles data efficiently and renders a self\u2011contained HTML report.</p>"},{"location":"architecture/#overview","title":"Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Source \u2502 \u2192 \u2502 Chunk iterator\u2502 \u2192 \u2502 Typed accumulators \u2502 \u2192 \u2502 HTML template \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     In-memory DataFrame(s)          numeric / categorical / datetime / boolean\n</code></pre>"},{"location":"architecture/#chunk-ingestion","title":"Chunk ingestion","text":"<ul> <li>Iterable of pandas DataFrames: consumed as-is.</li> <li>Single pandas DataFrame: treated as one chunk (or sliced by rows if you pre-split it).</li> </ul>"},{"location":"architecture/#typed-accumulators","title":"Typed accumulators","text":"<p>Each column kind is handled by a specialized accumulator with small, mergeable state:</p> <ul> <li>NumericAccumulator</li> <li>Moments (n, mean, M2, M3, M4) via Welford/P\u00e9bay (exact, mergeable)</li> <li>Min/Max, zeros, negatives, \u00b1inf, missing counters</li> <li>Reservoir sample (default 20k) for quantiles, histograms, and shape hints</li> <li>KMV (K\u2011Minimum Values) for approximate distinct</li> <li>Misra\u2013Gries top\u2011k for discrete integer\u2011like columns (on demand)</li> <li>Heaping %, granularity (decimals/step), bimodality hint</li> <li>Streaming correlation chips (optional, numeric vs numeric)</li> <li> <p>Extremes with row indices (min / max tracked across chunks)</p> </li> <li> <p>CategoricalAccumulator</p> </li> <li>KMV for distinct, Misra\u2013Gries for top\u2011k</li> <li>String length stats (avg, p90), empty strings</li> <li> <p>Case/trim variant distinctness</p> </li> <li> <p>DatetimeAccumulator</p> </li> <li>Min/Max timestamps (ns), counts by hour / day of week / month</li> <li> <p>Monotonicity hints</p> </li> <li> <p>BooleanAccumulator</p> </li> <li>True/False counts, missing, imbalance hints</li> </ul> <p>All accumulators expose <code>update(...)</code> and <code>finalize() \u2192 SummaryDataclass</code> for rendering.</p>"},{"location":"architecture/#streaming-correlations","title":"Streaming correlations","text":"<p><code>_StreamingCorr</code> maintains pairwise sufficient statistics for numeric columns and emits top absolute correlations above a configurable threshold for each column.</p>"},{"location":"architecture/#rendering-pipeline","title":"Rendering pipeline","text":"<ol> <li>Infer column kinds from the first chunk.</li> <li>Build accumulators and consume the first chunk.</li> <li>Consume remaining chunks, update streaming correlations if enabled.</li> <li>Compute summary metrics (missingness, duplicates, constant columns, etc.).</li> <li>Render the template with:</li> <li>Summary cards (rows, cols, processed bytes (\u2248), missing/duplicates)</li> <li>Top missing columns</li> <li>Variables (cards by type)</li> <li>Optional dataset sample</li> </ol> <p>The template is a single file with inline CSS/JS/images to produce a portable HTML.</p>"},{"location":"architecture/#shared-helpers-deduped","title":"Shared helpers (deduped)","text":"<p>Rendering utilities live in two small modules for reuse and testability:</p> <ul> <li><code>pysuricata/render/svg_utils.py</code></li> <li><code>safe_col_id</code>, <code>nice_ticks</code>, <code>fmt_tick</code>, <code>svg_empty</code></li> <li><code>pysuricata/render/format_utils.py</code></li> <li><code>human_bytes</code>, <code>fmt_num</code>, <code>fmt_compact</code></li> </ul> <p>These power both the main report and the individual variable cards with consistent tick/label formatting.</p>"},{"location":"architecture/#configuration","title":"Configuration","text":"<p><code>ReportConfig</code> controls chunk size, sample sizes, distinct/top\u2011k sketch sizes, and correlation settings, plus logging and checkpointing. It also exposes <code>random_seed</code> to make sampling deterministic for reproducible visuals.</p> <p>Key fields:</p> <ul> <li><code>chunk_size</code>: rows per chunk (default 200k)</li> <li><code>numeric_sample_k</code>: reservoir size for numeric sampling (default 20k)</li> <li><code>uniques_k</code>: KMV sketch size (default 2048)</li> <li><code>topk_k</code>: Misra\u2013Gries capacity (default 50)</li> <li><code>compute_correlations</code>: enable/disable streaming correlation chips</li> <li><code>corr_threshold</code>, <code>corr_max_cols</code>, <code>corr_max_per_col</code></li> <li><code>include_sample</code>, <code>sample_rows</code></li> <li>Checkpointing: write periodic pickles and (optional) partial HTML</li> </ul>"},{"location":"architecture/#processed-bytes-timing","title":"Processed bytes &amp; timing","text":"<p>The report shows: - Processed bytes (\u2248): cumulative bytes processed across chunks (not process RSS) - Precise generation time in seconds (e.g., <code>0.02s</code>)</p>"},{"location":"architecture/#security-correctness-notes","title":"Security &amp; correctness notes","text":"<ul> <li>HTML escaping: column names, labels, and chip text are escaped before rendering.</li> <li>Missing/inf handling: NaN and \u00b1Inf are excluded from moment calculations but reported separately.</li> <li>Approximation badges: estimates are marked with <code>(\u2248)</code> or an <code>approx</code> badge.</li> </ul>"},{"location":"architecture/#extending","title":"Extending","text":"<ul> <li>Add backends: polars/Arrow datasets or DuckDB scans can be plugged into the chunk iterator.</li> <li>Add quantile sketches: t\u2011digest or KLL can replace the default reservoir for better tail accuracy.</li> <li>Add new sections: drift comparisons, profile JSON export to file, CLI wrapper.</li> </ul>"},{"location":"architecture/#complexity-analysis","title":"Complexity Analysis","text":""},{"location":"architecture/#time-complexity","title":"Time Complexity","text":"<p>All accumulators are designed for streaming processing with constant-time operations per element:</p> <ul> <li>NumericAccumulator: O(1) per element for basic statistics, O(log k) for extreme tracking</li> <li>CategoricalAccumulator: O(1) per element for sketches, O(log k) for top-k tracking</li> <li>DatetimeAccumulator: O(1) per element for basic operations</li> <li>BooleanAccumulator: O(1) per element for counting operations</li> </ul>"},{"location":"architecture/#space-complexity","title":"Space Complexity","text":"<p>Memory usage is bounded and independent of dataset size:</p> <ul> <li>KMV Sketch: O(k) where k is the sketch size (default: 2,048)</li> <li>Reservoir Sampling: O(s) where s is the sample size (default: 20,000)</li> <li>Misra-Gries Top-K: O(k) where k is the number of top values (default: 50)</li> <li>Extreme Tracking: O(k) where k is max extremes (default: 5)</li> <li>Chunk Metadata: O(c) where c is max chunks tracked (default: 1,000)</li> </ul>"},{"location":"architecture/#memory-optimization-features","title":"Memory Optimization Features","text":"<ul> <li>Bounded Exact Counting: KMV switches from exact counting to approximation after tracking 100 unique values</li> <li>Heap-Based Extremes: Uses heapq for O(log k) insertions instead of O(k) list operations</li> <li>Optional Chunk Metadata: Can be disabled to save memory when visualization isn't needed</li> <li>Configurable Limits: All memory usage can be tuned via configuration parameters</li> </ul>"},{"location":"architecture/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Scalability: Can process datasets larger than available memory</li> <li>Memory Efficiency: Sub-linear memory growth (typically &lt;1KB per row)</li> <li>Processing Speed: Optimized for streaming with minimal overhead</li> <li>Accuracy: Maintains statistical accuracy while using bounded memory</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to PySuricata are documented here.</p>"},{"location":"changelog/#0013-2026-01-02","title":"[0.0.13] - 2026-01-02","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>CLI tool - New command-line interface with <code>pysuricata profile</code> and <code>pysuricata summarize</code> commands</li> <li>Comprehensive stress tests - New <code>test_complexity_analysis.py</code> with time/space profiling</li> <li>Python 3.14 support - Officially supported in package metadata</li> <li>CI Benchmark markers - Heavy stress tests marked with <code>@pytest.mark.benchmark</code> for exclusion in CI</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Memory leak fixes - Resolved memory leaks in KMV sketch, ExtremeTracker, and chunk metadata</li> <li>Documentation accuracy - Updated all performance claims to reflect actual benchmark results</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Realistic benchmarks - Updated README and docs with measured performance figures:</li> <li>1M rows \u00d7 10 columns: ~3 minutes (was incorrectly claimed as 15 seconds)</li> <li>Peak memory: ~50MB (verified accurate)</li> <li>Throughput: ~5,500 rows/second</li> </ul>"},{"location":"changelog/#0011-2025-12-xx","title":"[0.0.11] - 2025-12-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Enhanced documentation with mathematical formulas for all variable types</li> <li>Comprehensive examples gallery</li> <li>Detailed algorithm documentation (Welford, P\u00e9bay, KMV, Misra-Gries)</li> <li>CI/CD documentation checks</li> </ul>"},{"location":"changelog/#improved","title":"Improved","text":"<ul> <li>Documentation structure with logical navigation</li> <li>Missing values analysis documentation</li> <li>Configuration guide with all parameters</li> </ul>"},{"location":"changelog/#0010-previous-release","title":"[0.0.10] - Previous Release","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Streaming correlation computation</li> <li>Missing values chunk-level tracking</li> <li>Enhanced datetime analysis</li> <li>Boolean variable profiling</li> </ul>"},{"location":"changelog/#earlier-versions","title":"Earlier Versions","text":"<p>See GitHub Releases for complete history.</p> <p>Last updated: 2026-01-02</p>"},{"location":"complexity-analysis/","title":"Complexity Analysis","text":"<p>This document provides a detailed analysis of the time and space complexity of PySuricata's streaming data processing algorithms, including the memory leak fixes implemented.</p>"},{"location":"complexity-analysis/#overview","title":"Overview","text":"<p>PySuricata uses streaming algorithms to process datasets of any size with bounded memory usage. The key insight is that all statistical computations can be performed using small, mergeable state that grows sub-linearly with the dataset size.</p>"},{"location":"complexity-analysis/#core-algorithms","title":"Core Algorithms","text":""},{"location":"complexity-analysis/#1-kmv-k-minimum-values-sketch","title":"1. KMV (K-Minimum Values) Sketch","text":"<p>Purpose: Approximate distinct count estimation for categorical columns.</p> <p>Algorithm: Maintains the k smallest hash values seen, estimates distinct count as <code>k / (kth_smallest_hash / 2^64)</code>.</p> <p>Time Complexity: - <code>add(value)</code>: O(log k) - Binary search and insert - <code>estimate()</code>: O(1) - Direct calculation - <code>merge(other)</code>: O(k log k) - Merge and sort</p> <p>Space Complexity: O(k) where k is the sketch size (default: 2,048)</p> <p>Memory Optimization:  - Before Fix: Used unbounded <code>_exact_values</code> set for exact counting, causing O(n) memory growth - After Fix: Bounded <code>_exact_counter</code> dict with <code>_max_exact_tracking</code> limit (default: 100) - Transition: Switches from exact counting to approximation when limit is reached</p> <p>Memory Usage:  - Exact mode: O(min(n, max_exact_tracking)) - Approximation mode: O(k) - Total: O(min(n, max_exact_tracking) + k)</p>"},{"location":"complexity-analysis/#2-reservoir-sampling","title":"2. Reservoir Sampling","text":"<p>Purpose: Maintains a uniform random sample for quantile estimation and histogram generation.</p> <p>Algorithm: Replace elements in the reservoir with probability <code>sample_size / total_elements_seen</code>.</p> <p>Time Complexity: - <code>add(value)</code>: O(1) - Constant time replacement - <code>get_sample()</code>: O(1) - Direct access to reservoir - <code>merge(other)</code>: O(s) - Merge two reservoirs</p> <p>Space Complexity: O(s) where s is the sample size (default: 20,000)</p> <p>Memory Usage: Constant O(s) regardless of dataset size.</p>"},{"location":"complexity-analysis/#3-misra-gries-top-k","title":"3. Misra-Gries Top-K","text":"<p>Purpose: Tracks the most frequent values in categorical columns.</p> <p>Algorithm: Maintains a counter for each of the k most frequent values, decrements all counters when a new value is added.</p> <p>Time Complexity: - <code>add(value)</code>: O(1) - Constant time update - <code>items()</code>: O(k) - Return top-k items - <code>merge(other)</code>: O(k) - Merge counters</p> <p>Space Complexity: O(k) where k is the number of top values (default: 50)</p> <p>Memory Usage: Constant O(k) regardless of dataset size.</p>"},{"location":"complexity-analysis/#4-extreme-tracking-fixed","title":"4. Extreme Tracking (Fixed)","text":"<p>Purpose: Tracks the minimum and maximum values with their indices.</p> <p>Algorithm: Uses bounded heaps to maintain the k smallest and k largest values.</p> <p>Time Complexity: - <code>update(values, indices)</code>: O(n log k) - Process n values, each taking O(log k) - <code>get_extremes()</code>: O(k log k) - Extract and sort from heaps - <code>merge(other)</code>: O(k log k) - Merge heaps</p> <p>Space Complexity: O(k) where k is max extremes (default: 5)</p> <p>Memory Optimization: - Before Fix: Used lists that grew temporarily to O(k \u00d7 chunks) during processing - After Fix: Uses <code>heapq</code> for bounded heaps with O(k) space - Heap Implementation: Min-heap for minimums, negated max-heap for maximums</p> <p>Memory Usage: Constant O(k) regardless of dataset size or number of chunks.</p>"},{"location":"complexity-analysis/#5-streaming-moments-welfords-algorithm","title":"5. Streaming Moments (Welford's Algorithm)","text":"<p>Purpose: Computes mean, variance, skewness, and kurtosis in a single pass.</p> <p>Algorithm: Maintains running sums of powers of deviations from the mean.</p> <p>Time Complexity: - <code>update(values)</code>: O(n) - Process n values - <code>get_stats()</code>: O(1) - Direct calculation from moments - <code>merge(other)</code>: O(1) - Merge moment statistics</p> <p>Space Complexity: O(1) - Constant space for moment statistics</p> <p>Memory Usage: Constant O(1) regardless of dataset size.</p>"},{"location":"complexity-analysis/#6-chunk-metadata-tracking-optimized","title":"6. Chunk Metadata Tracking (Optimized)","text":"<p>Purpose: Tracks per-chunk statistics for visualization.</p> <p>Algorithm: Maintains lists of chunk boundaries and missing counts.</p> <p>Time Complexity: - <code>mark_chunk_boundary()</code>: O(1) - Append to lists - <code>finalize()</code>: O(c) - Process c chunks</p> <p>Space Complexity: O(c) where c is max chunks tracked (default: 1,000)</p> <p>Memory Optimization: - Before Fix: Unbounded lists growing O(num_chunks) - After Fix: Optional tracking with <code>enable_chunk_metadata</code> flag and <code>max_chunks</code> limit - Bounded Growth: Switches to summary mode when limit is exceeded</p> <p>Memory Usage: - Enabled: O(min(num_chunks, max_chunks)) - Disabled: O(1)</p>"},{"location":"complexity-analysis/#accumulator-complexity","title":"Accumulator Complexity","text":""},{"location":"complexity-analysis/#numericaccumulator","title":"NumericAccumulator","text":"<p>Components: - StreamingMoments: O(1) space - ReservoirSampler: O(s) space - KMV: O(k) space - ExtremeTracker: O(k) space - MisraGries: O(k) space - StreamingHistogram: O(b) space (b = bins)</p> <p>Total Space Complexity: O(s + k + b) where: - s = sample_size (default: 20,000) - k = sketch_size (default: 2,048) - b = histogram_bins (default: 25)</p> <p>Time Complexity per Element: O(1) for basic operations, O(log k) for extremes</p>"},{"location":"complexity-analysis/#categoricalaccumulator","title":"CategoricalAccumulator","text":"<p>Components: - KMV: O(k) space - MisraGries: O(k) space - String length tracking: O(1) space</p> <p>Total Space Complexity: O(k)</p> <p>Time Complexity per Element: O(1) for basic operations, O(log k) for top-k</p>"},{"location":"complexity-analysis/#datetimeaccumulator","title":"DatetimeAccumulator","text":"<p>Components: - Min/Max tracking: O(1) space - Frequency counters: O(1) space</p> <p>Total Space Complexity: O(1)</p> <p>Time Complexity per Element: O(1)</p>"},{"location":"complexity-analysis/#booleanaccumulator","title":"BooleanAccumulator","text":"<p>Components: - Counters: O(1) space</p> <p>Total Space Complexity: O(1)</p> <p>Time Complexity per Element: O(1)</p>"},{"location":"complexity-analysis/#memory-leak-analysis","title":"Memory Leak Analysis","text":""},{"location":"complexity-analysis/#before-fixes","title":"Before Fixes","text":"<p>KMV Memory Leak: - Cause: <code>_exact_values</code> set grew unboundedly for low-cardinality columns - Impact: O(n) memory growth where n is the number of unique values - Example: Gender column with 2 unique values would store all 1M+ values</p> <p>ExtremeTracker Memory Leak: - Cause: Temporary lists grew to O(k \u00d7 chunks) during processing - Impact: Memory spikes proportional to number of chunks processed - Example: 1000 chunks \u00d7 5 extremes = 5000 temporary list elements</p> <p>Chunk Metadata Memory Leak: - Cause: Unbounded lists for chunk boundaries and missing counts - Impact: O(num_chunks) memory growth - Example: 10,000 chunks would store 10,000 boundary values</p>"},{"location":"complexity-analysis/#after-fixes","title":"After Fixes","text":"<p>KMV Fix: - Solution: Bounded <code>_exact_counter</code> with transition to approximation - Memory: O(min(n, max_exact_tracking) + k) - Benefit: Constant memory usage for large datasets</p> <p>ExtremeTracker Fix: - Solution: Heap-based implementation with bounded space - Memory: O(k) constant space - Benefit: No memory spikes during processing</p> <p>Chunk Metadata Fix: - Solution: Optional tracking with configurable limits - Memory: O(min(num_chunks, max_chunks)) or O(1) if disabled - Benefit: Configurable memory usage based on needs</p>"},{"location":"complexity-analysis/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"complexity-analysis/#scalability","title":"Scalability","text":"<ul> <li>Dataset Size: Can process datasets larger than available memory</li> <li>Memory Growth: Sub-linear (typically &lt;1KB per row)</li> <li>Processing Speed: Optimized for streaming with minimal overhead</li> <li>Accuracy: Maintains statistical accuracy while using bounded memory</li> </ul>"},{"location":"complexity-analysis/#memory-efficiency","title":"Memory Efficiency","text":"<p>Per Row Memory Usage: - NumericAccumulator: ~0.1-0.5 bytes per row - CategoricalAccumulator: ~0.05-0.2 bytes per row - DatetimeAccumulator: ~0.01 bytes per row - BooleanAccumulator: ~0.01 bytes per row</p> <p>Total Memory Usage: - Base: ~50-100 MB for default configuration - Per Column: ~1-5 MB depending on data type - Per Million Rows: ~100-500 MB additional</p>"},{"location":"complexity-analysis/#configuration-impact","title":"Configuration Impact","text":"<p>Memory vs. Accuracy Trade-offs: - <code>numeric_sample_size</code>: Larger = better accuracy, more memory - <code>uniques_sketch_size</code>: Larger = better distinct count accuracy, more memory - <code>top_k_size</code>: Larger = more top values tracked, more memory - <code>max_extremes</code>: Larger = more extreme values tracked, more memory - <code>enable_chunk_metadata</code>: Disable to save memory when visualization not needed</p>"},{"location":"complexity-analysis/#validation-results","title":"Validation Results","text":""},{"location":"complexity-analysis/#memory-leak-fix-validation","title":"Memory Leak Fix Validation","text":"<p>Test Results (200k rows, 6 columns): - Memory Growth: 27.64 MB (target: &lt;100 MB) \u2705 - Peak Memory: 11.44 MB (target: &lt;200 MB) \u2705 - Processing Time: 22.87 seconds - Memory Efficiency: &lt;0.1 bytes per row \u2705</p> <p>Stress Test Results (1M rows): - Memory Growth: &lt;200 MB \u2705 - Peak Memory: &lt;500 MB \u2705 - Memory per Row: &lt;1KB \u2705</p>"},{"location":"complexity-analysis/#performance-impact","title":"Performance Impact","text":"<p>Before Fixes: - Memory growth: O(n) for low-cardinality columns - Memory spikes: O(k \u00d7 chunks) during processing - Unbounded growth: O(num_chunks) for metadata</p> <p>After Fixes: - Memory growth: O(1) constant - Memory spikes: Eliminated - Bounded growth: O(k) with configurable limits</p>"},{"location":"complexity-analysis/#conclusion","title":"Conclusion","text":"<p>The memory leak fixes successfully transform PySuricata from a memory-intensive system to a truly streaming system with bounded memory usage. The key improvements are:</p> <ol> <li>Bounded Data Structures: All components now use bounded memory</li> <li>Heap-Based Algorithms: Efficient O(log k) operations for extreme tracking</li> <li>Configuration Control: Fine-grained control over memory usage vs. accuracy trade-offs</li> <li>Optional Features: Chunk metadata can be disabled to save memory</li> <li>Sub-linear Growth: Memory usage grows sub-linearly with dataset size</li> </ol> <p>These optimizations enable PySuricata to process datasets of any size on memory-constrained systems while maintaining statistical accuracy and performance.</p>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Complete guide to configuring PySuricata for your specific needs.</p>"},{"location":"configuration/#overview","title":"Overview","text":"<p>PySuricata is highly configurable via the <code>ReportConfig</code> class hierarchy:</p> <pre><code>ReportConfig\n\u251c\u2500\u2500 compute: ComputeOptions  # Analysis parameters\n\u2514\u2500\u2500 render: RenderOptions    # Display parameters\n</code></pre>"},{"location":"configuration/#quick-start","title":"Quick Start","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Create config\nconfig = ReportConfig()\n\n# Customize settings\nconfig.compute.chunk_size = 250_000\nconfig.compute.random_seed = 42\nconfig.compute.compute_correlations = True\n\n# Generate report\nreport = profile(df, config=config)\n</code></pre>"},{"location":"configuration/#computeoptions","title":"ComputeOptions","text":"<p>Control data processing and analysis.</p>"},{"location":"configuration/#basic-parameters","title":"Basic Parameters","text":"<p><code>chunk_size: int = 200_000</code></p> <p>Rows per chunk when processing data.</p> <ul> <li>Larger: Faster processing, more memory</li> <li>Smaller: Less memory, more overhead</li> <li>Recommended: 100K-500K for most datasets</li> </ul> <pre><code>config.compute.chunk_size = 250_000\n</code></pre> <p><code>columns: Optional[List[str]] = None</code></p> <p>Analyze only specific columns. If <code>None</code>, analyze all.</p> <pre><code>config.compute.columns = [\"age\", \"income\", \"city\"]\n</code></pre> <p><code>random_seed: Optional[int] = None</code></p> <p>Random seed for deterministic sampling. Set for reproducibility.</p> <pre><code>config.compute.random_seed = 42  # Same report every run\n</code></pre>"},{"location":"configuration/#numeric-configuration","title":"Numeric Configuration","text":"<p><code>numeric_sample_size: int = 20_000</code></p> <p>Reservoir sample size for quantiles and histograms.</p> <ul> <li>Larger: More accurate quantiles, more memory</li> <li>Smaller: Less memory, slightly less accurate</li> <li>Recommended: 10K-50K</li> </ul> <pre><code>config.compute.numeric_sample_size = 50_000\n</code></pre> <p><code>uniques_sketch_size: int = 2_048</code></p> <p>KMV sketch size for distinct count estimation.</p> <ul> <li>Error: \\(\\approx 1/\\sqrt{k}\\)</li> <li>k=1024: ~3.1% error</li> <li>k=2048: ~2.2% error (default)</li> <li>k=4096: ~1.6% error</li> </ul> <pre><code>config.compute.uniques_sketch_size = 4_096  # More accurate\n</code></pre>"},{"location":"configuration/#categorical-configuration","title":"Categorical Configuration","text":"<p><code>top_k_size: int = 50</code></p> <p>Number of top values to track (Misra-Gries algorithm).</p> <ul> <li>Larger: More top values, more memory</li> <li>Smaller: Fewer top values, less memory</li> <li>Guarantee: All items with frequency &gt; n/k found</li> </ul> <pre><code>config.compute.top_k_size = 100  # Track top 100\n</code></pre>"},{"location":"configuration/#correlation-configuration","title":"Correlation Configuration","text":"<p>Correlation settings are available through the public <code>ComputeOptions</code> API.</p> <p><code>compute_correlations: bool = True</code></p> <p>Enable/disable pairwise correlation computation.</p> <pre><code>from pysuricata import profile, ProfileConfig, ComputeOptions\n\nconfig = ProfileConfig(compute=ComputeOptions(\n    compute_correlations=False  # Disable for speed\n))\nreport = profile(df, config=config)\n</code></pre> <p><code>corr_threshold: float = 0.5</code></p> <p>Minimum |r| to report.</p> <pre><code>config = ProfileConfig(compute=ComputeOptions(\n    corr_threshold=0.7  # Only strong correlations\n))\n</code></pre> <p><code>corr_max_cols: int = 50</code></p> <p>Maximum columns for correlation computation. Skip if exceeded.</p> <pre><code>config = ProfileConfig(compute=ComputeOptions(\n    corr_max_cols=100  # Higher limit\n))\n</code></pre> <p><code>corr_max_per_col: int = 10</code></p> <p>Maximum correlations to show per column.</p> <pre><code>config = ProfileConfig(compute=ComputeOptions(\n    corr_max_per_col=5  # Show top 5\n))\n</code></pre>"},{"location":"configuration/#renderoptions","title":"RenderOptions","text":"<p>Control report display and formatting.</p>"},{"location":"configuration/#basic-parameters_1","title":"Basic Parameters","text":"<p><code>title: Optional[str] = None</code></p> <p>Custom report title. If <code>None</code>, uses \"Data Profile Report\".</p> <pre><code>config.render.title = \"Customer Data Analysis - Q4 2024\"\n</code></pre> <p><code>description: Optional[str] = None</code></p> <p>Markdown-formatted description shown at top of report.</p> <pre><code>config.render.description = \"\"\"\n# Analysis Overview\n\nDataset contains customer transactions from **2024 Q4**.\n\nKey metrics:\n- 1.5M transactions\n- 50K unique customers\n\"\"\"\n</code></pre> <p><code>include_sample: bool = True</code></p> <p>Include sample rows in report.</p> <pre><code>config.render.include_sample = False  # Exclude sample\n</code></pre> <p><code>sample_rows: int = 10</code></p> <p>Number of sample rows to show (if <code>include_sample=True</code>).</p> <pre><code>config.render.sample_rows = 20  # Show 20 rows\n</code></pre>"},{"location":"configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"configuration/#memory-constrained-environment","title":"Memory-Constrained Environment","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Smaller sketches\nconfig.compute.top_k_size = 20  # Fewer top values\nconfig.compute.compute_correlations = False  # Skip correlations\n</code></pre>"},{"location":"configuration/#maximum-accuracy","title":"Maximum Accuracy","text":"<pre><code>config = ReportConfig()\nconfig.compute.numeric_sample_size = 100_000  # Large samples\nconfig.compute.uniques_sketch_size = 8_192  # Large sketches\nconfig.compute.top_k_size = 200  # Many top values\nconfig.compute.corr_threshold = 0.0  # All correlations\n</code></pre>"},{"location":"configuration/#speed-optimized","title":"Speed Optimized","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 500_000  # Large chunks\nconfig.compute.numeric_sample_size = 10_000  # Small samples\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.compute.top_k_size = 20  # Few top values\n</code></pre>"},{"location":"configuration/#reproducible-reports","title":"Reproducible Reports","text":"<pre><code>config = ReportConfig()\nconfig.compute.random_seed = 42  # Deterministic\nconfig.render.title = f\"Report generated {datetime.now()}\"\n</code></pre>"},{"location":"configuration/#production-data-quality-checks","title":"Production Data Quality Checks","text":"<pre><code># Only check specific columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"customer_id\", \"transaction_amount\", \"timestamp\"]\nconfig.render.include_sample = False  # No PII in reports\n\n# Generate stats only (no HTML)\nfrom pysuricata import summarize\nstats = summarize(df, config=config)\n\n# Assert quality thresholds\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"configuration/#legacy-engineconfig","title":"Legacy EngineConfig","text":"<p>Older versions used <code>EngineConfig</code>. It's still supported but deprecated.</p> <pre><code># Old way (deprecated)\nfrom pysuricata.config import EngineConfig\ncfg = EngineConfig(chunk_size=200_000, sample_k=20_000)\n\n# New way (recommended)\nfrom pysuricata import ReportConfig\nconfig = ReportConfig()\nconfig.compute.chunk_size = 200_000\nconfig.compute.numeric_sample_size = 20_000\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Not currently supported. All configuration via code.</p>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Invalid configurations raise <code>ValueError</code>:</p> <pre><code>config = ReportConfig()\nconfig.compute.chunk_size = -1  # Invalid\n# Raises: ValueError: chunk_size must be positive\n</code></pre>"},{"location":"configuration/#performance-impact","title":"Performance Impact","text":"Parameter Increase \u2192 Impact <code>chunk_size</code> \u2191 Faster, more memory <code>numeric_sample_size</code> \u2191 More accurate quantiles, more memory <code>uniques_sketch_size</code> \u2191 More accurate distinct, more memory <code>top_k_size</code> \u2191 More top values, more memory <code>compute_correlations</code> False Much faster, less memory"},{"location":"configuration/#see-also","title":"See Also","text":"<ul> <li>Performance Tips - Optimization strategies</li> <li>Advanced Features - Advanced usage patterns</li> <li>API Reference - Complete API documentation</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"contributing/","title":"Contributing to PySuricata","text":"<p>Thank you for considering contributing to PySuricata! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+ </li> <li><code>uv</code> package manager (recommended) or <code>pip</code></li> <li>Git</li> </ul>"},{"location":"contributing/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/alvarodiez20/pysuricata.git\ncd pysuricata\n</code></pre>"},{"location":"contributing/#install-dependencies","title":"Install Dependencies","text":"Using uv (recommended)Using pip <pre><code>uv sync --dev\nuv run python -c \"import pysuricata; print('Success!')\"\n</code></pre> <pre><code>pip install -e \".[dev]\"\npython -c \"import pysuricata; print('Success!')\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=pysuricata --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_numeric.py\n\n# Run tests in parallel\nuv run pytest -n auto\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>PySuricata uses Ruff for linting and formatting.</p> <pre><code># Format code\nuv run ruff format pysuricata/\n\n# Check linting\nuv run ruff check pysuricata/\n\n# Auto-fix issues\nuv run ruff check --fix pysuricata/\n</code></pre>"},{"location":"contributing/#style-guidelines","title":"Style Guidelines","text":"<ul> <li>Follow PEP 8</li> <li>Line length: 88 characters (Black-style)</li> <li>Use type hints for function signatures</li> <li>Docstrings: Google style</li> </ul> <p>Example:</p> <pre><code>def compute_mean(values: np.ndarray) -&gt; float:\n    \"\"\"Compute arithmetic mean of values.\n\n    Args:\n        values: Array of numeric values\n\n    Returns:\n        Mean value\n\n    Raises:\n        ValueError: If array is empty\n    \"\"\"\n    if len(values) == 0:\n        raise ValueError(\"Cannot compute mean of empty array\")\n    return float(np.mean(values))\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#build-documentation-locally","title":"Build Documentation Locally","text":"<pre><code># Install docs dependencies\nuv sync --dev\n\n# Build docs\nuv run mkdocs serve\n\n# Open http://localhost:8000 in browser\n</code></pre>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Add mathematical formulas for algorithms</li> <li>Link to related pages</li> <li>Update relevant sections when changing code</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-feature-branch","title":"1. Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Branch naming: - <code>feature/</code> - New features - <code>fix/</code> - Bug fixes - <code>docs/</code> - Documentation only - <code>refactor/</code> - Code refactoring - <code>test/</code> - Test improvements</p>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write tests for new functionality</li> <li>Update documentation</li> <li>Follow code style guidelines</li> <li>Keep commits atomic and well-described</li> </ul>"},{"location":"contributing/#3-run-checks","title":"3. Run Checks","text":"<pre><code># Format\nuv run ruff format pysuricata/\n\n# Lint\nuv run ruff check pysuricata/\n\n# Test\nuv run pytest\n\n# Type check (if using mypy)\nuv run mypy pysuricata/\n\n# Build docs\nuv run mkdocs build --strict\n</code></pre>"},{"location":"contributing/#4-commit-changes","title":"4. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add support for XYZ\"\n</code></pre> <p>Commit message format: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>refactor:</code> - Code refactoring - <code>test:</code> - Test updates - <code>chore:</code> - Build/tooling changes</p>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create Pull Request on GitHub with: - Clear description of changes - Link to related issues - Screenshots for UI changes - Checklist of completed items</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions/classes in isolation.</p> <pre><code>def test_welford_mean():\n    \"\"\"Test Welford mean computation\"\"\"\n    from pysuricata.accumulators.algorithms import StreamingMoments\n\n    moments = StreamingMoments()\n    values = [1.0, 2.0, 3.0, 4.0, 5.0]\n\n    for v in values:\n        moments.update(np.array([v]))\n\n    result = moments.finalize()\n    assert abs(result[\"mean\"] - 3.0) &lt; 1e-10\n</code></pre>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<p>Test components working together.</p> <pre><code>def test_full_profile():\n    \"\"\"Test end-to-end profiling\"\"\"\n    df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [\"a\", \"b\", \"c\"]})\n    report = profile(df)\n\n    assert report.html is not None\n    assert len(report.stats[\"columns\"]) == 2\n</code></pre>"},{"location":"contributing/#property-based-tests","title":"Property-Based Tests","text":"<p>Use hypothesis for randomized testing.</p> <pre><code>from hypothesis import given\nfrom hypothesis.strategies import lists, floats\n\n@given(lists(floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_welford_matches_numpy(values):\n    \"\"\"Welford should match NumPy\"\"\"\n    moments = StreamingMoments()\n    for v in values:\n        moments.update(np.array([v]))\n\n    result = moments.finalize()\n    expected = np.mean(values)\n\n    assert abs(result[\"mean\"] - expected) &lt; 1e-6\n</code></pre>"},{"location":"contributing/#architecture-overview","title":"Architecture Overview","text":"<pre><code>pysuricata/\n\u251c\u2500\u2500 api.py              # Public API (profile, summarize)\n\u251c\u2500\u2500 report.py           # Report generation orchestration\n\u251c\u2500\u2500 config.py           # Configuration classes\n\u251c\u2500\u2500 accumulators/       # Streaming accumulators\n\u2502   \u251c\u2500\u2500 numeric.py      # Numeric statistics\n\u2502   \u251c\u2500\u2500 categorical.py  # Categorical analysis\n\u2502   \u251c\u2500\u2500 datetime.py     # Temporal analysis\n\u2502   \u2514\u2500\u2500 boolean.py      # Boolean analysis\n\u251c\u2500\u2500 compute/            # Data processing\n\u2502   \u251c\u2500\u2500 adapters/       # pandas/polars adapters\n\u2502   \u251c\u2500\u2500 analysis/       # Correlations, metrics\n\u2502   \u2514\u2500\u2500 processing/     # Chunking, inference\n\u251c\u2500\u2500 render/             # HTML generation\n\u2502   \u251c\u2500\u2500 *_card.py       # Variable type cards\n\u2502   \u251c\u2500\u2500 html.py         # Main template\n\u2502   \u2514\u2500\u2500 svg_utils.py    # SVG charts\n\u2514\u2500\u2500 templates/          # HTML templates\n</code></pre>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#add-new-statistic-to-numeric-analysis","title":"Add New Statistic to Numeric Analysis","text":"<ol> <li> <p>Update accumulator (<code>pysuricata/accumulators/numeric.py</code>): <pre><code>class NumericAccumulator:\n    def __init__(self, ...):\n        self._new_stat = 0  # Add state\n\n    def update(self, values):\n        # Update new statistic\n        self._new_stat += some_computation(values)\n\n    def finalize(self):\n        return NumericSummary(\n            ...\n            new_stat=self._new_stat  # Include in summary\n        )\n</code></pre></p> </li> <li> <p>Update summary dataclass (<code>pysuricata/accumulators/numeric.py</code>): <pre><code>@dataclass\nclass NumericSummary:\n    ...\n    new_stat: float = 0.0\n</code></pre></p> </li> <li> <p>Update renderer (<code>pysuricata/render/numeric_card.py</code>): <pre><code>def render_card(self, stats):\n    # Add new_stat to HTML\n    html += f\"&lt;div&gt;New Stat: {stats.new_stat:.2f}&lt;/div&gt;\"\n</code></pre></p> </li> <li> <p>Add tests (<code>tests/test_numeric.py</code>): <pre><code>def test_new_stat():\n    acc = NumericAccumulator(\"test\")\n    acc.update(np.array([1, 2, 3]))\n    summary = acc.finalize()\n    assert summary.new_stat == expected_value\n</code></pre></p> </li> <li> <p>Update documentation (<code>docs/stats/numeric.md</code>): <pre><code>### New Statistic\n\nMathematical definition:\n\\[\n\\text{NewStat} = \\sum_{i=1}^{n} f(x_i)\n\\]\n\nInterpretation: ...\n</code></pre></p> </li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>(For maintainers only)</p> <ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create git tag: <code>git tag v0.x.y</code></li> <li>Push tag: <code>git push origin v0.x.y</code></li> <li>CI/CD automatically builds and publishes to PyPI</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others learn and grow</li> <li>Focus on constructive feedback</li> <li>Assume good intentions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83d\udce7 Email: alvarodiez20@gmail.com</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p> <p>Thank you for contributing to PySuricata! \ud83c\udf89</p>"},{"location":"examples/","title":"Examples Gallery","text":"<p>Real-world examples showing how to use PySuricata in various scenarios.</p>"},{"location":"examples/#small-dataset-iris","title":"Small Dataset (Iris)","text":"<p>Classic machine learning dataset with 150 rows \u00d7 5 columns.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load Iris dataset\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndf = pd.read_csv(url)\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"iris_report.html\")\n\nprint(f\"Rows: {len(df)}\")\nprint(f\"Columns: {len(df.columns)}\")\n# Output: Rows: 150, Columns: 5\n</code></pre> <p>Expected output: - 4 numeric variables (sepal/petal dimensions) - 1 categorical variable (species) - No missing values - Strong correlations between dimensions</p>"},{"location":"examples/#medium-dataset-titanic","title":"Medium Dataset (Titanic)","text":"<p>Popular dataset with mixed types and missing values.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load Titanic dataset\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ndf = pd.read_csv(url)\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"titanic_report.html\")\n</code></pre> <p>Features: - 891 rows \u00d7 12 columns - Numeric: age, fare, siblings/spouses - Categorical: name, ticket, cabin, embarked - Boolean: survived - Missing values in age (~20%), cabin (~77%)</p>"},{"location":"examples/#large-dataset-streaming","title":"Large Dataset (Streaming)","text":"<p>Process multi-GB dataset in bounded memory.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndef read_large_dataset():\n    \"\"\"Generator yielding chunks\"\"\"\n    for i in range(100):\n        yield pd.read_parquet(f\"data/part-{i}.parquet\")\n\n# Configure for large data\nconfig = ReportConfig()\nconfig.compute.chunk_size = 250_000\nconfig.compute.numeric_sample_size = 50_000\nconfig.compute.random_seed = 42\n\n# Profile\nreport = profile(read_large_dataset(), config=config)\nreport.save_html(\"large_dataset_report.html\")\n</code></pre>"},{"location":"examples/#wide-dataset-many-columns","title":"Wide Dataset (Many Columns)","text":"<p>Handle datasets with hundreds of columns.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile, ReportConfig\n\n# Create wide dataset\nn_rows, n_cols = 10_000, 500\ndf = pd.DataFrame(\n    np.random.randn(n_rows, n_cols),\n    columns=[f\"feature_{i}\" for i in range(n_cols)]\n)\n\n# Disable correlations (too expensive for 500 columns)\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False\n\nreport = profile(df, config=config)\nreport.save_html(\"wide_dataset_report.html\")\n</code></pre> <p>Note: For \\(p &gt; 100\\) columns, correlation computation is O(p\u00b2) and may be slow.</p>"},{"location":"examples/#time-series-data","title":"Time Series Data","text":"<p>Analyze temporal patterns in datetime columns.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile\n\n# Generate time series\ndates = pd.date_range(\"2023-01-01\", periods=10_000, freq=\"H\")\ndf = pd.DataFrame({\n    \"timestamp\": dates,\n    \"value\": np.random.randn(10_000).cumsum(),\n    \"category\": np.random.choice([\"A\", \"B\", \"C\"], 10_000)\n})\n\nreport = profile(df)\nreport.save_html(\"timeseries_report.html\")\n</code></pre> <p>Analysis includes: - Hour-of-day distribution - Day-of-week pattern - Month distribution - Monotonicity detection</p>"},{"location":"examples/#high-missing-values","title":"High Missing Values","text":"<p>Dataset with significant missing data.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile\n\n# Create dataset with missing values\ndf = pd.DataFrame({\n    \"always_present\": range(1000),\n    \"mostly_present\": [i if i % 10 != 0 else None for i in range(1000)],  # 10% missing\n    \"half_missing\": [i if i % 2 == 0 else None for i in range(1000)],     # 50% missing\n    \"mostly_missing\": [i if i % 10 == 0 else None for i in range(1000)],  # 90% missing\n})\n\nreport = profile(df)\nreport.save_html(\"missing_values_report.html\")\n</code></pre> <p>Report highlights: - Missing percentage per column - Chunk-level distribution - Data completeness visualizations</p>"},{"location":"examples/#all-categorical","title":"All Categorical","text":"<p>Text-heavy dataset (e.g., customer feedback).</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"customer_id\": [f\"CUST_{i:05d}\" for i in range(10_000)],\n    \"product\": np.random.choice([\"Product A\", \"Product B\", \"Product C\"], 10_000),\n    \"rating\": np.random.choice([\"Poor\", \"Fair\", \"Good\", \"Excellent\"], 10_000),\n    \"feedback\": [f\"Comment {i}\" for i in range(10_000)],\n})\n\nreport = profile(df)\nreport.save_html(\"categorical_report.html\")\n</code></pre> <p>Analysis includes: - Top values and frequencies - Distinct counts (KMV sketch) - String length statistics - Entropy and Gini metrics</p>"},{"location":"examples/#polars-dataframe","title":"Polars DataFrame","text":"<p>Use polars instead of pandas.</p> <pre><code>import polars as pl\nfrom pysuricata import profile\n\n# Create polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(1000),\n    \"value\": [float(i) for i in range(1000)],\n    \"category\": [\"A\"] * 500 + [\"B\"] * 500\n})\n\n# Profile works natively with polars\nreport = profile(df)\nreport.save_html(\"polars_report.html\")\n</code></pre>"},{"location":"examples/#polars-lazyframe","title":"Polars LazyFrame","text":"<p>Streaming evaluation with polars.</p> <pre><code>import polars as pl\nfrom pysuricata import profile, ReportConfig\n\n# Create lazy frame\nlf = pl.scan_csv(\"large_file.csv\").filter(pl.col(\"value\") &gt; 0)\n\n# Configure chunk size\nconfig = ReportConfig()\nconfig.compute.chunk_size = 50_000\n\n# Profile lazily evaluated data\nreport = profile(lf, config=config)\nreport.save_html(\"polars_lazy_report.html\")\n</code></pre>"},{"location":"examples/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<p>Display reports inline in notebooks.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nreport = profile(df)\n\n# Display inline (automatic)\nreport\n\n# Or with custom size\nreport.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"examples/#programmatic-access-stats-only","title":"Programmatic Access (Stats Only)","text":"<p>Use for data quality checks without HTML.</p> <pre><code>from pysuricata import summarize\n\n# Get statistics only (faster than full report)\nstats = summarize(df)\n\n# Check data quality\nprint(f\"Rows: {stats['dataset']['rows']}\")\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.1f}%\")\nprint(f\"Duplicate rows: {stats['dataset']['duplicate_rows_pct_est']:.1f}%\")\n\n# Per-column stats\nfor col, col_stats in stats[\"columns\"].items():\n    if col_stats.get(\"missing_pct\", 0) &gt; 10:\n        print(f\"{col}: {col_stats['missing_pct']:.1f}% missing\")\n</code></pre>"},{"location":"examples/#cicd-data-quality-gates","title":"CI/CD Data Quality Gates","text":"<p>Enforce quality thresholds in pipelines.</p> <pre><code>from pysuricata import summarize\n\ndef validate_data_quality(df):\n    \"\"\"Validate data quality, raise if fails\"\"\"\n    stats = summarize(df)\n\n    # Check missing data\n    missing_pct = stats[\"dataset\"][\"missing_cells_pct\"]\n    assert missing_pct &lt; 5.0, f\"Too many missing values: {missing_pct:.1f}%\"\n\n    # Check duplicates\n    dup_pct = stats[\"dataset\"][\"duplicate_rows_pct_est\"]\n    assert dup_pct &lt; 1.0, f\"Too many duplicates: {dup_pct:.1f}%\"\n\n    # Check specific columns\n    for col in [\"customer_id\", \"transaction_id\"]:\n        col_stats = stats[\"columns\"][col]\n        assert col_stats[\"distinct\"] == col_stats[\"count\"], \\\n            f\"{col} has duplicates\"\n\n    print(\"\u2713 Data quality checks passed\")\n\n# Use in pipeline\nvalidate_data_quality(df)\n</code></pre>"},{"location":"examples/#custom-column-selection","title":"Custom Column Selection","text":"<p>Profile only specific columns.</p> <pre><code>from pysuricata import profile, ReportConfig\n\n# Large dataset, only analyze key columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"user_id\", \"purchase_amount\", \"timestamp\"]\n\nreport = profile(df, config=config)\nreport.save_html(\"key_columns_report.html\")\n</code></pre>"},{"location":"examples/#reproducible-reports","title":"Reproducible Reports","text":"<p>Generate identical reports across runs.</p> <pre><code>from pysuricata import profile, ReportConfig\nfrom datetime import datetime\n\n# Set random seed\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\n# Add metadata\nconfig.render.title = \"Weekly Data Report\"\nconfig.render.description = f\"\"\"\nReport generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\nDataset: production.customers\nVersion: 1.2.3\n\"\"\"\n\nreport = profile(df, config=config)\nreport.save_html(f\"report_{datetime.now().strftime('%Y%m%d')}.html\")\n</code></pre>"},{"location":"examples/#memory-constrained-environment","title":"Memory-Constrained Environment","text":"<p>Profile on device with limited RAM.</p> <pre><code>from pysuricata import profile, ReportConfig\n\n# Optimize for low memory\nconfig = ReportConfig()\nconfig.compute.chunk_size = 10_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Small sketches\nconfig.compute.top_k_size = 20  # Few top values\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\nreport.save_html(\"low_memory_report.html\")\n</code></pre>"},{"location":"examples/#export-statistics-as-json","title":"Export Statistics as JSON","text":"<p>Save stats for external processing.</p> <pre><code>from pysuricata import profile\n\nreport = profile(df)\n\n# Save HTML\nreport.save_html(\"report.html\")\n\n# Save JSON\nreport.save_json(\"report.json\")\n\n# Or load JSON for custom analysis\nimport json\nwith open(\"report.json\") as f:\n    stats = json.load(f)\n\n# Custom visualization\nimport matplotlib.pyplot as plt\nmissing = {col: s[\"missing_pct\"] for col, s in stats[\"columns\"].items()}\nplt.bar(missing.keys(), missing.values())\nplt.title(\"Missing Values by Column\")\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig(\"missing_chart.png\")\n</code></pre>"},{"location":"examples/#combine-multiple-datasets","title":"Combine Multiple Datasets","text":"<p>Compare multiple datasets (manual).</p> <pre><code>from pysuricata import summarize\n\n# Profile multiple datasets\nstats_train = summarize(df_train)\nstats_test = summarize(df_test)\n\n# Compare key metrics\nprint(\"Train vs Test Comparison:\")\nprint(f\"Rows: {stats_train['dataset']['rows']} vs {stats_test['dataset']['rows']}\")\nprint(f\"Missing: {stats_train['dataset']['missing_cells_pct']:.1f}% vs {stats_test['dataset']['missing_cells_pct']:.1f}%\")\n\n# Column-level comparison\nfor col in df_train.columns:\n    train_mean = stats_train[\"columns\"][col].get(\"mean\")\n    test_mean = stats_test[\"columns\"][col].get(\"mean\")\n    if train_mean and test_mean:\n        print(f\"{col} mean: {train_mean:.2f} vs {test_mean:.2f}\")\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Configuration for all options</li> <li>See Performance Tips for optimization</li> <li>Check Advanced Features for power user tips</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-pysuricata","title":"What is PySuricata?","text":"<p>PySuricata is a lightweight Python library for exploratory data analysis (EDA) that generates self-contained HTML reports using streaming algorithms for memory efficiency.</p>"},{"location":"faq/#when-should-i-use-pysuricata-vs-pandas-profiling","title":"When should I use PySuricata vs pandas-profiling?","text":"<p>Use PySuricata when: - Dataset &gt; 1 GB (memory-constrained) - Need streaming/bounded memory - Want minimal dependencies - Need reproducible reports (seeded sampling) - Working with polars</p> <p>Use pandas-profiling when: - Dataset &lt; 100 MB - Need interactive widgets - Want correlation heatmaps - Don't mind heavy dependencies</p>"},{"location":"faq/#is-pysuricata-production-ready","title":"Is PySuricata production-ready?","text":"<p>Yes! PySuricata is actively maintained with: - 90%+ test coverage - CI/CD pipeline - Semantic versioning - Regular releases on PyPI</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#how-do-i-install-pysuricata","title":"How do I install PySuricata?","text":"<pre><code>pip install pysuricata\n</code></pre>"},{"location":"faq/#what-are-the-dependencies","title":"What are the dependencies?","text":"<p>Core dependencies: - pandas (or polars) - markdown - Python 3.9+</p> <p>Optional: - polars (for polars DataFrames)</p>"},{"location":"faq/#why-is-my-installation-failing","title":"Why is my installation failing?","text":"<p>Common issues: 1. Python version: Requires 3.9+    <pre><code>python --version  # Check version\n</code></pre></p> <ol> <li>Conflicting packages: Try fresh virtual environment    <pre><code>python -m venv venv\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\npip install pysuricata\n</code></pre></li> </ol>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-generate-a-report","title":"How do I generate a report?","text":"<pre><code>from pysuricata import profile\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"faq/#can-i-profile-only-specific-columns","title":"Can I profile only specific columns?","text":"<p>Yes:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.columns = [\"col1\", \"col2\", \"col3\"]\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"faq/#how-do-i-make-reports-reproducible","title":"How do I make reports reproducible?","text":"<p>Set random seed:</p> <pre><code>config = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"faq/#can-i-get-statistics-without-html","title":"Can I get statistics without HTML?","text":"<p>Yes, use <code>summarize()</code>:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nprint(stats[\"dataset\"])\nprint(stats[\"columns\"][\"my_column\"])\n</code></pre>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-much-memory-does-pysuricata-use","title":"How much memory does PySuricata use?","text":"<p>Approximately: - Base overhead: ~50 MB - Per numeric column: ~160 KB (default sample_size=20K) - Per categorical column: ~100 KB - Independent of dataset size (streaming)</p>"},{"location":"faq/#my-report-is-slow-how-can-i-speed-it-up","title":"My report is slow. How can I speed it up?","text":"<p>Quick wins: 1. Disable correlations:    <pre><code>config.compute.compute_correlations = False\n</code></pre></p> <ol> <li> <p>Reduce sample sizes:    <pre><code>config.compute.numeric_sample_size = 10_000  # Default: 20_000\n</code></pre></p> </li> <li> <p>Increase chunk size:    <pre><code>config.compute.chunk_size = 500_000  # Default: 200_000\n</code></pre></p> </li> </ol>"},{"location":"faq/#can-pysuricata-handle-1-tb-datasets","title":"Can PySuricata handle 1 TB datasets?","text":"<p>Yes, with streaming:</p> <pre><code>def read_large_dataset():\n    for file in large_files:\n        yield pd.read_parquet(file)\n\nreport = profile(read_large_dataset())\n</code></pre> <p>Memory usage stays constant regardless of dataset size.</p>"},{"location":"faq/#why-are-correlations-slow","title":"Why are correlations slow?","text":"<p>Correlation computation is O(p\u00b2) where p = number of numeric columns.</p> <p>Solutions: - Disable for &gt; 100 columns - Increase threshold to show fewer correlations - Profile fewer columns</p>"},{"location":"faq/#technical-questions","title":"Technical Questions","text":""},{"location":"faq/#are-the-statistics-exact-or-approximate","title":"Are the statistics exact or approximate?","text":"<p>Exact: - Mean, variance, skewness, kurtosis (Welford/P\u00e9bay) - Min, max, count - Quantiles (if dataset fits in sample)</p> <p>Approximate: - Distinct count (KMV sketch, ~2% error with k=2048) - Top-k values (Misra-Gries, guaranteed for freq &gt; n/k) - Quantiles for huge datasets (from reservoir sample)</p>"},{"location":"faq/#how-accurate-are-the-approximations","title":"How accurate are the approximations?","text":"<p>Distinct count (KMV): - k=1024: ~3% error - k=2048: ~2% error (default) - k=4096: ~1.5% error</p> <p>Top-k (Misra-Gries): - Guaranteed to find all items with frequency &gt; n/k - Frequency estimates within \u00b1n/k</p>"},{"location":"faq/#what-algorithms-does-pysuricata-use","title":"What algorithms does PySuricata use?","text":"<ul> <li>Moments: Welford's online algorithm, P\u00e9bay's parallel merge</li> <li>Distinct count: K-Minimum Values (KMV) sketch</li> <li>Top-k: Misra-Gries algorithm</li> <li>Quantiles: Reservoir sampling (exact uniform sample)</li> <li>Correlations: Streaming Pearson correlation</li> </ul> <p>See Algorithms for details.</p>"},{"location":"faq/#does-pysuricata-support-distributed-computing","title":"Does PySuricata support distributed computing?","text":"<p>Partially: - Accumulators are mergeable: Can run on multiple machines and merge results - No built-in distribution: Must use external framework (Spark, Dask)</p> <p>Example with manual merge:</p> <pre><code># Machine 1\nacc1 = NumericAccumulator(\"col\")\nacc1.update(data_part1)\n\n# Machine 2\nacc2 = NumericAccumulator(\"col\")\nacc2.update(data_part2)\n\n# Merge\nacc1.merge(acc2)\nfinal_stats = acc1.finalize()\n</code></pre>"},{"location":"faq/#data-questions","title":"Data Questions","text":""},{"location":"faq/#does-pysuricata-modify-my-data","title":"Does PySuricata modify my data?","text":"<p>No. PySuricata only reads data, never modifies it.</p>"},{"location":"faq/#what-data-formats-are-supported","title":"What data formats are supported?","text":"<p>Anything that can be loaded into pandas or polars: - CSV, Parquet, JSON, Excel - SQL databases (via pandas read_sql) - APIs (via pandas read_json)</p> <p>Just load into DataFrame first:</p> <pre><code>df = pd.read_csv(\"data.csv\")\nreport = profile(df)\n</code></pre>"},{"location":"faq/#can-i-profile-streaming-data-kafka-etc","title":"Can I profile streaming data (Kafka, etc.)?","text":"<p>Yes, if you can iterate through chunks:</p> <pre><code>def read_from_kafka():\n    consumer = KafkaConsumer(...)\n    chunk = []\n    for message in consumer:\n        chunk.append(parse(message))\n        if len(chunk) &gt;= 10_000:\n            yield pd.DataFrame(chunk)\n            chunk = []\n\nreport = profile(read_from_kafka())\n</code></pre>"},{"location":"faq/#how-does-pysuricata-handle-missing-values","title":"How does PySuricata handle missing values?","text":"<ul> <li>Excluded from calculations: Missing values don't affect mean, variance, etc.</li> <li>Reported separately: Missing count and percentage shown</li> <li>Visualization: Missing data distribution per chunk</li> </ul> <p>See Missing Values.</p>"},{"location":"faq/#what-about-duplicate-rows","title":"What about duplicate rows?","text":"<p>PySuricata estimates duplicate percentage using KMV sketch (approximate). For exact duplicates:</p> <pre><code>exact_duplicates = df.duplicated().sum()\ndup_pct = (exact_duplicates / len(df)) * 100\n</code></pre>"},{"location":"faq/#report-questions","title":"Report Questions","text":""},{"location":"faq/#why-is-my-html-report-so-large","title":"Why is my HTML report so large?","text":"<p>Possible reasons: 1. Many columns: Each variable card adds HTML 2. Large sample: Reduce <code>sample_rows</code> 3. Many top values: Reduce <code>top_k_size</code></p> <p>Typical sizes: - 10 columns: ~500 KB - 50 columns: ~2 MB - 100 columns: ~4 MB</p>"},{"location":"faq/#can-i-customize-the-report-appearance","title":"Can I customize the report appearance?","text":"<p>Not directly. The report uses inline CSS for portability. </p> <p>Workaround: Modify the template in <code>pysuricata/templates/report_template.html</code> (advanced).</p>"},{"location":"faq/#can-i-export-to-pdf","title":"Can I export to PDF?","text":"<p>Not built-in. Options: 1. Print HTML to PDF in browser 2. Use tool like <code>wkhtmltopdf</code>:    <pre><code>wkhtmltopdf report.html report.pdf\n</code></pre></p>"},{"location":"faq/#how-do-i-display-reports-in-jupyter","title":"How do I display reports in Jupyter?","text":"<pre><code>report = profile(df)\nreport  # Auto-displays\n\n# Or with custom size\nreport.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"faq/#error-messages","title":"Error Messages","text":""},{"location":"faq/#out-of-memory-error","title":"\"Out of memory\" error","text":"<p>Reduce memory usage:</p> <pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Smaller chunks\nconfig.compute.numeric_sample_size = 5_000  # Smaller samples\nconfig.compute.compute_correlations = False  # Skip correlations\n</code></pre>"},{"location":"faq/#cannot-infer-type-error","title":"\"Cannot infer type\" error","text":"<p>Some columns may have mixed types. Clean data first:</p> <pre><code># Convert to consistent type\ndf[\"mixed_col\"] = df[\"mixed_col\"].astype(str)\n\n# Or drop problematic columns\ndf = df.drop(columns=[\"problematic_col\"])\n</code></pre>"},{"location":"faq/#build-failed-documentation","title":"\"Build failed\" (documentation)","text":"<p>If contributing and docs build fails:</p> <pre><code># Check mkdocs.yml syntax\nuv run python -c \"import yaml; yaml.safe_load(open('mkdocs.yml'))\"\n\n# Verify all files exist\nls docs/  # Check file names match mkdocs.yml\n\n# Build with verbose output\nuv run mkdocs build --verbose\n</code></pre>"},{"location":"faq/#contributing-questions","title":"Contributing Questions","text":""},{"location":"faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>See Contributing Guide.</p> <p>Ways to contribute: - Report bugs - Suggest features - Improve documentation - Submit pull requests - Help others in Discussions</p>"},{"location":"faq/#where-do-i-report-bugs","title":"Where do I report bugs?","text":"<p>GitHub Issues</p> <p>Include: - Python version - PySuricata version - Minimal reproducible example - Error message/traceback</p>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ul> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83d\udce7 Email: alvarodiez20@gmail.com</li> </ul>"},{"location":"faq/#still-have-questions","title":"Still have questions?","text":"<p>Ask in GitHub Discussions or open an issue.</p> <p>Last updated: 2025-10-12</p>"},{"location":"install/","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install pysuricata\n</code></pre> <p>Optional: install <code>polars</code> to use polars DataFrames directly:</p> <pre><code>pip install polars\n</code></pre> <p>Verify your installation:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pysuricata import profile\n&gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2, 3]})\n&gt;&gt;&gt; profile(df).html[:15]\n'&lt;!DOCTYPE html&gt;'\n</code></pre>"},{"location":"performance/","title":"Performance Tips","text":"<p>Optimize PySuricata for your specific use case with these strategies.</p>"},{"location":"performance/#quick-wins","title":"Quick Wins","text":""},{"location":"performance/#1-disable-correlations-for-many-columns","title":"1. Disable Correlations for Many Columns","text":"<p>For datasets with &gt; 100 numeric columns, correlation computation is O(p\u00b2) and can be slow.</p> <pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\n</code></pre> <p>Speed improvement: 2-10x for wide datasets</p>"},{"location":"performance/#2-increase-chunk-size","title":"2. Increase Chunk Size","text":"<p>Larger chunks mean fewer iterations and less overhead.</p> <pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 500_000  # Default: 200_000\n\nreport = profile(df, config=config)\n</code></pre> <p>Trade-off: More memory usage</p>"},{"location":"performance/#3-reduce-sample-sizes","title":"3. Reduce Sample Sizes","text":"<p>Smaller samples are faster to process.</p> <pre><code>config = ReportConfig()\nconfig.compute.numeric_sample_size = 10_000  # Default: 20_000\n\nreport = profile(df, config=config)\n</code></pre> <p>Trade-off: Slightly less accurate quantiles</p>"},{"location":"performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"performance/#memory-constrained-environments","title":"Memory-Constrained Environments","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Small sketches\nconfig.compute.top_k_size = 20  # Fewer top values\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\n</code></pre> <p>Memory usage: ~20-30 MB (vs ~50 MB default)</p>"},{"location":"performance/#monitor-memory-usage","title":"Monitor Memory Usage","text":"<pre><code>import psutil\nimport os\n\nprocess = psutil.Process(os.getpid())\nprint(f\"Memory before: {process.memory_info().rss / 1024**2:.1f} MB\")\n\nreport = profile(df, config=config)\n\nprint(f\"Memory after: {process.memory_info().rss / 1024**2:.1f} MB\")\n</code></pre>"},{"location":"performance/#speed-optimization","title":"Speed Optimization","text":""},{"location":"performance/#profile-only-key-columns","title":"Profile Only Key Columns","text":"<pre><code>config = ReportConfig()\nconfig.compute.columns = [\"user_id\", \"amount\", \"timestamp\"]\n\nreport = profile(df, config=config)\n</code></pre> <p>Speed improvement: Linear in number of columns</p>"},{"location":"performance/#use-polars-for-large-datasets","title":"Use Polars for Large Datasets","text":"<p>Polars can be faster than pandas for certain operations:</p> <pre><code>import polars as pl\n\ndf = pl.read_csv(\"large_file.csv\")\nreport = profile(df)  # Native polars support\n</code></pre>"},{"location":"performance/#parallelize-with-dask-advanced","title":"Parallelize with Dask (Advanced)","text":"<pre><code>import dask.dataframe as dd\n\n# Load with Dask\nddf = dd.read_csv(\"large_file.csv\")\n\n# Convert partitions to generator\ndef partition_generator():\n    for partition in ddf.partitions:\n        yield partition.compute()\n\nreport = profile(partition_generator())\n</code></pre>"},{"location":"performance/#benchmarks","title":"Benchmarks","text":""},{"location":"performance/#performance-by-dataset-size","title":"Performance by Dataset Size","text":"Rows Columns Time Throughput Memory 10K 10 ~3s ~3,000 rows/s 20 MB 100K 10 ~13s ~8,000 rows/s 30 MB 1M 10 ~3 min ~5,500 rows/s 50 MB 10M 10 ~30 min ~5,500 rows/s 50 MB <p>Benchmarks measured on Apple Silicon with Python 3.13. Actual times vary by hardware, data complexity, and configuration.</p>"},{"location":"performance/#scalability","title":"Scalability","text":"<p>PySuricata scales linearly with dataset size (O(n)) thanks to streaming algorithms:</p> <pre><code>Time(n) \u2248 k \u00d7 n\n</code></pre> <p>where k is constant per row processing time.</p>"},{"location":"performance/#comparison-to-competitors","title":"Comparison to Competitors","text":"Library Memory (1M+ rows) Streaming pysuricata ~50 MB (constant) \u2705 Yes pandas-profiling 1.2 GB \u274c No sweetviz 1.1 GB \u274c No pandas-eda 1.0 GB \u274c No <p>PySuricata's key advantage is bounded memory usage - competitors cannot process datasets larger than RAM.</p>"},{"location":"performance/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"performance/#for-maximum-speed","title":"For Maximum Speed","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 1_000_000  # Large chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 512  # Tiny sketches\nconfig.compute.top_k_size = 10  # Few top values\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.render.include_sample = False  # No sample in report\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"performance/#for-maximum-accuracy","title":"For Maximum Accuracy","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 100_000  # Smaller for better merging\nconfig.compute.numeric_sample_size = 100_000  # Large samples\nconfig.compute.uniques_sketch_size = 8_192  # Large sketches\nconfig.compute.top_k_size = 200  # Many top values\nconfig.compute.corr_threshold = 0.0  # All correlations\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"performance/#profiling-pysuricata","title":"Profiling PySuricata","text":"<p>Use Python's profiler to find bottlenecks:</p> <pre><code>import cProfile\nimport pstats\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\nreport = profile(df)\n\nprofiler.disable()\nstats = pstats.Stats(profiler)\nstats.sort_stats('cumulative')\nstats.print_stats(20)  # Top 20 functions\n</code></pre>"},{"location":"performance/#common-bottlenecks","title":"Common Bottlenecks","text":""},{"location":"performance/#1-correlation-computation","title":"1. Correlation Computation","text":"<p>Symptom: Slow for &gt; 100 numeric columns Solution: Disable correlations or increase threshold</p>"},{"location":"performance/#2-many-categorical-columns","title":"2. Many Categorical Columns","text":"<p>Symptom: Slow with &gt; 50 categorical columns Solution: Reduce <code>top_k_size</code>, increase <code>chunk_size</code></p>"},{"location":"performance/#3-very-wide-datasets-1000-columns","title":"3. Very Wide Datasets (&gt; 1000 columns)","text":"<p>Symptom: Slow overall Solution: Profile in batches, combine reports manually</p>"},{"location":"performance/#4-small-chunk-size","title":"4. Small Chunk Size","text":"<p>Symptom: Slow despite small dataset Solution: Increase <code>chunk_size</code> to reduce overhead</p>"},{"location":"performance/#production-optimization","title":"Production Optimization","text":""},{"location":"performance/#scheduled-reports","title":"Scheduled Reports","text":"<p>For regular reporting, optimize for speed:</p> <pre><code># Fast config for nightly reports\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False\nconfig.compute.numeric_sample_size = 10_000\nconfig.render.title = f\"Daily Report - {date.today()}\"\n\nreport = profile(df, config=config)\nreport.save_html(f\"reports/daily_{date.today()}.html\")\n</code></pre>"},{"location":"performance/#cicd-quality-checks","title":"CI/CD Quality Checks","text":"<p>Use <code>summarize()</code> for faster stats-only checks:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)  # Faster than profile()\n\n# Check thresholds\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"performance/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - All configuration options</li> <li>Examples - Real-world use cases</li> <li>Advanced Features - Power user tips</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get started with PySuricata in less than 5 minutes!</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install pysuricata from PyPI:</p> <pre><code>pip install pysuricata\n</code></pre> <p>For polars support (optional):</p> <pre><code>pip install pysuricata polars\n</code></pre>"},{"location":"quickstart/#command-line-usage","title":"Command Line Usage","text":"<p>The fastest way to profile a dataset:</p> <pre><code># Generate an HTML report\npysuricata profile data.csv --output report.html\n\n# Get JSON statistics (no HTML)\npysuricata summarize data.csv --output stats.json\n\n# With options\npysuricata profile data.csv -o report.html --seed 42 --no-correlations\n</code></pre> <p>See <code>pysuricata --help</code> for all options.</p>"},{"location":"quickstart/#your-first-report","title":"Your First Report","text":""},{"location":"quickstart/#step-1-import-and-load-data","title":"Step 1: Import and Load Data","text":"PandasPolarsFrom URL <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load a dataset\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\n# Load a dataset\ndf = pl.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n</code></pre> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load directly from URL\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndf = pd.read_csv(url)\n</code></pre>"},{"location":"quickstart/#step-2-generate-report","title":"Step 2: Generate Report","text":"<pre><code># Create the profile report\nreport = profile(df)\n\n# Save to HTML\nreport.save_html(\"iris_report.html\")\n</code></pre> <p>That's it! Open <code>iris_report.html</code> in your browser to see a comprehensive analysis.</p>"},{"location":"quickstart/#understanding-your-report","title":"Understanding Your Report","text":"<p>The generated report contains several sections:</p>"},{"location":"quickstart/#1-dataset-overview","title":"1. Dataset Overview","text":"<ul> <li>Number of rows and columns</li> <li>Memory usage (approximate)</li> <li>Missing values summary</li> <li>Duplicate rows estimate</li> <li>Processing time</li> </ul>"},{"location":"quickstart/#2-variables-section","title":"2. Variables Section","text":"<p>For each variable, you'll see:</p>"},{"location":"quickstart/#numeric-variables","title":"Numeric Variables","text":"<ul> <li>Count, missing percentage</li> <li>Mean, median, standard deviation</li> <li>Min, max, range</li> <li>Quantiles (Q1, Q2, Q3)</li> <li>Skewness and kurtosis</li> <li>Histogram visualization</li> </ul>"},{"location":"quickstart/#categorical-variables","title":"Categorical Variables","text":"<ul> <li>Count, missing percentage</li> <li>Number of unique values</li> <li>Top values with frequencies</li> <li>Diversity metrics</li> </ul>"},{"location":"quickstart/#datetime-variables","title":"DateTime Variables","text":"<ul> <li>Temporal range (min/max)</li> <li>Distribution by hour, day of week, month</li> <li>Timeline visualization</li> </ul>"},{"location":"quickstart/#boolean-variables","title":"Boolean Variables","text":"<ul> <li>True/False counts</li> <li>Balance ratio</li> <li>Missing percentage</li> </ul>"},{"location":"quickstart/#3-correlations-for-numeric-columns","title":"3. Correlations (for numeric columns)","text":"<ul> <li>Top correlations for each numeric variable</li> <li>Correlation strength indicators</li> </ul>"},{"location":"quickstart/#working-with-your-data","title":"Working with Your Data","text":""},{"location":"quickstart/#save-as-json-for-programmatic-access","title":"Save as JSON (for programmatic access)","text":"<pre><code># Generate statistics only\nfrom pysuricata import summarize\n\nstats = summarize(df)\nprint(stats[\"dataset\"])  # Dataset-level metrics\nprint(stats[\"columns\"][\"sepal_length\"])  # Per-column stats\n\n# Or save from report\nreport.save_json(\"iris_stats.json\")\n</code></pre>"},{"location":"quickstart/#display-in-jupyter-notebook","title":"Display in Jupyter Notebook","text":"<pre><code># In a Jupyter notebook\nfrom pysuricata import profile\n\nreport = profile(df)\nreport  # Automatically displays inline\n</code></pre> <p>For better display in notebooks:</p> <pre><code>report.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"quickstart/#quick-data-quality-check","title":"Quick Data Quality Check","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\n# Check data quality metrics\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.2f}%\")\nprint(f\"Duplicate rows: {stats['dataset']['duplicate_rows_pct_est']:.2f}%\")\n\n# Assert quality requirements\nassert stats['dataset']['missing_cells_pct'] &lt; 5.0, \"Too many missing values\"\n</code></pre>"},{"location":"quickstart/#profile-specific-columns","title":"Profile Specific Columns","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Select specific columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"sepal_length\", \"sepal_width\", \"species\"]\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#reproducible-reports","title":"Reproducible Reports","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Set random seed for deterministic sampling\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n# Same report every time!\n</code></pre>"},{"location":"quickstart/#process-large-files-in-chunks","title":"Process Large Files in Chunks","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Read and process in chunks\ndef read_chunks():\n    for chunk in pd.read_csv(\"large_file.csv\", chunksize=100_000):\n        yield chunk\n\nreport = profile(read_chunks())\nreport.save_html(\"large_file_report.html\")\n</code></pre>"},{"location":"quickstart/#configuration-basics","title":"Configuration Basics","text":"<p>PySuricata is highly configurable. Here are some common settings:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Adjust chunk size for memory management\nconfig.compute.chunk_size = 200_000  # Default\n\n# Control sample sizes\nconfig.compute.numeric_sample_size = 20_000  # For quantiles/histograms\nconfig.compute.uniques_sketch_size = 2_048   # For distinct counts\nconfig.compute.top_k_size = 50               # For top values\n\n# Enable/disable correlations\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5\n\n# Deterministic sampling\nconfig.compute.random_seed = 42\n\n# Generate report\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#performance-tips","title":"Performance Tips","text":""},{"location":"quickstart/#for-large-datasets-1-gb","title":"For Large Datasets (&gt; 1 GB)","text":"<pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.chunk_size = 250_000  # Larger chunks\nconfig.compute.numeric_sample_size = 10_000  # Smaller samples\nconfig.compute.compute_correlations = False  # Skip if not needed\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#for-memory-constrained-environments","title":"For Memory-Constrained Environments","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Smaller chunks\nconfig.compute.numeric_sample_size = 5_000  # Smaller samples\nconfig.compute.uniques_sketch_size = 1_024  # Smaller sketches\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#for-speed","title":"For Speed","text":"<pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.compute.top_k_size = 20  # Fewer top values\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first report, explore:</p> <ul> <li>Basic Usage - Detailed usage patterns</li> <li>Configuration - All configuration options</li> <li>Performance Tips - Optimize for your use case</li> <li>Examples Gallery - More real-world examples</li> <li>Statistical Methods - Understand the algorithms</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#report-is-too-large","title":"Report is too large","text":"<ul> <li>Reduce <code>numeric_sample_size</code></li> <li>Skip correlations: <code>config.compute.compute_correlations = False</code></li> <li>Profile fewer columns: <code>config.compute.columns = [\"col1\", \"col2\"]</code></li> </ul>"},{"location":"quickstart/#out-of-memory","title":"Out of memory","text":"<ul> <li>Reduce <code>chunk_size</code></li> <li>Reduce all sample sizes</li> <li>Process columns separately</li> </ul>"},{"location":"quickstart/#report-takes-too-long","title":"Report takes too long","text":"<ul> <li>Increase <code>chunk_size</code> (if memory allows)</li> <li>Disable correlations</li> <li>Reduce <code>top_k_size</code></li> </ul>"},{"location":"quickstart/#want-more-decimal-places","title":"Want more decimal places","text":"<pre><code># Not currently configurable, but stats JSON has full precision\nreport.save_json(\"stats.json\")\n</code></pre>"},{"location":"quickstart/#get-help","title":"Get Help","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udce7 Contact Maintainer</li> </ul> <p>Ready for more advanced features? Check out the Advanced Guide.</p>"},{"location":"sequence-diagrams-complexity/","title":"Annotated Sequence Diagrams with Complexity Analysis","text":"<p>This document contains sequence diagrams showing the data flow through PySuricata's streaming algorithms, annotated with time and space complexity for each operation.</p>"},{"location":"sequence-diagrams-complexity/#pandas-data-processing-flow","title":"Pandas Data Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Profile\n    participant Adapter as PandasAdapter\n    participant Accumulator as NumericAccumulator\n    participant KMV\n    participant Reservoir as ReservoirSampler\n    participant Extreme as ExtremeTracker\n    participant MG as MisraGries\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(1) - Configuration setup\n\n    Profile-&gt;&gt;Adapter: process_chunk(chunk)\n    Note over Adapter: O(n) - Iterate through rows\n\n    loop For each numeric column\n        Adapter-&gt;&gt;Accumulator: update(values)\n        Note over Accumulator: O(n) - Process n values\n\n        Accumulator-&gt;&gt;KMV: add(value)\n        Note over KMV: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;Reservoir: add(value)\n        Note over Reservoir: O(1) - Constant time&lt;br/&gt;Space: O(s) bounded\n\n        Accumulator-&gt;&gt;Extreme: update(values, indices)\n        Note over Extreme: O(n log k) - Process n values&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;MG: add(value)\n        Note over MG: O(1) - Constant time&lt;br/&gt;Space: O(k) bounded\n    end\n\n    Accumulator-&gt;&gt;Profile: finalize()\n    Note over Accumulator: O(k log k) - Extract results&lt;br/&gt;Space: O(k) bounded\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results</code></pre>"},{"location":"sequence-diagrams-complexity/#polars-data-processing-flow","title":"Polars Data Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Profile\n    participant Adapter as PolarsAdapter\n    participant Accumulator as NumericAccumulator\n    participant KMV\n    participant Reservoir as ReservoirSampler\n    participant Extreme as ExtremeTracker\n    participant MG as MisraGries\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(1) - Configuration setup\n\n    Profile-&gt;&gt;Adapter: process_chunk(chunk)\n    Note over Adapter: O(n) - Iterate through rows\n\n    loop For each numeric column\n        Adapter-&gt;&gt;Accumulator: update(values)\n        Note over Accumulator: O(n) - Process n values\n\n        Accumulator-&gt;&gt;KMV: add(value)\n        Note over KMV: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;Reservoir: add(value)\n        Note over Reservoir: O(1) - Constant time&lt;br/&gt;Space: O(s) bounded\n\n        Accumulator-&gt;&gt;Extreme: update(values, indices)\n        Note over Extreme: O(n log k) - Process n values&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;MG: add(value)\n        Note over MG: O(1) - Constant time&lt;br/&gt;Space: O(k) bounded\n    end\n\n    Accumulator-&gt;&gt;Profile: finalize()\n    Note over Accumulator: O(k log k) - Extract results&lt;br/&gt;Space: O(k) bounded\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results</code></pre>"},{"location":"sequence-diagrams-complexity/#kmv-sketch-memory-optimization","title":"KMV Sketch Memory Optimization","text":"<pre><code>sequenceDiagram\n    participant KMV\n    participant ExactCounter as _exact_counter\n    participant Values as _values\n    participant Hash as _u64\n\n    Note over KMV: Before Fix: O(n) memory growth&lt;br/&gt;After Fix: O(k) bounded memory\n\n    KMV-&gt;&gt;ExactCounter: add(value)\n    Note over ExactCounter: O(1) - Dict lookup&lt;br/&gt;Space: O(min(n, max_exact_tracking))\n\n    alt Exact mode (count &lt; max_exact_tracking)\n        ExactCounter-&gt;&gt;ExactCounter: increment counter\n        Note over ExactCounter: O(1) - Dict update\n    else Transition to approximation mode\n        ExactCounter-&gt;&gt;Values: convert to hashes\n        Note over Values: O(k) - Convert exact values&lt;br/&gt;Space: O(k) bounded\n\n        ExactCounter-&gt;&gt;ExactCounter: clear()\n        Note over ExactCounter: O(1) - Free memory\n\n        Values-&gt;&gt;Values: sort()\n        Note over Values: O(k log k) - Sort hashes\n    end\n\n    alt Approximation mode\n        KMV-&gt;&gt;Hash: _u64(value)\n        Note over Hash: O(1) - Hash computation\n\n        Hash-&gt;&gt;Values: insert if smaller\n        Note over Values: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n    end\n\n    KMV-&gt;&gt;Values: estimate()\n    Note over Values: O(1) - Direct calculation&lt;br/&gt;Space: O(k) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#extremetracker-memory-optimization","title":"ExtremeTracker Memory Optimization","text":"<pre><code>sequenceDiagram\n    participant Extreme as ExtremeTracker\n    participant MinHeap as _min_heap\n    participant MaxHeap as _max_heap\n    participant Heapq\n\n    Note over Extreme: Before Fix: O(k \u00d7 chunks) temporary growth&lt;br/&gt;After Fix: O(k) constant space\n\n    Extreme-&gt;&gt;Extreme: update(values, indices)\n    Note over Extreme: O(n log k) - Process n values\n\n    loop For each value\n        Extreme-&gt;&gt;MinHeap: _add_to_min_heap(index, value)\n        Note over MinHeap: O(log k) - Heap insert&lt;br/&gt;Space: O(k) bounded\n\n        Extreme-&gt;&gt;MaxHeap: _add_to_max_heap(index, value)\n        Note over MaxHeap: O(log k) - Heap insert&lt;br/&gt;Space: O(k) bounded\n    end\n\n    alt Min heap not full\n        MinHeap-&gt;&gt;Heapq: heappush(value, index)\n        Note over Heapq: O(log k) - Heap insert\n    else Min heap full\n        MinHeap-&gt;&gt;MinHeap: find largest value\n        Note over MinHeap: O(k) - Linear search\n\n        alt New value smaller\n            MinHeap-&gt;&gt;MinHeap: replace largest\n            Note over MinHeap: O(k) - Find and replace\n            MinHeap-&gt;&gt;Heapq: heapify()\n            Note over Heapq: O(k) - Restore heap property\n        end\n    end\n\n    alt Max heap not full\n        MaxHeap-&gt;&gt;Heapq: heappush(-value, index)\n        Note over Heapq: O(log k) - Heap insert (negated)\n    else Max heap full\n        MaxHeap-&gt;&gt;MaxHeap: find smallest original value\n        Note over MaxHeap: O(k) - Linear search\n\n        alt New value larger\n            MaxHeap-&gt;&gt;MaxHeap: replace smallest\n            Note over MaxHeap: O(k) - Find and replace\n            MaxHeap-&gt;&gt;Heapq: heapify()\n            Note over Heapq: O(k) - Restore heap property\n        end\n    end\n\n    Extreme-&gt;&gt;Extreme: get_extremes()\n    Note over Extreme: O(k log k) - Extract and sort&lt;br/&gt;Space: O(k) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#chunk-metadata-optimization","title":"Chunk Metadata Optimization","text":"<pre><code>sequenceDiagram\n    participant Accumulator as NumericAccumulator\n    participant Config as NumericConfig\n    participant Boundaries as _chunk_boundaries\n    participant Missing as _chunk_missing\n    participant Counter as _chunk_count\n\n    Note over Accumulator: Before Fix: O(num_chunks) unbounded growth&lt;br/&gt;After Fix: O(min(num_chunks, max_chunks)) bounded\n\n    Accumulator-&gt;&gt;Config: check enable_chunk_metadata\n    Note over Config: O(1) - Configuration check\n\n    alt Chunk metadata enabled\n        Accumulator-&gt;&gt;Counter: check _chunk_count &lt; max_chunks\n        Note over Counter: O(1) - Counter check\n\n        alt Under limit\n            Accumulator-&gt;&gt;Boundaries: append(cumulative_rows)\n            Note over Boundaries: O(1) - List append&lt;br/&gt;Space: O(chunk_count)\n\n            Accumulator-&gt;&gt;Missing: append(missing_count)\n            Note over Missing: O(1) - List append&lt;br/&gt;Space: O(chunk_count)\n\n            Accumulator-&gt;&gt;Counter: increment()\n            Note over Counter: O(1) - Counter increment\n        else Over limit\n            Accumulator-&gt;&gt;Config: disable chunk metadata\n            Note over Config: O(1) - Switch to summary mode\n\n            Accumulator-&gt;&gt;Boundaries: stop tracking\n            Note over Boundaries: O(1) - Stop appending&lt;br/&gt;Space: O(max_chunks) bounded\n        end\n    else Chunk metadata disabled\n        Accumulator-&gt;&gt;Accumulator: skip tracking\n        Note over Accumulator: O(1) - No memory usage&lt;br/&gt;Space: O(1) constant\n    end\n\n    Accumulator-&gt;&gt;Accumulator: finalize()\n    Note over Accumulator: O(chunk_count) - Process metadata&lt;br/&gt;Space: O(min(chunk_count, max_chunks)) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#memory-monitoring-integration","title":"Memory Monitoring Integration","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Tracemalloc\n    participant Psutil\n    participant Process\n    participant Profile\n    participant Accumulator\n\n    User-&gt;&gt;Tracemalloc: start()\n    Note over Tracemalloc: O(1) - Start memory tracking\n\n    User-&gt;&gt;Psutil: Process(os.getpid())\n    Note over Psutil: O(1) - Get process handle\n\n    User-&gt;&gt;Process: memory_info().rss\n    Note over Process: O(1) - Get initial memory\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(n) - Process data\n\n    loop During processing\n        Profile-&gt;&gt;Accumulator: update(chunk)\n        Note over Accumulator: O(n) - Process chunk\n\n        User-&gt;&gt;Process: memory_info().rss\n        Note over Process: O(1) - Monitor memory\n\n        User-&gt;&gt;Tracemalloc: get_traced_memory()\n        Note over Tracemalloc: O(1) - Get traced memory\n    end\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results\n\n    User-&gt;&gt;Process: memory_info().rss\n    Note over Process: O(1) - Get final memory\n\n    User-&gt;&gt;Tracemalloc: get_traced_memory()\n    Note over Tracemalloc: O(1) - Get peak memory\n\n    User-&gt;&gt;Tracemalloc: stop()\n    Note over Tracemalloc: O(1) - Stop tracking</code></pre>"},{"location":"sequence-diagrams-complexity/#complexity-summary","title":"Complexity Summary","text":""},{"location":"sequence-diagrams-complexity/#time-complexity","title":"Time Complexity","text":"<ul> <li>Per Element: O(1) for basic operations, O(log k) for heap operations</li> <li>Per Chunk: O(n) where n is chunk size</li> <li>Total: O(N) where N is total dataset size</li> </ul>"},{"location":"sequence-diagrams-complexity/#space-complexity","title":"Space Complexity","text":"<ul> <li>KMV: O(k) bounded (was O(n) unbounded)</li> <li>ExtremeTracker: O(k) bounded (was O(k \u00d7 chunks) temporary)</li> <li>Chunk Metadata: O(min(num_chunks, max_chunks)) bounded (was O(num_chunks) unbounded)</li> <li>Total: O(k + s + c) where k=sketch_size, s=sample_size, c=max_chunks</li> </ul>"},{"location":"sequence-diagrams-complexity/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Before Fixes: O(n) growth for low-cardinality columns</li> <li>After Fixes: O(1) constant growth</li> <li>Memory per Row: &lt;1KB (typically &lt;0.1KB)</li> <li>Peak Memory: &lt;200MB for 1M rows</li> </ul> <p>The memory leak fixes successfully transform PySuricata from a memory-intensive system to a truly streaming system with bounded memory usage.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#basic","title":"Basic","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nrep = profile(df)\nrep.save_html(\"report.html\")\n</code></pre>"},{"location":"usage/#also-save-stats-as-json","title":"Also save stats as JSON","text":"<pre><code>from pysuricata import profile\n\nrep = profile(df)\nrep.save_json(\"report.json\")\n</code></pre>"},{"location":"usage/#streaming-large-in-memory-data","title":"Streaming large in-memory data","text":"<pre><code>from pysuricata import profile, ReportConfig\nimport pandas as pd\n\ncfg = ReportConfig()\n\n# From an iterable/generator yielding pandas DataFrame chunks\ndef chunk_iter():\n    for i in range(10):\n        yield pd.read_csv(f\"/data/part-{i}.csv\")  # you pre-chunk externally\n\nrep = profile((ch for ch in chunk_iter()), config=cfg)\nrep.save_html(\"report.html\")\n</code></pre>"},{"location":"usage/#streaming-polars","title":"Streaming polars","text":"<pre><code>import polars as pl\nfrom pysuricata import profile\n\ndf = pl.read_parquet(\"/data/big.parquet\")\nrep = profile(df)  # eager or LazyFrame supported\nrep.save_html(\"report.html\")\n</code></pre>"},{"location":"usage/#streaming-with-polars-iterables-and-lazyframe","title":"Streaming with polars iterables and LazyFrame","text":"<p>Keep Polars end\u2011to\u2011end. The engine consumes either Pandas or Polars chunks.</p> <p>Iterable of Polars DataFrames:</p> <pre><code>import polars as pl\nfrom pysuricata import profile, ReportConfig\n\ndf = pl.DataFrame({\n    \"a\": list(range(100_000)),\n    \"b\": [float(i) if i % 5 else None for i in range(100_000)],\n})\n\nstep = 20_000\nchunks = (df.slice(i, min(step, df.height - i)) for i in range(0, df.height, step))\nrep = profile(chunks, config=ReportConfig())\nrep.save_html(\"polars_iterable_report.html\")\n</code></pre> <p>Polars LazyFrame (windowed collect under the hood):</p> <pre><code>import polars as pl\nfrom pysuricata import profile, ReportConfig, ComputeOptions\n\nlf = (\n    pl.LazyFrame({\n        \"x\": list(range(200_000)),\n        \"y\": [float(i) if i % 7 else None for i in range(200_000)],\n        \"z\": [\"a\" if i % 2 else \"b\" for i in range(200_000)],\n    })\n    .with_columns(pl.col(\"x\") * 2)\n)\n\ncfg = ReportConfig(compute=ComputeOptions(chunk_size=50_000))\nrep = profile(lf, config=cfg)\nrep.save_html(\"polars_lazy_report.html\")\n</code></pre>"},{"location":"usage/#deterministic-visuals-reproducible-sampling","title":"Deterministic visuals (reproducible sampling)","text":"<p>Use <code>random_seed</code> to make histogram sampling deterministic across runs.</p> <pre><code>from pysuricata import profile, ReportConfig\n\ncfg = ReportConfig()\ncfg.compute.random_seed = 42\nrep = profile(df, config=cfg)\n</code></pre>"},{"location":"usage/#programmatic-summary","title":"Programmatic summary","text":"<p>Ask for a compact JSON-like dictionary of stats:</p> <pre><code>from pysuricata import summarize\nsummary = summarize(df)\nprint(summary[\"dataset\"])           # rows_est, cols, missing_cells, duplicates, top-missing\nprint(summary[\"columns\"][\"amount\"]) # per-column stats by type\n</code></pre>"},{"location":"usage/#processed-bytes-and-timing","title":"Processed bytes and timing","text":"<p>The report displays: - Processed bytes (\u2248): total bytes handled across chunks (not peak RSS) - Precise generation time in seconds (e.g., 0.02s)</p>"},{"location":"usage/#end-to-end-minimal-example","title":"End-to-end minimal example","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndf = pd.DataFrame(\n    {\n        \"amount\": [1.0, 2.5, None, 4.0, 5.5],\n        \"country\": [\"US\", \"US\", \"DE\", None, \"FR\"],\n        \"ts\": pd.to_datetime([\"2021-01-01\", \"2021-01-02\", None, \"2021-01-04\", \"2021-01-05\"]),\n        \"flag\": [True, False, True, None, False],\n    }\n)\n\ncfg = ReportConfig()\ncfg.compute.random_seed = 0\nrep = profile(df, config=cfg)\nrep.save_html(\"report.html\")\n</code></pre>"},{"location":"why-pysuricata/","title":"Why PySuricata?","text":"<p>PySuricata is a lightweight, high-performance Python library for exploratory data analysis (EDA) that generates self-contained HTML reports. Built on cutting-edge streaming algorithms, it's designed to handle datasets of any size with minimal memory footprint and maximum accuracy.</p>"},{"location":"why-pysuricata/#key-advantages","title":"Key Advantages","text":""},{"location":"why-pysuricata/#true-streaming-architecture","title":"\ud83d\ude80 True Streaming Architecture","text":"<p>Unlike competitors that load entire datasets into memory, PySuricata uses streaming algorithms that process data in bounded memory:</p> <ul> <li>Memory-efficient: Process datasets larger than RAM with O(1) or O(log n) memory per column</li> <li>Constant updates: O(1) amortized time per value using Welford/P\u00e9bay algorithms</li> <li>Mergeable state: Combine results from multiple chunks/threads/nodes exactly</li> </ul>"},{"location":"why-pysuricata/#performance-comparison","title":"\u26a1 Performance Comparison","text":"Library Memory (1M+ rows) Streaming Dependencies pysuricata ~50 MB (constant) \u2705 Yes pandas only pandas-profiling 1.2+ GB \u274c No 20+ packages ydata-profiling 1.2+ GB \u274c No 25+ packages sweetviz 1.1+ GB \u274c No 15+ packages pandas-eda 1.0+ GB \u274c No 10+ packages <p>Key Advantage</p> <p>PySuricata maintains ~50MB memory usage regardless of dataset size, while competitors require memory proportional to data.</p>"},{"location":"why-pysuricata/#minimal-dependencies","title":"\ud83d\udce6 Minimal Dependencies","text":"<p>PySuricata core dependencies: - pandas (or polars) - markdown - Built-in Python stdlib</p> <p>Total installed size: ~10 MB</p> <p>Competitors: - pandas-profiling/ydata-profiling: 100+ MB (includes scipy, matplotlib, seaborn, etc.) - sweetviz: 80+ MB (includes matplotlib, scipy, statsmodels)</p>"},{"location":"why-pysuricata/#mathematical-accuracy","title":"\ud83c\udfaf Mathematical Accuracy","text":"<p>PySuricata implements proven algorithms with mathematical guarantees:</p>"},{"location":"why-pysuricata/#exact-statistics-welfordpebay","title":"Exact Statistics (Welford/P\u00e9bay)","text":"<ul> <li>Mean, variance, skewness, kurtosis computed exactly</li> <li>Numerically stable (avoids catastrophic cancellation)</li> <li>Exactly mergeable across chunks</li> </ul>"},{"location":"why-pysuricata/#approximate-statistics-probabilistic-data-structures","title":"Approximate Statistics (Probabilistic Data Structures)","text":"<ul> <li>KMV sketch for distinct counts: error \u03b5 \u2248 1/\u221ak</li> <li>Misra-Gries for top-k: frequency within n/k</li> <li>Reservoir sampling: uniform probability guarantees</li> </ul> <p>All approximations are unbiased and come with error bounds.</p>"},{"location":"why-pysuricata/#framework-flexibility","title":"\ud83d\udd04 Framework Flexibility","text":"<p>Unified API for multiple dataframe libraries:</p> PandasPolarsStreaming <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\ndf = pl.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndef chunk_generator():\n    for i in range(10):\n        yield pd.read_csv(f\"part-{i}.csv\")\n\nreport = profile(chunk_generator())\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"why-pysuricata/#portable-reports","title":"\ud83d\udcc4 Portable Reports","text":"<p>Reports are single HTML files with:</p> <ul> <li>Inline CSS and JavaScript (no external dependencies)</li> <li>Inline SVG charts (no image files)</li> <li>Base64-encoded logo (completely self-contained)</li> <li>Shareable via email, cloud storage, or static hosting</li> </ul> <p>Competitors often require: - Separate CSS/JS files - Image directories - External CDN links (breaks without internet)</p>"},{"location":"why-pysuricata/#deep-customization","title":"\u2699\ufe0f Deep Customization","text":"<p>Extensive configuration without code modification:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.chunk_size = 250_000\nconfig.compute.numeric_sample_size = 50_000\nconfig.compute.uniques_sketch_size = 4096\nconfig.compute.top_k_size = 100\nconfig.compute.random_seed = 42  # Deterministic sampling\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"why-pysuricata/#comprehensive-analysis","title":"\ud83d\udcca Comprehensive Analysis","text":"<p>PySuricata analyzes four variable types with specialized algorithms:</p>"},{"location":"why-pysuricata/#numeric-variables","title":"Numeric Variables","text":"<ul> <li>Moments (mean, variance, skewness, kurtosis)</li> <li>Quantiles (exact or KLL/t-digest)</li> <li>Outliers (IQR, MAD, z-score)</li> <li>Histograms (Freedman-Diaconis binning)</li> <li>Streaming correlations</li> </ul>"},{"location":"why-pysuricata/#categorical-variables","title":"Categorical Variables","text":"<ul> <li>Top-k values (Misra-Gries)</li> <li>Distinct count (KMV sketch)</li> <li>Entropy, Gini impurity</li> <li>String length statistics</li> <li>Case/trim variants</li> </ul>"},{"location":"why-pysuricata/#datetime-variables","title":"DateTime Variables","text":"<ul> <li>Temporal range and coverage</li> <li>Hour/day-of-week/month distributions</li> <li>Monotonicity detection</li> <li>Gap analysis</li> <li>Timeline visualizations</li> </ul>"},{"location":"why-pysuricata/#boolean-variables","title":"Boolean Variables","text":"<ul> <li>True/false ratios</li> <li>Entropy calculation</li> <li>Imbalance detection</li> <li>Balance scores</li> </ul>"},{"location":"why-pysuricata/#detailed-comparisons","title":"Detailed Comparisons","text":""},{"location":"why-pysuricata/#vs-pandas-profiling-ydata-profiling","title":"vs. pandas-profiling / ydata-profiling","text":"Feature PySuricata pandas-profiling Memory model Streaming (bounded) In-memory (full dataset) Large datasets \u2705 GB to TB \u274c Limited by RAM Speed Fast (O(n) single pass) Slow (multiple passes) Dependencies Minimal (~10 MB) Heavy (100+ MB) Report format Single HTML HTML + assets Polars support \u2705 Native \u274c Convert required Exact algorithms Welford/P\u00e9bay NumPy/SciPy Configurability High (all parameters) Medium Reproducible \u2705 Seeded sampling Partial <p>When to use pandas-profiling: - Small datasets (&lt; 100 MB) - Need interactive widgets - Want correlation heatmaps with color scales</p> <p>When to use PySuricata: - Large datasets (&gt; 1 GB) - Memory-constrained environments - Production pipelines (reproducibility) - Need portable reports - Streaming data sources</p>"},{"location":"why-pysuricata/#vs-sweetviz","title":"vs. sweetviz","text":"Feature PySuricata Sweetviz Dataset comparison Single dataset Compare two datasets Memory efficiency \u2705 Streaming \u274c In-memory Visualizations SVG (lightweight) Matplotlib (heavy) Statistical depth High (full moments) Medium Speed Fast Medium Customization High Low <p>When to use Sweetviz: - Comparing train/test splits - Need target variable analysis - Visual comparison is primary goal</p> <p>When to use PySuricata: - Single dataset profiling - Large datasets - Need mathematical rigor - Production deployment</p>"},{"location":"why-pysuricata/#vs-pandas-eda","title":"vs. pandas-eda","text":"Feature PySuricata pandas-eda Scope Full EDA Basic statistics Missing values Advanced (chunk dist.) Basic summary Algorithm sophistication High (sketches) Low (exact only) Large datasets \u2705 Streaming \u274c In-memory Documentation Comprehensive Minimal"},{"location":"why-pysuricata/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"why-pysuricata/#1-data-quality-monitoring-production","title":"1. Data Quality Monitoring (Production)","text":"<pre><code>from pysuricata import summarize\n\n# In your data pipeline\nstats = summarize(df)\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"why-pysuricata/#2-large-dataset-profiling-research","title":"2. Large Dataset Profiling (Research)","text":"<pre><code># Profile 10GB dataset in bounded memory\ndef read_large_dataset():\n    for part in range(100):\n        yield pd.read_parquet(f\"data/part-{part}.parquet\")\n\nreport = profile(read_large_dataset())\nreport.save_html(\"large_dataset_report.html\")\n</code></pre>"},{"location":"why-pysuricata/#3-reproducible-reports-ml-pipelines","title":"3. Reproducible Reports (ML Pipelines)","text":"<pre><code># Deterministic sampling for CI/CD\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n# Same report on every run\n</code></pre>"},{"location":"why-pysuricata/#4-memory-constrained-environments-edgeiot","title":"4. Memory-Constrained Environments (Edge/IoT)","text":"<pre><code># Profile on device with 512 MB RAM\nconfig = ReportConfig()\nconfig.compute.chunk_size = 10_000\nconfig.compute.numeric_sample_size = 5_000\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"why-pysuricata/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"why-pysuricata/#memory-usage","title":"Memory Usage","text":"<pre><code>graph LR\n    A[Dataset Size] --&gt; B[1 GB]\n    A --&gt; C[10 GB]\n    A --&gt; D[100 GB]\n    B --&gt; E[PySuricata: 50 MB]\n    B --&gt; F[pandas-profiling: 1.2 GB]\n    C --&gt; G[PySuricata: 50 MB]\n    C --&gt; H[pandas-profiling: OOM]\n    D --&gt; I[PySuricata: 50 MB]\n    D --&gt; J[pandas-profiling: OOM]</code></pre>"},{"location":"why-pysuricata/#processing-time","title":"Processing Time","text":"<p>Measured on Apple Silicon (M-series) with Python 3.13:</p> Dataset Size Processing Time Throughput 10K rows ~3s ~3,000 rows/s 100K rows ~13s ~8,000 rows/s 1M rows ~3 min ~5,500 rows/s 10M rows ~30 min ~5,500 rows/s"},{"location":"why-pysuricata/#scalability","title":"Scalability","text":"<p>PySuricata scales linearly with dataset size with constant memory: - 1M rows \u2192 ~3 min, 50 MB - 10M rows \u2192 ~30 min, 50 MB - 100M rows \u2192 ~5 hours, 50 MB</p> <p>Competitors fail (OOM) on large datasets while PySuricata handles them in bounded memory.</p>"},{"location":"why-pysuricata/#algorithm-innovation","title":"Algorithm Innovation","text":""},{"location":"why-pysuricata/#streaming-moments-welford-pebay","title":"Streaming Moments (Welford + P\u00e9bay)","text":"<p>Update complexity: O(1) per value Space complexity: O(1) per column Numerical stability: Excellent (no catastrophic cancellation)</p> <pre><code># Exact mean/variance in single pass\nfor value in stream:\n    n += 1\n    delta = value - mean\n    mean += delta / n\n    M2 += delta * (value - mean)\nvariance = M2 / (n - 1)\n</code></pre>"},{"location":"why-pysuricata/#k-minimum-values-kmv-for-cardinality","title":"K-Minimum Values (KMV) for Cardinality","text":"<p>Update complexity: O(log k) per value Space complexity: O(k) per column Error bound: \u03b5 \u2248 1/\u221ak Unbiased: E[estimate] = true_cardinality</p> <pre><code># Estimate distinct count with k=2048\nn_distinct \u2248 (k - 1) / kth_smallest_hash\n# Error: ~2.2% (95% confidence)\n</code></pre>"},{"location":"why-pysuricata/#misra-gries-for-top-k","title":"Misra-Gries for Top-K","text":"<p>Update complexity: O(k) per value (amortized O(1)) Space complexity: O(k) per column Accuracy: Frequency estimate within n/k Guaranteed: All items with freq &gt; n/k found</p>"},{"location":"why-pysuricata/#community-ecosystem","title":"Community &amp; Ecosystem","text":""},{"location":"why-pysuricata/#active-development","title":"Active Development","text":"<ul> <li>Regular releases on PyPI</li> <li>CI/CD with 90%+ test coverage</li> <li>Comprehensive documentation</li> <li>Responsive issue tracking</li> </ul>"},{"location":"why-pysuricata/#integrations","title":"Integrations","text":"<ul> <li>Works with pandas, polars</li> <li>Compatible with Jupyter notebooks</li> <li>Integrates with MLflow, Weights &amp; Biases</li> <li>Exports JSON for custom processing</li> </ul>"},{"location":"why-pysuricata/#support","title":"Support","text":"<ul> <li>Detailed documentation with examples</li> <li>Mathematical references for algorithms</li> <li>Active GitHub discussions</li> <li>Community contributions welcome</li> </ul>"},{"location":"why-pysuricata/#getting-started","title":"Getting Started","text":"<p>Install via pip:</p> <pre><code>pip install pysuricata\n</code></pre> <p>Generate your first report:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"your_data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"why-pysuricata/#learn-more","title":"Learn More","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Configuration - Customize every aspect</li> <li>Performance Tips - Optimize for your use case</li> <li>Statistical Methods - Understand the algorithms</li> <li>API Reference - Complete function documentation</li> </ul>"},{"location":"why-pysuricata/#conclusion","title":"Conclusion","text":"<p>Choose PySuricata when you need:</p> <p>\u2705 Memory efficiency for large datasets \u2705 Proven algorithms with mathematical guarantees \u2705 Fast, single-pass processing \u2705 Portable, self-contained reports \u2705 Minimal dependencies \u2705 Framework flexibility (pandas/polars) \u2705 Production-ready reliability \u2705 Deep customization  </p> <p>Choose competitors when you need:</p> <ul> <li>Interactive widgets (pandas-profiling)</li> <li>Dataset comparison views (sweetviz)</li> <li>Correlation heatmaps with color scales</li> <li>Small datasets only</li> </ul> <p>Ready to profile your data? Get started \u2192</p>"},{"location":"algorithms/sampling/","title":"Reservoir Sampling","text":"<p>Maintain uniform random sample of fixed size \\(k\\) from stream of unknown length.</p>"},{"location":"algorithms/sampling/#algorithm-r-vitter","title":"Algorithm R (Vitter)","text":"<pre><code>class ReservoirSampler:\n    def __init__(self, k):\n        self.k = k\n        self.reservoir = []\n        self.n = 0\n\n    def add(self, item):\n        self.n += 1\n        if len(self.reservoir) &lt; self.k:\n            self.reservoir.append(item)\n        else:\n            j = random.randint(0, self.n - 1)\n            if j &lt; self.k:\n                self.reservoir[j] = item\n</code></pre>"},{"location":"algorithms/sampling/#guarantee","title":"Guarantee","text":"<p>Every element has exactly probability \\(k/n\\) of being in sample.</p>"},{"location":"algorithms/sampling/#proof-sketch","title":"Proof Sketch","text":"<p>Element \\(i\\) enters reservoir with probability \\(\\min(1, k/i)\\).</p> <p>It survives subsequent updates:</p> \\[ P(\\text{survive}) = \\prod_{j=i+1}^{n} \\left(1 - \\frac{1}{j}\\right) = \\frac{i}{n} \\] <p>Total probability:</p> \\[ P(\\text{in sample}) = \\frac{k}{i} \\cdot \\frac{i}{n} = \\frac{k}{n} \\]"},{"location":"algorithms/sampling/#complexity","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Time per element: O(1) amortized</li> <li>Uniform guarantee: Exact</li> </ul>"},{"location":"algorithms/sampling/#use-cases","title":"Use Cases","text":"<ul> <li>Quantile estimation (sort sample)</li> <li>Histogram construction</li> <li>Representative sampling</li> </ul>"},{"location":"algorithms/sampling/#see-also","title":"See Also","text":"<ul> <li>Sketch Algorithms - Other streaming algorithms</li> <li>Numeric Analysis - Using reservoir for quantiles</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"algorithms/sketches/","title":"Sketch Algorithms","text":"<p>Sketch algorithms (probabilistic data structures) enable approximate analytics on massive streams with bounded memory and mathematical error guarantees.</p>"},{"location":"algorithms/sketches/#overview","title":"Overview","text":"<p>Sketches trade perfect accuracy for: - Constant memory: Independent of dataset size - Fast updates: O(1) or O(log k) per element - Mergeability: Combine sketches from parallel streams - Mathematical guarantees: Provable error bounds</p>"},{"location":"algorithms/sketches/#k-minimum-values-kmv","title":"K-Minimum Values (KMV)","text":""},{"location":"algorithms/sketches/#purpose","title":"Purpose","text":"<p>Estimate distinct count (cardinality) of a set.</p>"},{"location":"algorithms/sketches/#algorithm","title":"Algorithm","text":"<ol> <li>Hash each element to [0,1]: \\(h(x) \\sim \\text{Uniform}(0,1)\\)</li> <li>Keep the \\(k\\) smallest hash values</li> <li>Estimate: \\(\\hat{d} = \\frac{k-1}{x_k}\\) where \\(x_k\\) is the \\(k\\)-th smallest hash</li> </ol>"},{"location":"algorithms/sketches/#error-bound","title":"Error Bound","text":"\\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Example: \\(k=2048\\) \u2192 ~2.2% error at 95% confidence</p>"},{"location":"algorithms/sketches/#implementation","title":"Implementation","text":"<pre><code>import heapq, hashlib\n\nclass KMV:\n    def __init__(self, k):\n        self.k = k\n        self.heap = []  # Max-heap of k smallest hashes\n\n    def add(self, value):\n        h = self._hash(value)\n        if len(self.heap) &lt; self.k:\n            heapq.heappush(self.heap, -h)  # Negative for max-heap\n        elif h &lt; -self.heap[0]:\n            heapq.heapreplace(self.heap, -h)\n\n    def estimate(self):\n        if len(self.heap) == 0:\n            return 0\n        if len(self.heap) &lt; self.k:\n            return len(self.heap)\n        kth_min = -self.heap[0]\n        return (self.k - 1) / kth_min if kth_min &gt; 0 else float('inf')\n\n    def _hash(self, value):\n        return int(hashlib.md5(str(value).encode()).hexdigest(), 16) / (2**128)\n</code></pre>"},{"location":"algorithms/sketches/#mergeability","title":"Mergeability","text":"<p>Union of two KMV sketches: merge heaps and keep k smallest.</p> <pre><code>def merge(kmv1, kmv2):\n    combined = KMV(kmv1.k)\n    for h in kmv1.heap + kmv2.heap:\n        combined.heap.append(h)\n    combined.heap = heapq.nlargest(combined.k, combined.heap)\n    heapq.heapify(combined.heap)\n    return combined\n</code></pre>"},{"location":"algorithms/sketches/#misra-gries-algorithm","title":"Misra-Gries Algorithm","text":""},{"location":"algorithms/sketches/#purpose_1","title":"Purpose","text":"<p>Find top-k most frequent items (heavy hitters).</p>"},{"location":"algorithms/sketches/#algorithm_1","title":"Algorithm","text":"<ol> <li>Maintain dictionary of \u2264k items with counts</li> <li>For each new item:</li> <li>If in dictionary: increment count</li> <li>Else if dictionary not full: add with count 1</li> <li>Else: decrement all counts, remove zeros</li> <li>Output: items remaining in dictionary</li> </ol>"},{"location":"algorithms/sketches/#guarantee","title":"Guarantee","text":"<p>For any item with true frequency \\(f &gt; n/k\\): - Guaranteed to appear in output - Estimated frequency within \\(\\pm n/k\\) of true frequency</p>"},{"location":"algorithms/sketches/#implementation_1","title":"Implementation","text":"<pre><code>class MisraGries:\n    def __init__(self, k):\n        self.k = k\n        self.counts = {}\n\n    def add(self, item):\n        if item in self.counts:\n            self.counts[item] += 1\n        elif len(self.counts) &lt; self.k:\n            self.counts[item] = 1\n        else:\n            # Decrement all counts\n            to_remove = []\n            for key in self.counts:\n                self.counts[key] -= 1\n                if self.counts[key] == 0:\n                    to_remove.append(key)\n            for key in to_remove:\n                del self.counts[key]\n\n    def top_k(self):\n        return sorted(self.counts.items(), key=lambda x: -x[1])\n</code></pre>"},{"location":"algorithms/sketches/#complexity","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Update: O(k) worst-case, O(1) amortized</li> <li>Output: O(k log k) for sorting</li> </ul>"},{"location":"algorithms/sketches/#mergeability_1","title":"Mergeability","text":"<p>Sum counts from multiple Misra-Gries structures.</p>"},{"location":"algorithms/sketches/#hyperloglog-hll","title":"HyperLogLog (HLL)","text":""},{"location":"algorithms/sketches/#purpose_2","title":"Purpose","text":"<p>Estimate distinct count with very low memory.</p>"},{"location":"algorithms/sketches/#key-idea","title":"Key Idea","text":"<p>Count leading zeros in binary representation of hashes.</p> <p>Intuition: If max leading zeros = \\(m\\), roughly \\(2^m\\) distinct elements seen.</p>"},{"location":"algorithms/sketches/#algorithm_2","title":"Algorithm","text":"<ol> <li>Hash elements to binary strings</li> <li>Split hash into \\(2^b\\) buckets (first \\(b\\) bits)</li> <li>Track max leading zeros per bucket</li> <li>Combine with harmonic mean</li> </ol>"},{"location":"algorithms/sketches/#estimator","title":"Estimator","text":"\\[ \\hat{d} = \\alpha_m \\cdot m^2 \\cdot \\left(\\sum_{j=1}^{m} 2^{-M_j}\\right)^{-1} \\] <p>where: - \\(m = 2^b\\) = number of buckets - \\(M_j\\) = max leading zeros in bucket \\(j\\) - \\(\\alpha_m\\) = bias correction constant</p>"},{"location":"algorithms/sketches/#error","title":"Error","text":"\\[ \\text{Relative standard error} \\approx \\frac{1.04}{\\sqrt{m}} \\] <p>Example: \\(m=1024\\) (10 KB) \u2192 ~3% error</p>"},{"location":"algorithms/sketches/#properties","title":"Properties","text":"<ul> <li>Space: \\(O(m \\log \\log n)\\) bits</li> <li>Mergeable: Element-wise max of bucket values</li> <li>Production-ready: Used in Redis, BigQuery</li> </ul> <p>Not implemented in current version</p> <p>PySuricata uses KMV instead of HLL. HLL may be added in future for even lower memory.</p>"},{"location":"algorithms/sketches/#reservoir-sampling","title":"Reservoir Sampling","text":""},{"location":"algorithms/sketches/#purpose_3","title":"Purpose","text":"<p>Maintain uniform random sample of fixed size \\(k\\) from stream.</p>"},{"location":"algorithms/sketches/#algorithm-algorithm-r","title":"Algorithm (Algorithm R)","text":"<pre><code>import random\n\nclass ReservoirSampler:\n    def __init__(self, k):\n        self.k = k\n        self.reservoir = []\n        self.n = 0\n\n    def add(self, item):\n        self.n += 1\n        if len(self.reservoir) &lt; self.k:\n            self.reservoir.append(item)\n        else:\n            j = random.randint(0, self.n - 1)\n            if j &lt; self.k:\n                self.reservoir[j] = item\n\n    def get_sample(self):\n        return self.reservoir.copy()\n</code></pre>"},{"location":"algorithms/sketches/#guarantee_1","title":"Guarantee","text":"<p>Every element has exactly probability \\(k/n\\) of being in the sample.</p> <p>Proof sketch: - Element \\(i\\) enters reservoir with probability \\(\\min(1, k/i)\\) - Survives subsequent updates with probability \\(\\prod_{j=i+1}^{n} (1 - 1/j)\\) - Total: \\(k/n\\)</p>"},{"location":"algorithms/sketches/#complexity_1","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Update: O(1)</li> <li>Uniform guarantee: Exact</li> </ul>"},{"location":"algorithms/sketches/#use-cases","title":"Use Cases","text":"<ul> <li>Quantile estimation (sort sample)</li> <li>Histogram construction</li> <li>Outlier detection</li> </ul>"},{"location":"algorithms/sketches/#bloom-filters","title":"Bloom Filters","text":""},{"location":"algorithms/sketches/#purpose_4","title":"Purpose","text":"<p>Test set membership with false positives, no false negatives.</p>"},{"location":"algorithms/sketches/#algorithm_3","title":"Algorithm","text":"<ol> <li>Initialize bit array of size \\(m\\) to 0</li> <li>Use \\(k\\) hash functions</li> <li>Add: set bits at \\(h_1(x), h_2(x), \\ldots, h_k(x)\\) to 1</li> <li>Query: check if all \\(k\\) bits are 1</li> </ol>"},{"location":"algorithms/sketches/#false-positive-rate","title":"False Positive Rate","text":"\\[ P_{\\text{fp}} \\approx \\left(1 - e^{-kn/m}\\right)^k \\]"},{"location":"algorithms/sketches/#optimal-parameters","title":"Optimal Parameters","text":"<p>For desired \\(P_{\\text{fp}}\\) and \\(n\\) elements:</p> \\[ \\begin{aligned} m &amp;= -\\frac{n \\ln P_{\\text{fp}}}{(\\ln 2)^2} \\\\ k &amp;= \\frac{m}{n} \\ln 2 \\end{aligned} \\] <p>Example: \\(n=10^6\\), \\(P_{\\text{fp}}=0.01\\) \u2192 \\(m \\approx 9.6\\) Mb, \\(k=7\\)</p> <p>Not implemented in current version</p> <p>Bloom filters are not currently used in PySuricata but may be added for duplicate detection optimization.</p>"},{"location":"algorithms/sketches/#count-min-sketch","title":"Count-Min Sketch","text":""},{"location":"algorithms/sketches/#purpose_5","title":"Purpose","text":"<p>Estimate frequencies of items in stream.</p>"},{"location":"algorithms/sketches/#algorithm_4","title":"Algorithm","text":"<ol> <li>Initialize \\(d \\times w\\) matrix of counters to 0</li> <li>Use \\(d\\) hash functions mapping to \\([0, w)\\)</li> <li>Add item: increment counters at \\(M[i, h_i(x)]\\) for \\(i=1,\\ldots,d\\)</li> <li>Estimate frequency: \\(\\hat{f}(x) = \\min_i M[i, h_i(x)]\\)</li> </ol>"},{"location":"algorithms/sketches/#error-bound_1","title":"Error Bound","text":"<p>With probability \\(1-\\delta\\):</p> \\[ \\hat{f}(x) \\le f(x) + \\epsilon n \\] <p>where \\(w = \\lceil e / \\epsilon \\rceil\\) and \\(d = \\lceil \\ln(1/\\delta) \\rceil\\).</p>"},{"location":"algorithms/sketches/#comparison-to-misra-gries","title":"Comparison to Misra-Gries","text":"<ul> <li>Count-Min: Point queries, any item</li> <li>Misra-Gries: Top-k queries, only frequent items</li> </ul> <p>Not implemented in current version</p> <p>PySuricata uses Misra-Gries for top-k. Count-Min Sketch may be added for full frequency queries.</p>"},{"location":"algorithms/sketches/#choosing-algorithms","title":"Choosing Algorithms","text":"Need Algorithm Memory Error Distinct count KMV O(k) \\(1/\\sqrt{k}\\) Distinct count (min memory) HyperLogLog O(m log log n) \\(1/\\sqrt{m}\\) Top-k items Misra-Gries O(k) \\(n/k\\) frequency Top-k items (point queries) Count-Min O(dw) \\(\\epsilon n\\) Uniform sample Reservoir O(k) Exact Membership test Bloom filter O(m) \\(P_{\\text{fp}}\\)"},{"location":"algorithms/sketches/#implementation-in-pysuricata","title":"Implementation in PySuricata","text":""},{"location":"algorithms/sketches/#numericaccumulator","title":"NumericAccumulator","text":"<pre><code>self._uniques = KMV(config.uniques_sketch_size)  # Distinct count\nself._sample = ReservoirSampler(config.sample_size)  # Quantiles\n</code></pre>"},{"location":"algorithms/sketches/#categoricalaccumulator","title":"CategoricalAccumulator","text":"<pre><code>self._topk = MisraGries(config.top_k_size)  # Top values\nself._uniques = KMV(config.uniques_sketch_size)  # Distinct count\n</code></pre>"},{"location":"algorithms/sketches/#references","title":"References","text":"<ol> <li> <p>Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p> </li> <li> <p>Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p> </li> <li> <p>Flajolet, P. et al. (2007), \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\", DMTCS, AH: 137\u2013156.</p> </li> <li> <p>Vitter, J.S. (1985), \"Random Sampling with a Reservoir\", ACM TOMS, 11(1): 37\u201357.</p> </li> <li> <p>Bloom, B.H. (1970), \"Space/time trade-offs in hash coding with allowable errors\", CACM, 13(7): 422\u2013426.</p> </li> <li> <p>Cormode, G., Muthukrishnan, S. (2005), \"An Improved Data Stream Summary: The Count-Min Sketch and its Applications\", Journal of Algorithms, 55(1): 58\u201375.</p> </li> </ol>"},{"location":"algorithms/sketches/#see-also","title":"See Also","text":"<ul> <li>Streaming Algorithms - Welford/P\u00e9bay moments</li> <li>Numeric Analysis - Using KMV and reservoir</li> <li>Categorical Analysis - Using Misra-Gries</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"algorithms/streaming/","title":"Streaming Statistics Algorithms","text":"<p>Deep dive into the streaming algorithms that power PySuricata's memory-efficient statistics computation.</p>"},{"location":"algorithms/streaming/#overview","title":"Overview","text":"<p>Streaming algorithms compute statistics in single-pass, constant-memory mode, enabling analysis of datasets larger than RAM.</p>"},{"location":"algorithms/streaming/#key-algorithms","title":"Key Algorithms","text":"<ul> <li>Welford's algorithm: Online mean and variance</li> <li>P\u00e9bay's formulas: Parallel merging of moments</li> <li>Higher moments: Skewness and kurtosis extension</li> <li>Numerical stability: Avoiding catastrophic cancellation</li> </ul>"},{"location":"algorithms/streaming/#welfords-online-algorithm","title":"Welford's Online Algorithm","text":""},{"location":"algorithms/streaming/#the-problem","title":"The Problem","text":"<p>Naive variance formula:</p> \\[ s^2 = \\frac{1}{n-1}\\left(\\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}\\right) \\] <p>Issues: - Requires two passes (one for \\(\\sum x_i\\), one for \\(\\sum x_i^2\\)) - Catastrophic cancellation if \\(\\sum x_i^2 \\approx (\\sum x_i)^2 / n\\) - Poor numerical stability</p>"},{"location":"algorithms/streaming/#welfords-solution","title":"Welford's Solution","text":"<p>State variables: - \\(n\\): count - \\(\\mu\\): running mean - \\(M_2\\): sum of squared deviations from current mean</p> <p>Update formulas:</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\mu &amp;\\leftarrow \\mu + \\frac{\\delta}{n} \\\\ \\delta_2 &amp;= x - \\mu_{\\text{new}} \\\\ M_2 &amp;\\leftarrow M_2 + \\delta \\cdot \\delta_2 \\end{aligned} \\] <p>Finalize:</p> \\[ \\text{variance} = \\frac{M_2}{n-1} \\]"},{"location":"algorithms/streaming/#derivation","title":"Derivation","text":"<p>Starting from the definition:</p> \\[ M_2^{(n)} = \\sum_{i=1}^{n} (x_i - \\mu^{(n)})^2 \\] <p>After adding \\(x_{n+1}\\):</p> \\[ M_2^{(n+1)} = \\sum_{i=1}^{n+1} (x_i - \\mu^{(n+1)})^2 \\] <p>Expand using the mean update:</p> \\[ \\mu^{(n+1)} = \\mu^{(n)} + \\frac{x_{n+1} - \\mu^{(n)}}{n+1} = \\mu^{(n)} + \\frac{\\delta}{n+1} \\] <p>After algebraic manipulation (see Welford 1962):</p> \\[ M_2^{(n+1)} = M_2^{(n)} + \\delta \\cdot (x_{n+1} - \\mu^{(n+1)}) \\] <p>Key insight: Update uses both old and new mean, providing numerical stability.</p>"},{"location":"algorithms/streaming/#pseudocode","title":"Pseudocode","text":"<pre><code>def welford_update(n, mean, M2, x):\n    \"\"\"Update running moments with new value x\"\"\"\n    n_new = n + 1\n    delta = x - mean\n    mean_new = mean + delta / n_new\n    delta2 = x - mean_new\n    M2_new = M2 + delta * delta2\n    return n_new, mean_new, M2_new\n\ndef welford_finalize(n, mean, M2):\n    \"\"\"Compute final statistics\"\"\"\n    if n &lt; 2:\n        return mean, None\n    variance = M2 / (n - 1)\n    return mean, variance\n</code></pre>"},{"location":"algorithms/streaming/#properties","title":"Properties","text":"<ol> <li>Single-pass: Only one scan through data</li> <li>Constant memory: O(1) space (3 numbers)</li> <li>Numerically stable: No catastrophic cancellation</li> <li>Exact: Same result as two-pass (up to FP rounding)</li> <li>Online: Can process streaming data</li> </ol>"},{"location":"algorithms/streaming/#pebays-parallel-merge","title":"P\u00e9bay's Parallel Merge","text":""},{"location":"algorithms/streaming/#the-problem_1","title":"The Problem","text":"<p>How to combine partial results from multiple chunks/threads?</p> <p>Given: - State A: \\((n_a, \\mu_a, M_{2a})\\) - State B: \\((n_b, \\mu_b, M_{2b})\\)</p> <p>Want: Combined state \\((n, \\mu, M_2)\\) equivalent to processing all data together.</p>"},{"location":"algorithms/streaming/#pebays-solution","title":"P\u00e9bay's Solution","text":"<p>Combined state:</p> \\[ \\begin{aligned} n &amp;= n_a + n_b \\\\ \\delta &amp;= \\mu_b - \\mu_a \\\\ \\mu &amp;= \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\\\ M_2 &amp;= M_{2a} + M_{2b} + \\delta^2 \\cdot \\frac{n_a n_b}{n} \\end{aligned} \\]"},{"location":"algorithms/streaming/#derivation_1","title":"Derivation","text":"<p>The combined mean is the weighted average:</p> \\[ \\mu = \\frac{n_a \\mu_a + n_b \\mu_b}{n_a + n_b} = \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\] <p>For \\(M_2\\), use the identity:</p> \\[ M_2 = \\sum (x_i - \\mu)^2 = \\sum (x_i - \\mu_a)^2 - n(\\mu - \\mu_a)^2 \\] <p>Applying to both groups and summing:</p> \\[ \\begin{aligned} M_2 &amp;= M_{2a} + n_a(\\mu_a - \\mu)^2 + M_{2b} + n_b(\\mu_b - \\mu)^2 \\\\ &amp;= M_{2a} + M_{2b} + n_a\\left(-\\delta \\frac{n_b}{n}\\right)^2 + n_b\\left(\\delta \\frac{n_a}{n}\\right)^2 \\\\ &amp;= M_{2a} + M_{2b} + \\delta^2 \\frac{n_a n_b (n_b + n_a)}{n^2} \\\\ &amp;= M_{2a} + M_{2b} + \\delta^2 \\frac{n_a n_b}{n} \\end{aligned} \\]"},{"location":"algorithms/streaming/#pseudocode_1","title":"Pseudocode","text":"<pre><code>def pebay_merge(n_a, mean_a, M2_a, n_b, mean_b, M2_b):\n    \"\"\"Merge two partial states\"\"\"\n    n = n_a + n_b\n    if n == 0:\n        return 0, 0.0, 0.0\n\n    delta = mean_b - mean_a\n    mean = mean_a + delta * n_b / n\n    M2 = M2_a + M2_b + delta**2 * n_a * n_b / n\n\n    return n, mean, M2\n</code></pre>"},{"location":"algorithms/streaming/#properties_1","title":"Properties","text":"<ol> <li>Associative: Order of merging doesn't matter</li> <li>Commutative: A \u222a B = B \u222a A</li> <li>Exact: Same result as single-pass over concatenated data</li> <li>Parallel: Enables multi-threading, distributed computation</li> <li>Fast: O(1) time to merge two states</li> </ol>"},{"location":"algorithms/streaming/#higher-moments-extension","title":"Higher Moments Extension","text":""},{"location":"algorithms/streaming/#third-and-fourth-moments","title":"Third and Fourth Moments","text":"<p>State variables: - \\(n\\), \\(\\mu\\), \\(M_2\\), \\(M_3\\), \\(M_4\\)</p> <p>Where: - \\(M_3 = \\sum (x_i - \\mu)^3\\) - \\(M_4 = \\sum (x_i - \\mu)^4\\)</p>"},{"location":"algorithms/streaming/#online-update","title":"Online Update","text":"<pre><code>def moments_update(n, mean, M2, M3, M4, x):\n    \"\"\"Update all four moments\"\"\"\n    n_new = n + 1\n    delta = x - mean\n    delta_n = delta / n_new\n    delta_n2 = delta_n * delta_n\n    term1 = delta * delta_n * n\n\n    mean_new = mean + delta_n\n    M4_new = M4 + term1 * delta_n2 * (n_new*n_new - 3*n_new + 3) + 6*delta_n2*M2 - 4*delta_n*M3\n    M3_new = M3 + term1 * delta_n * (n_new - 2) - 3*delta_n*M2\n    M2_new = M2 + term1\n\n    return n_new, mean_new, M2_new, M3_new, M4_new\n</code></pre>"},{"location":"algorithms/streaming/#pebay-merge-for-higher-moments","title":"P\u00e9bay Merge for Higher Moments","text":"<pre><code>def pebay_merge_moments(n_a, mean_a, M2_a, M3_a, M4_a,\n                        n_b, mean_b, M2_b, M3_b, M4_b):\n    \"\"\"Merge higher moments\"\"\"\n    n = n_a + n_b\n    if n == 0:\n        return 0, 0.0, 0.0, 0.0, 0.0\n\n    delta = mean_b - mean_a\n    delta2 = delta * delta\n    delta3 = delta2 * delta\n    delta4 = delta3 * delta\n\n    mean = mean_a + delta * n_b / n\n\n    M2 = M2_a + M2_b + delta2 * n_a * n_b / n\n\n    M3 = M3_a + M3_b + \\\n         delta3 * n_a * n_b * (n_a - n_b) / (n * n) + \\\n         3 * delta * (n_a * M2_b - n_b * M2_a) / n\n\n    M4 = M4_a + M4_b + \\\n         delta4 * n_a * n_b * (n_a*n_a - n_a*n_b + n_b*n_b) / (n * n * n) + \\\n         6 * delta2 * (n_a*n_a * M2_b + n_b*n_b * M2_a) / (n * n) + \\\n         4 * delta * (n_a * M3_b - n_b * M3_a) / n\n\n    return n, mean, M2, M3, M4\n</code></pre>"},{"location":"algorithms/streaming/#computing-skewness-and-kurtosis","title":"Computing Skewness and Kurtosis","text":"<pre><code>def compute_shape(n, M2, M3, M4):\n    \"\"\"Compute skewness and excess kurtosis\"\"\"\n    if n &lt; 3:\n        return None, None\n\n    variance = M2 / (n - 1)\n    if variance == 0:\n        return None, None\n\n    # Skewness (g1)\n    g1 = (n / ((n-1) * (n-2))) * (M3 / n) / (variance ** 1.5)\n\n    if n &lt; 4:\n        return g1, None\n\n    # Excess kurtosis (g2)\n    g2 = ((n * (n+1)) / ((n-1) * (n-2) * (n-3))) * (M4 / n) / (variance ** 2) - \\\n         (3 * (n-1) ** 2) / ((n-2) * (n-3))\n\n    return g1, g2\n</code></pre>"},{"location":"algorithms/streaming/#numerical-stability-analysis","title":"Numerical Stability Analysis","text":""},{"location":"algorithms/streaming/#why-naive-formula-fails","title":"Why Naive Formula Fails","text":"<p>Consider \\(x_i \\approx 10^9 + \\epsilon_i\\) where \\(|\\epsilon_i| \\ll 10^9\\).</p> <p>Naive formula:</p> \\[ \\sum x_i^2 \\approx n \\cdot 10^{18}, \\quad \\left(\\sum x_i\\right)^2 / n \\approx n \\cdot 10^{18} \\] <p>Subtraction loses precision (catastrophic cancellation).</p> <p>Welford's formula:</p> <p>Works with deviations \\(x_i - \\mu\\), which are \\(O(\\epsilon)\\), avoiding large intermediate values.</p>"},{"location":"algorithms/streaming/#condition-number","title":"Condition Number","text":"<p>For variance computation, Welford's algorithm has condition number \\(\\kappa \\approx 1\\), while naive formula has \\(\\kappa \\approx \\frac{\\bar{x}^2}{\\sigma^2}\\) (can be huge).</p>"},{"location":"algorithms/streaming/#parallelization","title":"Parallelization","text":""},{"location":"algorithms/streaming/#mapreduce-pattern","title":"MapReduce Pattern","text":"<pre><code># Map phase: compute partial moments per chunk\ndef map_chunk(chunk):\n    n, mean, M2, M3, M4 = 0, 0.0, 0.0, 0.0, 0.0\n    for x in chunk:\n        n, mean, M2, M3, M4 = moments_update(n, mean, M2, M3, M4, x)\n    return n, mean, M2, M3, M4\n\n# Reduce phase: merge all partial results\ndef reduce_moments(states):\n    result = (0, 0.0, 0.0, 0.0, 0.0)\n    for state in states:\n        result = pebay_merge_moments(*result, *state)\n    return result\n\n# Usage\npartial_states = [map_chunk(chunk) for chunk in chunks]\nfinal_state = reduce_moments(partial_states)\n</code></pre>"},{"location":"algorithms/streaming/#multi-threading","title":"Multi-threading","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef parallel_moments(data, n_threads=4):\n    chunks = np.array_split(data, n_threads)\n\n    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n        states = list(executor.map(map_chunk, chunks))\n\n    return reduce_moments(states)\n</code></pre>"},{"location":"algorithms/streaming/#implementation-in-pysuricata","title":"Implementation in PySuricata","text":""},{"location":"algorithms/streaming/#streamingmoments-class","title":"StreamingMoments Class","text":"<pre><code>class StreamingMoments:\n    def __init__(self):\n        self.n = 0\n        self.mean = 0.0\n        self.M2 = 0.0\n        self.M3 = 0.0\n        self.M4 = 0.0\n\n    def update(self, values: np.ndarray):\n        \"\"\"Update with array of values\"\"\"\n        for x in values:\n            if not np.isfinite(x):\n                continue\n            self.n, self.mean, self.M2, self.M3, self.M4 = \\\n                moments_update(self.n, self.mean, self.M2, self.M3, self.M4, x)\n\n    def merge(self, other: 'StreamingMoments'):\n        \"\"\"Merge with another moments object\"\"\"\n        self.n, self.mean, self.M2, self.M3, self.M4 = \\\n            pebay_merge_moments(\n                self.n, self.mean, self.M2, self.M3, self.M4,\n                other.n, other.mean, other.M2, other.M3, other.M4\n            )\n\n    def finalize(self):\n        \"\"\"Compute final statistics\"\"\"\n        if self.n &lt; 2:\n            return {\"mean\": self.mean, \"variance\": None, \"skewness\": None, \"kurtosis\": None}\n\n        variance = self.M2 / (self.n - 1)\n        std = math.sqrt(variance)\n        skewness, kurtosis = compute_shape(self.n, self.M2, self.M3, self.M4)\n\n        return {\n            \"count\": self.n,\n            \"mean\": self.mean,\n            \"variance\": variance,\n            \"std\": std,\n            \"skewness\": skewness,\n            \"kurtosis\": kurtosis\n        }\n</code></pre>"},{"location":"algorithms/streaming/#validation","title":"Validation","text":""},{"location":"algorithms/streaming/#test-properties","title":"Test Properties","text":"<pre><code>def test_welford_equivalence():\n    \"\"\"Verify Welford = two-pass\"\"\"\n    data = np.random.randn(10000)\n\n    # Welford\n    n, mean, M2 = 0, 0.0, 0.0\n    for x in data:\n        n, mean, M2 = welford_update(n, mean, M2, x)\n    var_welford = M2 / (n - 1)\n\n    # Two-pass\n    mean_twopass = np.mean(data)\n    var_twopass = np.var(data, ddof=1)\n\n    assert np.isclose(mean, mean_twopass)\n    assert np.isclose(var_welford, var_twopass)\n\ndef test_pebay_merge():\n    \"\"\"Verify merge = concatenate\"\"\"\n    data_a = np.random.randn(5000)\n    data_b = np.random.randn(3000)\n\n    # Separate\n    state_a = compute_moments(data_a)\n    state_b = compute_moments(data_b)\n    merged = pebay_merge_moments(*state_a, *state_b)\n\n    # Combined\n    data_combined = np.concatenate([data_a, data_b])\n    combined = compute_moments(data_combined)\n\n    assert np.allclose(merged, combined)\n</code></pre>"},{"location":"algorithms/streaming/#references","title":"References","text":"<ol> <li> <p>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p> </li> <li> <p>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212.</p> </li> <li> <p>Chan, T.F., Golub, G.H., LeVeque, R.J. (1983), \"Algorithms for Computing the Sample Variance: Analysis and Recommendations\", The American Statistician, 37(3): 242\u2013247.</p> </li> <li> <p>West, D.H.D. (1979), \"Updating Mean and Variance Estimates: An Improved Method\", Communications of the ACM, 22(9): 532\u2013535.</p> </li> <li> <p>Wikipedia: Algorithms for calculating variance - Link</p> </li> </ol>"},{"location":"algorithms/streaming/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Application of these algorithms</li> <li>Sketch Algorithms - Other streaming algorithms</li> <li>Performance Tips - Optimization strategies</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/correlations/","title":"Correlation Analysis","text":"<p>PySuricata computes pairwise correlations between numeric columns using streaming algorithms that operate in bounded memory.</p>"},{"location":"analytics/correlations/#overview","title":"Overview","text":"<p>Correlation analysis reveals linear relationships between numeric variables, helping identify: - Redundant features (highly correlated) - Related measurements (positively/negatively correlated) - Independent variables (near-zero correlation)</p>"},{"location":"analytics/correlations/#key-features","title":"Key Features","text":"<ul> <li>Streaming computation: O(p\u00b2) space for p numeric columns</li> <li>Single-pass algorithm: No need to store full data</li> <li>Exact Pearson correlation: Not approximate</li> <li>Configurable threshold: Only report significant correlations</li> <li>Per-column top-k: Show most correlated pairs</li> </ul>"},{"location":"analytics/correlations/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":""},{"location":"analytics/correlations/#definition","title":"Definition","text":"<p>For two numeric variables X and Y with observations \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\):</p> \\[ r_{XY} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\] <p>where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means.</p>"},{"location":"analytics/correlations/#alternative-formula","title":"Alternative Formula","text":"\\[ r_{XY} = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n\\sum y_i^2 - (\\sum y_i)^2}} \\] <p>This form enables streaming computation by maintaining sufficient statistics.</p>"},{"location":"analytics/correlations/#properties","title":"Properties","text":"<ul> <li>Range: \\(r \\in [-1, 1]\\)</li> <li>Interpretation:</li> <li>\\(r = 1\\): perfect positive linear relationship</li> <li>\\(r = -1\\): perfect negative linear relationship</li> <li>\\(r = 0\\): no linear relationship</li> <li>\\(0 &lt; r &lt; 1\\): positive correlation</li> <li>\\(-1 &lt; r &lt; 0\\): negative correlation</li> </ul>"},{"location":"analytics/correlations/#strength-guidelines","title":"Strength Guidelines","text":"<p>| \\(|r|\\) Range | Strength | |---------------|----------| | 0.0 - 0.2 | Very weak | | 0.2 - 0.4 | Weak | | 0.4 - 0.6 | Moderate | | 0.6 - 0.8 | Strong | | 0.8 - 1.0 | Very strong |</p>"},{"location":"analytics/correlations/#streaming-algorithm","title":"Streaming Algorithm","text":""},{"location":"analytics/correlations/#sufficient-statistics","title":"Sufficient Statistics","text":"<p>To compute \\(r_{XY}\\) without storing all data, maintain:</p> \\[ \\begin{aligned} n &amp;= \\text{count of pairs} \\\\ S_x &amp;= \\sum x_i \\\\ S_y &amp;= \\sum y_i \\\\ S_{xx} &amp;= \\sum x_i^2 \\\\ S_{yy} &amp;= \\sum y_i^2 \\\\ S_{xy} &amp;= \\sum x_i y_i \\end{aligned} \\]"},{"location":"analytics/correlations/#update-step","title":"Update Step","text":"<p>For each new pair \\((x, y)\\):</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ S_x &amp;\\leftarrow S_x + x \\\\ S_y &amp;\\leftarrow S_y + y \\\\ S_{xx} &amp;\\leftarrow S_{xx} + x^2 \\\\ S_{yy} &amp;\\leftarrow S_{yy} + y^2 \\\\ S_{xy} &amp;\\leftarrow S_{xy} + xy \\end{aligned} \\]"},{"location":"analytics/correlations/#finalize","title":"Finalize","text":"<p>Compute correlation:</p> \\[ r = \\frac{nS_{xy} - S_x S_y}{\\sqrt{nS_{xx} - S_x^2} \\sqrt{nS_{yy} - S_y^2}} \\]"},{"location":"analytics/correlations/#missing-values","title":"Missing Values","text":"<p>Only pairs with both values present are included:</p> <pre><code>mask = ~(isnan(x) | isnan(y) | isinf(x) | isinf(y))\nx_valid = x[mask]\ny_valid = y[mask]\n# Update with valid pairs only\n</code></pre>"},{"location":"analytics/correlations/#statistical-significance","title":"Statistical Significance","text":""},{"location":"analytics/correlations/#t-test-for-correlation","title":"t-test for Correlation","text":"<p>Test \\(H_0: \\rho = 0\\) (no correlation in population).</p> <p>Test statistic:</p> \\[ t = r \\sqrt{\\frac{n-2}{1-r^2}} \\] <p>Under \\(H_0\\), \\(t \\sim t_{n-2}\\) (Student's t-distribution with \\(n-2\\) degrees of freedom).</p> <p>P-value:</p> \\[ p = 2 \\cdot P(T_{n-2} &gt; |t|) \\] <p>Reject \\(H_0\\) if \\(p &lt; \\alpha\\) (e.g., \\(\\alpha = 0.05\\)).</p> <p>Not implemented in current version</p> <p>Significance tests are planned for future release. Current version reports raw correlations.</p>"},{"location":"analytics/correlations/#multiple-testing-correction","title":"Multiple Testing Correction","text":"<p>When testing \\(m = \\binom{p}{2}\\) pairs, use Bonferroni correction:</p> \\[ \\alpha_{\\text{adj}} = \\frac{\\alpha}{m} \\] <p>Or False Discovery Rate (FDR) control via Benjamini-Hochberg procedure.</p> <p>Example: 50 columns \u2192 1,225 pairs - Bonferroni: \\(\\alpha_{\\text{adj}} = 0.05/1225 \\approx 0.00004\\) - Very conservative</p>"},{"location":"analytics/correlations/#implementation","title":"Implementation","text":""},{"location":"analytics/correlations/#streamingcorr-class","title":"StreamingCorr Class","text":"<pre><code>class StreamingCorr:\n    def __init__(self, columns: List[str]):\n        self.cols = columns\n        self.pairs = {}  # (col1, col2) -&gt; {n, sx, sy, sxx, syy, sxy}\n\n    def update(self, df: pd.DataFrame):\n        \"\"\"Update with chunk of data\"\"\"\n        for i, col1 in enumerate(self.cols):\n            for j in range(i+1, len(self.cols)):\n                col2 = self.cols[j]\n\n                # Extract values\n                x = df[col1].to_numpy()\n                y = df[col2].to_numpy()\n\n                # Filter valid pairs\n                mask = np.isfinite(x) &amp; np.isfinite(y)\n                x_valid = x[mask]\n                y_valid = y[mask]\n\n                if len(x_valid) == 0:\n                    continue\n\n                # Update sufficient statistics\n                key = (col1, col2)\n                if key not in self.pairs:\n                    self.pairs[key] = {\n                        'n': 0, 'sx': 0, 'sy': 0,\n                        'sxx': 0, 'syy': 0, 'sxy': 0\n                    }\n\n                stats = self.pairs[key]\n                stats['n'] += len(x_valid)\n                stats['sx'] += float(np.sum(x_valid))\n                stats['sy'] += float(np.sum(y_valid))\n                stats['sxx'] += float(np.sum(x_valid ** 2))\n                stats['syy'] += float(np.sum(y_valid ** 2))\n                stats['sxy'] += float(np.sum(x_valid * y_valid))\n\n    def finalize(self, threshold: float = 0.0) -&gt; Dict:\n        \"\"\"Compute final correlations\"\"\"\n        results = {}\n\n        for (col1, col2), stats in self.pairs.items():\n            n = stats['n']\n            if n &lt; 2:\n                continue\n\n            # Compute correlation\n            num = n * stats['sxy'] - stats['sx'] * stats['sy']\n            den1 = n * stats['sxx'] - stats['sx'] ** 2\n            den2 = n * stats['syy'] - stats['sy'] ** 2\n\n            if den1 &lt;= 0 or den2 &lt;= 0:\n                continue\n\n            r = num / (math.sqrt(den1) * math.sqrt(den2))\n\n            # Filter by threshold\n            if abs(r) &gt;= threshold:\n                results[(col1, col2)] = r\n\n        return results\n</code></pre>"},{"location":"analytics/correlations/#complexity","title":"Complexity","text":""},{"location":"analytics/correlations/#space-complexity","title":"Space Complexity","text":"<p>For \\(p\\) numeric columns: - Number of pairs: \\(m = \\binom{p}{2} = \\frac{p(p-1)}{2} = O(p^2)\\) - Space per pair: O(1) (6 floating-point values) - Total space: O(p\u00b2)</p> <p>Example: - 10 columns \u2192 45 pairs \u2192 ~2 KB - 50 columns \u2192 1,225 pairs \u2192 ~50 KB - 100 columns \u2192 4,950 pairs \u2192 ~200 KB</p>"},{"location":"analytics/correlations/#time-complexity","title":"Time Complexity","text":"<p>Per chunk with \\(n\\) rows and \\(p\\) columns: - Iterate over \\(O(p^2)\\) pairs - For each pair: \\(O(n)\\) to compute valid mask and sums - Total per chunk: O(n p\u00b2)</p> <p>For dataset with \\(N\\) total rows: - Total time: O(N p\u00b2)</p>"},{"location":"analytics/correlations/#when-to-disable","title":"When to Disable","text":"<p>For large \\(p\\) (many columns), correlation computation can be expensive:</p> <ul> <li>\\(p &gt; 100\\): Consider disabling or using sampling</li> <li>\\(p &gt; 500\\): Strongly recommend disabling</li> </ul> <p>Configuration:</p> <pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Disable\n</code></pre>"},{"location":"analytics/correlations/#configuration","title":"Configuration","text":"<p>Control correlation analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ProfileConfig, ComputeOptions\n\n# Using the public API\nconfig = ProfileConfig(compute=ComputeOptions(\n    compute_correlations=True,  # Default\n    corr_threshold=0.5,  # Default (only |r| &gt;= 0.5)\n    corr_max_cols=50,  # Default (skip if &gt; 50 cols)\n    corr_max_per_col=10  # Default (top 10 per column)\n))\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"analytics/correlations/#interpretation","title":"Interpretation","text":""},{"location":"analytics/correlations/#high-positive-correlation-r-08","title":"High Positive Correlation (r &gt; 0.8)","text":"<p>Interpretation: Variables move together strongly.</p> <p>Examples: - Height and weight (r \u2248 0.7-0.8) - Temperature in \u00b0F and \u00b0C (r = 1.0, exact conversion) - Revenue and profit (r \u2248 0.8-0.9)</p> <p>Actionable insights: - Potential redundancy (consider removing one feature) - Useful for imputation (predict one from other) - Check for derived features (one computed from other)</p>"},{"location":"analytics/correlations/#high-negative-correlation-r-08","title":"High Negative Correlation (r &lt; -0.8)","text":"<p>Interpretation: Variables move in opposite directions.</p> <p>Examples: - Latitude and temperature (r \u2248 -0.5 to -0.7) - Altitude and air pressure (r \u2248 -0.9) - Discount and profit margin (r \u2248 -0.6)</p> <p>Actionable insights: - Substitutes or inverse relationships - Consider composite features (sum, ratio)</p>"},{"location":"analytics/correlations/#low-correlation-r-02","title":"Low Correlation (|r| &lt; 0.2)","text":"<p>Interpretation: Little to no linear relationship.</p> <p>Note: Variables may still have nonlinear relationships (e.g., quadratic, exponential).</p> <p>Actionable insights: - Independent features (good for model diversity) - May need nonlinear analysis (polynomial features, interactions)</p>"},{"location":"analytics/correlations/#limitations","title":"Limitations","text":""},{"location":"analytics/correlations/#linear-relationships-only","title":"Linear Relationships Only","text":"<p>Pearson correlation measures linear association only.</p> <p>Example: Quadratic relationship \\(y = x^2\\) - Correlation: \\(r \\approx 0\\) (if x spans negative and positive) - But strong nonlinear relationship exists</p> <p>Solutions: - Use Spearman rank correlation (monotonic relationships) - Plot scatter plots - Use mutual information (any dependency)</p>"},{"location":"analytics/correlations/#sensitive-to-outliers","title":"Sensitive to Outliers","text":"<p>Single extreme value can dominate correlation.</p> <p>Solutions: - Use Spearman instead (rank-based, robust) - Remove outliers before computing - Use robust correlation measures (MAD-based)</p>"},{"location":"analytics/correlations/#correlation-causation","title":"Correlation \u2260 Causation","text":"<p>High correlation does not imply causation.</p> <p>Example: Ice cream sales and drowning deaths (r \u2248 0.9) - Spurious correlation (confounded by temperature/summer)</p>"},{"location":"analytics/correlations/#alternatives","title":"Alternatives","text":""},{"location":"analytics/correlations/#spearman-rank-correlation","title":"Spearman Rank Correlation","text":"<p>Measures monotonic (not necessarily linear) relationships.</p> \\[ \\rho_s = 1 - \\frac{6\\sum d_i^2}{n(n^2 - 1)} \\] <p>where \\(d_i\\) is the rank difference for observation \\(i\\).</p> <p>Advantages: - Captures monotonic nonlinear relationships - Robust to outliers - No distribution assumptions</p> <p>Disadvantages: - Requires sorting (more expensive) - Not streamable (needs ranks)</p> <p>Not implemented in current version</p> <p>Spearman correlation is planned for future release.</p>"},{"location":"analytics/correlations/#kendall-tau","title":"Kendall Tau","text":"<p>Another rank-based correlation measure.</p> <p>Advantages: - More robust than Spearman - Better for small samples</p> <p>Disadvantages: - Even more expensive to compute (O(n log n) or O(n\u00b2))</p>"},{"location":"analytics/correlations/#mutual-information","title":"Mutual Information","text":"<p>Measures any dependency (linear or nonlinear).</p> \\[ MI(X, Y) = \\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)} \\] <p>Advantages: - Detects any relationship - Information-theoretic</p> <p>Disadvantages: - Requires binning (continuous \u2192 discrete) - Harder to interpret than correlation</p>"},{"location":"analytics/correlations/#examples","title":"Examples","text":""},{"location":"analytics/correlations/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndf = pd.DataFrame({\n    \"x\": range(100),\n    \"y\": [2*i + 1 for i in range(100)],  # y = 2x + 1\n    \"z\": [100 - i for i in range(100)]    # z = 100 - x\n})\n\nconfig = ReportConfig()\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5\n\nreport = profile(df, config=config)\n# Expect: r(x,y) \u2248 1.0, r(x,z) \u2248 -1.0, r(y,z) \u2248 -1.0\n</code></pre>"},{"location":"analytics/correlations/#access-correlations-programmatically","title":"Access Correlations Programmatically","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\nx_stats = stats[\"columns\"][\"x\"]\ncorrelations = x_stats.get(\"corr_top\", [])\n\nfor col, r in correlations:\n    print(f\"x vs {col}: r = {r:.3f}\")\n</code></pre>"},{"location":"analytics/correlations/#high-dimensional-data","title":"High-Dimensional Data","text":"<pre><code># Many columns: disable correlations\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False  # Too expensive\n\nreport = profile(wide_df, config=config)\n</code></pre>"},{"location":"analytics/correlations/#references","title":"References","text":"<ol> <li> <p>Pearson, K. (1895), \"Notes on regression and inheritance in the case of two parents\", Proceedings of the Royal Society of London, 58: 240\u2013242.</p> </li> <li> <p>Rodgers, J.L., Nicewander, W.A. (1988), \"Thirteen Ways to Look at the Correlation Coefficient\", The American Statistician, 42(1): 59\u201366.</p> </li> <li> <p>Spearman, C. (1904), \"The proof and measurement of association between two things\", American Journal of Psychology, 15: 72\u2013101.</p> </li> <li> <p>Benjamini, Y., Hochberg, Y. (1995), \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", Journal of the Royal Statistical Society B, 57(1): 289\u2013300.</p> </li> <li> <p>Wikipedia: Pearson correlation coefficient - Link</p> </li> <li> <p>Wikipedia: Spearman's rank correlation - Link</p> </li> </ol>"},{"location":"analytics/correlations/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Univariate numeric statistics</li> <li>Streaming Algorithms - Streaming computation techniques</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/duplicates/","title":"Duplicate Detection","text":"<p>PySuricata estimates duplicate rows using memory-efficient hash-based algorithms.</p>"},{"location":"analytics/duplicates/#duplicate-rate","title":"Duplicate Rate","text":"<p>For dataset with \\(n\\) total rows and \\(d\\) distinct rows:</p> \\[ DR = \\frac{n - d}{n} = 1 - \\frac{d}{n} \\]"},{"location":"analytics/duplicates/#detection-method","title":"Detection Method","text":"<p>Uses KMV sketch on row hashes for approximate distinct count:</p> <pre><code># Conceptual algorithm\nfor row in dataset:\n    row_hash = hash(tuple(row))\n    kmv.add(row_hash)\n\nn_distinct = kmv.estimate()\nduplicate_rate = (n_total - n_distinct) / n_total\n</code></pre>"},{"location":"analytics/duplicates/#exact-vs-approximate","title":"Exact vs Approximate","text":"<p>Exact (for small datasets): <pre><code>exact_duplicates = df.duplicated().sum()\ndup_pct = (exact_duplicates / len(df)) * 100\n</code></pre></p> <p>Approximate (PySuricata for large datasets): - Uses KMV sketch - ~2% error with default settings - Constant memory</p>"},{"location":"analytics/duplicates/#see-also","title":"See Also","text":"<ul> <li>Sketch Algorithms - KMV details</li> <li>Data Quality - Quality metrics</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/missing-values/","title":"Missing Values Analysis","text":"<p>Comprehensive guide to PySuricata's intelligent missing values analysis with adaptive display and chunk-level distribution tracking.</p>"},{"location":"analytics/missing-values/#overview","title":"Overview","text":"<p>Missing data is ubiquitous in real-world datasets. PySuricata provides:</p> <ul> <li>Intelligent display: Adaptive limits based on dataset size</li> <li>Chunk-level tracking: See missing data distribution across chunks</li> <li>Pattern detection: Identify systematic missingness</li> <li>Smart filtering: Show only significant missing columns</li> <li>Expandable UI: Progressive disclosure for many columns</li> </ul>"},{"location":"analytics/missing-values/#missing-data-mechanisms","title":"Missing Data Mechanisms","text":""},{"location":"analytics/missing-values/#mar-mcar-mnar","title":"MAR, MCAR, MNAR","text":"<p>Missing Completely At Random (MCAR): - Missingness independent of observed/unobserved data - \\(P(\\text{missing} | X, Y) = P(\\text{missing})\\) - Example: Sensor randomly fails</p> <p>Missing At Random (MAR): - Missingness depends on observed data only - \\(P(\\text{missing} | X, Y_{\\text{obs}}) = P(\\text{missing} | X)\\) - Example: Older patients skip optional questions</p> <p>Missing Not At Random (MNAR): - Missingness depends on unobserved values - Example: High earners don't report income</p> <p>Detection not automated</p> <p>Determining mechanism requires domain knowledge. PySuricata shows patterns to help investigation.</p>"},{"location":"analytics/missing-values/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"analytics/missing-values/#missing-rate","title":"Missing Rate","text":"<p>For column with \\(n_{\\text{total}}\\) observations:</p> \\[ MR = \\frac{n_{\\text{missing}}}{n_{\\text{total}}} \\]"},{"location":"analytics/missing-values/#missing-pattern-entropy","title":"Missing Pattern Entropy","text":"<p>For \\(k\\) different missing patterns (combinations of missing columns):</p> \\[ H_{\\text{pattern}} = -\\sum_{i=1}^{k} p_i \\log_2 p_i \\] <p>where \\(p_i\\) is the proportion of rows with pattern \\(i\\).</p> <p>High entropy: Many different patterns (complex missingness) Low entropy: Few patterns (systematic missingness)</p> <p>Not implemented</p> <p>Pattern entropy computation is planned for future release.</p>"},{"location":"analytics/missing-values/#intelligent-display-system","title":"Intelligent Display System","text":""},{"location":"analytics/missing-values/#dynamic-limits","title":"Dynamic Limits","text":"<p>Limits adapt to dataset size:</p> Dataset Size Initial Display Expanded Display \u226410 columns All All 11-50 columns 10 25 51-200 columns 12 25 &gt;200 columns 15 25"},{"location":"analytics/missing-values/#smart-filtering","title":"Smart Filtering","text":"<p>Threshold: Only show columns with &gt;\\(t\\)% missing (default \\(t=0.5\\)%)</p> <p>Rationale: Columns with &lt;0.5% missing are usually not concerning.</p>"},{"location":"analytics/missing-values/#expandable-ui","title":"Expandable UI","text":"<p>For datasets with many missing columns: 1. Initial view: Show top \\(n\\) columns 2. Expand button: Reveal up to 25 total 3. Smooth animation: JavaScript-powered transition</p>"},{"location":"analytics/missing-values/#chunk-level-distribution","title":"Chunk-Level Distribution","text":"<p>Track missing data per chunk to identify: - Temporal patterns (early vs. late data) - Batch patterns (certain files have more missing) - System issues (outages, collection failures)</p>"},{"location":"analytics/missing-values/#visualization","title":"Visualization","text":"<p>Horizontal bar showing missing percentage per chunk:</p> <pre><code>Chunk 1  \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591  40%\nChunk 2  \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  20%\nChunk 3  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591   0%\nChunk 4  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591  70%\n</code></pre> <p>Reveals chunk 4 has data quality issue.</p>"},{"location":"analytics/missing-values/#configuration","title":"Configuration","text":"<pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Missing columns display threshold (default 0.5%)\n# (Not yet configurable in current version)\n\n# Maximum initial display (default: dynamic based on dataset size)\n# (Not yet configurable in current version)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"analytics/missing-values/#implementation","title":"Implementation","text":""},{"location":"analytics/missing-values/#missingcolumnsanalyzer","title":"MissingColumnsAnalyzer","text":"<pre><code>class MissingColumnsAnalyzer:\n    MIN_THRESHOLD_PCT = 0.5\n    MAX_INITIAL_DISPLAY = 8\n    MAX_EXPANDED_DISPLAY = 25\n\n    def analyze_missing_columns(self, miss_list, n_cols, n_rows):\n        \"\"\"Analyze and filter missing columns\"\"\"\n        # Filter significant missing\n        significant = [\n            item for item in miss_list \n            if item[1] &gt;= self.MIN_THRESHOLD_PCT\n        ]\n\n        # Determine limits\n        initial_limit = self._get_initial_display_limit(n_cols, n_rows)\n        expanded_limit = self._get_expanded_display_limit(n_cols, n_rows)\n\n        # Build result\n        return MissingColumnsResult(\n            initial_columns=significant[:initial_limit],\n            expanded_columns=significant[:expanded_limit],\n            needs_expandable=len(significant) &gt; initial_limit,\n            total_significant=len(significant),\n            total_insignificant=len(miss_list) - len(significant)\n        )\n</code></pre>"},{"location":"analytics/missing-values/#interpreting-results","title":"Interpreting Results","text":""},{"location":"analytics/missing-values/#high-missing-percentage-50","title":"High Missing Percentage (&gt;50%)","text":"<p>Possible causes: - Optional field (by design) - Data collection issue - Recent column (added midway) - Rare event (e.g., \"error_message\" only on errors)</p> <p>Actions: - Verify if intentional - Consider imputation or exclusion - Check data pipeline</p>"},{"location":"analytics/missing-values/#systematic-patterns","title":"Systematic Patterns","text":"<p>Multiple columns missing together:</p> <p>Possible causes: - Related optional section (e.g., address fields) - Batch import failure - Survey skip logic</p> <p>Actions: - Analyze co-occurrence - Check data source - Document business logic</p>"},{"location":"analytics/missing-values/#increasing-over-time","title":"Increasing Over Time","text":"<p>More missing in later chunks:</p> <p>Possible causes: - Degrading data quality - System malfunction - Intentional change</p> <p>Actions: - Investigate recent changes - Alert data engineering team</p>"},{"location":"analytics/missing-values/#littles-mcar-test","title":"Little's MCAR Test","text":"<p>Statistical test for MCAR assumption.</p> <p>Null hypothesis: Data is MCAR</p> <p>Test statistic: Compare means of subgroups defined by missing patterns</p> <p>P-value interpretation: - Large p-value: Consistent with MCAR - Small p-value: Reject MCAR (MAR or MNAR)</p> <p>Not implemented</p> <p>Little's test is planned for future release. Currently, users must perform external analysis.</p> <p>Reference: Little, R.J.A. (1988), \"A Test of Missing Completely at Random for Multivariate Data with Missing Values\", JASA, 83(404): 1198\u20131202.</p>"},{"location":"analytics/missing-values/#imputation-considerations","title":"Imputation Considerations","text":""},{"location":"analytics/missing-values/#meanmedian-imputation","title":"Mean/Median Imputation","text":"\\[ x_{\\text{imputed}} = \\begin{cases} x &amp; \\text{if observed} \\\\ \\bar{x} &amp; \\text{if missing} \\end{cases} \\] <p>Pros: Simple, fast Cons: Reduces variance, distorts correlations</p>"},{"location":"analytics/missing-values/#multiple-imputation","title":"Multiple Imputation","text":"<p>Generate \\(m\\) complete datasets with different imputations, analyze separately, combine results.</p> <p>Pros: Preserves uncertainty Cons: Complex, computationally expensive</p>"},{"location":"analytics/missing-values/#model-based","title":"Model-Based","text":"<p>Use ML model to predict missing values from other columns.</p> <p>Pros: Can capture complex relationships Cons: Requires training, may introduce bias</p> <p>PySuricata does not impute</p> <p>PySuricata is a profiling tool, not a preprocessing tool. Imputation should be done separately based on domain knowledge.</p>"},{"location":"analytics/missing-values/#best-practices","title":"Best Practices","text":"<ol> <li>Document missingness: Record why data is missing</li> <li>Distinguish NULL types: NULL vs. empty string vs. \"N/A\"</li> <li>Set thresholds: Define acceptable missing percentages</li> <li>Monitor trends: Track missing rates over time</li> <li>Investigate patterns: Look for systematic missingness</li> </ol>"},{"location":"analytics/missing-values/#examples","title":"Examples","text":""},{"location":"analytics/missing-values/#basic-usage","title":"Basic Usage","text":"<pre><code>from pysuricata import profile\n\n# Dataset with missing values\ndf = pd.DataFrame({\n    \"age\": [25, 30, None, 45, 50],\n    \"income\": [50000, None, None, 80000, 90000],\n    \"city\": [\"NYC\", \"LA\", None, \"Chicago\", None]\n})\n\nreport = profile(df)\n# Report shows missing percentages and patterns\n</code></pre>"},{"location":"analytics/missing-values/#access-missing-statistics","title":"Access Missing Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.1f}%\")\n\nfor col, col_stats in stats[\"columns\"].items():\n    missing_pct = col_stats.get(\"missing_pct\", 0)\n    if missing_pct &gt; 10:\n        print(f\"{col}: {missing_pct:.1f}% missing\")\n</code></pre>"},{"location":"analytics/missing-values/#references","title":"References","text":"<ol> <li> <p>Little, R.J.A., Rubin, D.B. (2019), Statistical Analysis with Missing Data, 3rd ed., Wiley.</p> </li> <li> <p>Rubin, D.B. (1976), \"Inference and Missing Data\", Biometrika, 63(3): 581\u2013592.</p> </li> <li> <p>Schafer, J.L., Graham, J.W. (2002), \"Missing Data: Our View of the State of the Art\", Psychological Methods, 7(2): 147\u2013177.</p> </li> <li> <p>Wikipedia: Missing data - Link</p> </li> </ol>"},{"location":"analytics/missing-values/#see-also","title":"See Also","text":"<ul> <li>Data Quality - Overall quality metrics</li> <li>Numeric Analysis - Handling missing in numeric columns</li> <li>Configuration - Display settings</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/quality/","title":"Data Quality Metrics","text":"<p>PySuricata computes several data quality metrics automatically.</p>"},{"location":"analytics/quality/#dataset-level-metrics","title":"Dataset-Level Metrics","text":""},{"location":"analytics/quality/#missing-cells-percentage","title":"Missing Cells Percentage","text":"\\[ \\text{Missing\\%} = \\frac{\\sum_{\\text{cols}} n_{\\text{missing}}}{\\text{rows} \\times \\text{cols}} \\times 100 \\] <p>Thresholds: - &lt; 5%: Good quality - 5-20%: Moderate issues - &gt; 20%: Significant problems</p>"},{"location":"analytics/quality/#duplicate-rows-approximate","title":"Duplicate Rows (Approximate)","text":"\\[ \\text{Dup\\%} = \\left(1 - \\frac{n_{\\text{distinct}}}{n_{\\text{total}}}\\right) \\times 100 \\]"},{"location":"analytics/quality/#constant-columns","title":"Constant Columns","text":"<p>Columns with single unique value (zero variance).</p>"},{"location":"analytics/quality/#highly-correlated-pairs","title":"Highly Correlated Pairs","text":"<p>Pairs with |r| &gt; 0.95 may indicate redundancy.</p>"},{"location":"analytics/quality/#column-level-metrics","title":"Column-Level Metrics","text":""},{"location":"analytics/quality/#completeness","title":"Completeness","text":"\\[ \\text{Completeness} = \\frac{n_{\\text{present}}}{n_{\\text{total}}} \\times 100 \\]"},{"location":"analytics/quality/#cardinality","title":"Cardinality","text":"<ul> <li>Very low (&lt; 10): Consider as categorical</li> <li>Very high (&gt; 0.9n): Consider as identifier</li> </ul>"},{"location":"analytics/quality/#outliers","title":"Outliers","text":"<p>Percentage of values outside acceptable ranges.</p>"},{"location":"analytics/quality/#quality-checks-in-cicd","title":"Quality Checks in CI/CD","text":"<pre><code>from pysuricata import summarize\n\ndef check_quality(df):\n    stats = summarize(df)\n\n    # Assertions\n    assert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\n    assert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n\n    for col, col_stats in stats[\"columns\"].items():\n        if \"unique\" in col.lower():\n            # Expect high cardinality for ID columns\n            assert col_stats[\"distinct\"] == col_stats[\"count\"]\n</code></pre>"},{"location":"analytics/quality/#see-also","title":"See Also","text":"<ul> <li>Missing Values - Missing data analysis</li> <li>Duplicates - Duplicate detection</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/boolean/","title":"Boolean Variable Analysis","text":"<p>Comprehensive documentation for analyzing boolean (True/False) variables in PySuricata with information-theoretic measures.</p>"},{"location":"stats/boolean/#overview","title":"Overview","text":"<p>PySuricata treats boolean variables as columns with two distinct values (True/False, 1/0, Yes/No). Analysis focuses on balance, information content, and missing patterns.</p>"},{"location":"stats/boolean/#key-features","title":"Key Features","text":"<ul> <li>True/False counts with percentages</li> <li>Balance ratio (distribution symmetry)</li> <li>Entropy (information content)</li> <li>Information per value (bits)</li> <li>Imbalance detection (skewed distributions)</li> <li>Missing value handling</li> </ul>"},{"location":"stats/boolean/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each boolean column:</p> <ul> <li>Count: total non-null values</li> <li>True count: number of True values</li> <li>False count: number of False values</li> <li>Missing count: number of missing/null values</li> <li>True percentage: \\(p = n_{\\text{true}} / n\\)</li> <li>False percentage: \\(1 - p\\)</li> <li>Missing percentage: \\(n_{\\text{missing}} / n_{\\text{total}}\\)</li> <li>Entropy: Shannon entropy in bits</li> <li>Balance score: measure of distribution symmetry</li> <li>Imbalance ratio: deviation from 50/50 split</li> </ul>"},{"location":"stats/boolean/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/boolean/#basic-counts","title":"Basic Counts","text":"<p>Let the boolean column have: - \\(n_{\\text{true}}\\) = count of True values - \\(n_{\\text{false}}\\) = count of False values - \\(n_{\\text{missing}}\\) = count of missing/null values - \\(n = n_{\\text{true}} + n_{\\text{false}}\\) = non-null count - \\(n_{\\text{total}} = n + n_{\\text{missing}}\\) = total observations</p>"},{"location":"stats/boolean/#probability","title":"Probability","text":"<p>Probability of True:</p> \\[ p = \\frac{n_{\\text{true}}}{n} \\] <p>Probability of False:</p> \\[ q = 1 - p = \\frac{n_{\\text{false}}}{n} \\]"},{"location":"stats/boolean/#truefalse-ratio","title":"True/False Ratio","text":"\\[ R = \\frac{n_{\\text{true}}}{n_{\\text{false}}} \\] <p>Interpretation: - \\(R = 1\\): perfectly balanced (50/50) - \\(R &gt; 1\\): more True than False - \\(R &lt; 1\\): more False than True - \\(R \\to \\infty\\): nearly all True - \\(R \\to 0\\): nearly all False</p>"},{"location":"stats/boolean/#imbalance-ratio","title":"Imbalance Ratio","text":"<p>Measures deviation from balanced distribution:</p> \\[ I = \\frac{|n_{\\text{true}} - n_{\\text{false}}|}{n} = |2p - 1| \\] <p>Properties: - \\(I = 0\\): perfectly balanced (\\(p = 0.5\\)) - \\(I = 1\\): completely imbalanced (\\(p = 0\\) or \\(p = 1\\)) - Range: \\([0, 1]\\)</p> <p>Interpretation: - \\(I &lt; 0.2\\): well balanced (40/60 to 60/40) - \\(0.2 \\le I &lt; 0.6\\): moderately imbalanced - \\(I \\ge 0.6\\): severely imbalanced - \\(I &gt; 0.9\\): nearly constant</p>"},{"location":"stats/boolean/#balance-score","title":"Balance Score","text":"<p>Alternative measure of balance:</p> \\[ B = 1 - |0.5 - p| \\] <p>Properties: - \\(B = 1\\): perfectly balanced (\\(p = 0.5\\)) - \\(B = 0.5\\): completely imbalanced (\\(p = 0\\) or \\(p = 1\\)) - Range: \\([0.5, 1]\\)</p> <p>Interpretation: - \\(B &gt; 0.9\\): well balanced - \\(0.7 &lt; B \\le 0.9\\): moderately balanced - \\(B \\le 0.7\\): imbalanced</p>"},{"location":"stats/boolean/#shannon-entropy","title":"Shannon Entropy","text":"<p>Measures the information content or uncertainty in the boolean distribution:</p> \\[ H = -p \\log_2(p) - (1-p) \\log_2(1-p) \\] <p>By convention, \\(0 \\log_2(0) = 0\\).</p> <p>Properties: - \\(H = 0\\) bits if \\(p = 0\\) or \\(p = 1\\) (no uncertainty, deterministic) - \\(H = 1\\) bit if \\(p = 0.5\\) (maximum uncertainty, uniformly random) - Range: \\([0, 1]\\) bits</p> <p>Interpretation: - \\(H &lt; 0.5\\): low information content, predictable - \\(H \\approx 1.0\\): high information content, unpredictable - \\(H = 1.0\\): fair coin flip</p> <p>Entropy vs. Probability:</p> \\(p\\) \\(H\\) (bits) Interpretation 0.0 0.00 No information (constant False) 0.1 0.47 Low entropy, mostly False 0.5 1.00 Maximum entropy, balanced 0.9 0.47 Low entropy, mostly True 1.0 0.00 No information (constant True)"},{"location":"stats/boolean/#information-content-per-true-value","title":"Information Content per True Value","text":"<p>Average information conveyed by each True observation:</p> \\[ IC_{\\text{true}} = -\\log_2(p) \\text{ bits} \\] <p>Example: - \\(p = 0.5\\): \\(IC = 1\\) bit (unsurprising) - \\(p = 0.1\\): \\(IC = 3.32\\) bits (rare event, informative) - \\(p = 0.01\\): \\(IC = 6.64\\) bits (very rare, very informative)</p> <p>Use case: In imbalanced classification, rare class has higher information content.</p>"},{"location":"stats/boolean/#information-content-per-false-value","title":"Information Content per False Value","text":"\\[ IC_{\\text{false}} = -\\log_2(1 - p) \\text{ bits} \\]"},{"location":"stats/boolean/#statistical-tests","title":"Statistical Tests","text":""},{"location":"stats/boolean/#binomial-test-for-balance","title":"Binomial Test for Balance","text":"<p>Test if \\(p = 0.5\\) (balanced distribution).</p> <p>Null hypothesis: \\(H_0: p = 0.5\\)</p> <p>Test statistic:</p> \\[ Z = \\frac{\\hat{p} - 0.5}{\\sqrt{0.5 \\cdot 0.5 / n}} \\] <p>Under \\(H_0\\) and large \\(n\\), \\(Z \\sim N(0, 1)\\).</p> <p>P-value (two-tailed):</p> \\[ \\text{p-value} = 2 \\cdot \\Phi(-|Z|) \\] <p>where \\(\\Phi\\) is the standard normal CDF.</p> <p>Interpretation: - Small p-value (&lt; 0.05): reject balance hypothesis (distribution is skewed) - Large p-value: consistent with balanced distribution</p> <p>Not implemented in current version</p> <p>Statistical tests are planned for future release.</p>"},{"location":"stats/boolean/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Count True/False \\(O(n)\\) \\(O(1)\\) Single pass Entropy \\(O(1)\\) \\(O(1)\\) From counts All metrics \\(O(n)\\) \\(O(1)\\) Single pass <p>Boolean analysis is extremely efficient: O(1) space, O(n) time.</p>"},{"location":"stats/boolean/#configuration","title":"Configuration","text":"<p>Control boolean analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Boolean-specific config\n# (Currently no boolean-specific parameters)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/boolean/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/boolean/#booleanaccumulator-class","title":"BooleanAccumulator Class","text":"<pre><code>class BooleanAccumulator:\n    def __init__(self, name: str, config: BooleanConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n        self.true_count = 0\n        self.false_count = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing\n        # Count True values\n        # Count False values\n        pass\n\n    def finalize(self) -&gt; BooleanSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute percentages\n        # Compute entropy\n        # Compute balance scores\n        # Detect imbalance\n        return BooleanSummary(\n            count=self.count,\n            missing=self.missing,\n            true_count=self.true_count,\n            false_count=self.false_count,\n            true_pct=self.true_count / max(1, self.count),\n            entropy=self._compute_entropy(),\n            balance_score=self._compute_balance(),\n            imbalance_ratio=self._compute_imbalance(),\n        )\n\n    def _compute_entropy(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        p = self.true_count / self.count\n        if p == 0.0 or p == 1.0:\n            return 0.0\n        return -(p * math.log2(p) + (1-p) * math.log2(1-p))\n\n    def _compute_balance(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        p = self.true_count / self.count\n        return 1.0 - abs(0.5 - p)\n\n    def _compute_imbalance(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        return abs(self.true_count - self.false_count) / self.count\n</code></pre>"},{"location":"stats/boolean/#examples","title":"Examples","text":""},{"location":"stats/boolean/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"is_active\": [True, False, True, True, None, False]\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/boolean/#imbalanced-boolean","title":"Imbalanced Boolean","text":"<pre><code># Highly imbalanced (10% True)\ndf = pd.DataFrame({\n    \"is_fraud\": [False] * 90 + [True] * 10\n})\n\nreport = profile(df)\n# Will show low entropy, high imbalance\n</code></pre>"},{"location":"stats/boolean/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nactive_stats = stats[\"columns\"][\"is_active\"]\n\nprint(f\"True count: {active_stats['true_count']}\")\nprint(f\"True %: {active_stats['true_pct']:.1%}\")\nprint(f\"Entropy: {active_stats['entropy']:.2f} bits\")\nprint(f\"Balance: {active_stats['balance_score']:.2f}\")\n</code></pre>"},{"location":"stats/boolean/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/boolean/#well-balanced-p-05","title":"Well-Balanced (p \u2248 0.5)","text":"<ul> <li>Entropy \u2248 1.0 bit</li> <li>Balance score &gt; 0.9</li> <li>Imbalance ratio &lt; 0.2</li> </ul> <p>Implications: - High information content - Good for binary classification (no class imbalance) - Unpredictable values</p> <p>Example: Fair coin flip, A/B test with even split.</p>"},{"location":"stats/boolean/#imbalanced-p-05-or-p-05","title":"Imbalanced (p &lt;&lt; 0.5 or p &gt;&gt; 0.5)","text":"<ul> <li>Entropy &lt; 0.5 bits</li> <li>Balance score &lt; 0.7</li> <li>Imbalance ratio &gt; 0.6</li> </ul> <p>Implications: - Low information content - May need rebalancing for ML - Predictable values</p> <p>Example: Fraud detection (1% positive), rare disease (0.1% positive).</p>"},{"location":"stats/boolean/#nearly-constant-p-001-or-p-099","title":"Nearly Constant (p &lt; 0.01 or p &gt; 0.99)","text":"<ul> <li>Entropy &lt; 0.1 bits</li> <li>Balance score \u2248 0.5</li> <li>Imbalance ratio &gt; 0.98</li> </ul> <p>Implications: - Almost no information - Consider removing column - May indicate data quality issue</p> <p>Example: \"is_deleted\" flag in active records table (all False).</p>"},{"location":"stats/boolean/#use-in-machine-learning","title":"Use in Machine Learning","text":""},{"location":"stats/boolean/#class-imbalance","title":"Class Imbalance","text":"<p>For binary classification with boolean target:</p> <p>Balanced (\\(0.4 &lt; p &lt; 0.6\\)): - Standard algorithms work well - Use accuracy as metric</p> <p>Moderately imbalanced (\\(0.1 &lt; p &lt; 0.4\\) or \\(0.6 &lt; p &lt; 0.9\\)): - Consider class weights - Use F1-score, AUC-ROC - Try SMOTE for oversampling</p> <p>Severely imbalanced (\\(p &lt; 0.1\\) or \\(p &gt; 0.9\\)): - Must use rebalancing techniques - Precision-recall curve essential - Consider anomaly detection instead</p>"},{"location":"stats/boolean/#entropy-as-feature-quality","title":"Entropy as Feature Quality","text":"<p>High entropy boolean features (\\(H \\approx 1\\)): - Good discriminative power - Worth including in model</p> <p>Low entropy boolean features (\\(H &lt; 0.5\\)): - Low information content - May not help model - Consider interaction terms</p>"},{"location":"stats/boolean/#special-cases","title":"Special Cases","text":""},{"location":"stats/boolean/#all-true-or-all-false","title":"All True or All False","text":"<ul> <li>Entropy = 0 (no information)</li> <li>Balance score = 0.5 (worst)</li> <li>Imbalance ratio = 1.0 (complete)</li> </ul> <p>Recommendation: Remove column (constant value).</p>"},{"location":"stats/boolean/#all-missing","title":"All Missing","text":"<ul> <li>No non-null values</li> <li>Statistics undefined</li> </ul> <p>Recommendation: Remove column or investigate data source.</p>"},{"location":"stats/boolean/#three-valued-boolean","title":"Three-Valued Boolean","text":"<p>Columns with True, False, and many NULLs:</p> <p>Interpretation: May be ternary (True/False/Unknown) rather than binary.</p> <p>Recommendation:  - Report missing percentage - Consider as categorical instead - Imputation may not be appropriate</p>"},{"location":"stats/boolean/#references","title":"References","text":"<ol> <li> <p>Shannon, C.E. (1948), \"A Mathematical Theory of Communication\", Bell System Technical Journal, 27: 379\u2013423.</p> </li> <li> <p>Cover, T.M., Thomas, J.A. (2006), Elements of Information Theory, 2nd ed., Wiley.</p> </li> <li> <p>Chawla, N.V. et al. (2002), \"SMOTE: Synthetic Minority Over-sampling Technique\", JAIR, 16: 321\u2013357.</p> </li> <li> <p>He, H., Garcia, E.A. (2009), \"Learning from Imbalanced Data\", IEEE TKDE, 21(9): 1263\u20131284.</p> </li> <li> <p>Wikipedia: Entropy (information theory) - Link</p> </li> <li> <p>Wikipedia: Binary classification - Link</p> </li> </ol>"},{"location":"stats/boolean/#see-also","title":"See Also","text":"<ul> <li>Categorical Analysis - For multi-class variables</li> <li>Data Quality - Quality metrics</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/categorical/","title":"Categorical Variable Analysis","text":"<p>This page provides comprehensive documentation for how PySuricata analyzes categorical (string, object) variables using scalable streaming algorithms with mathematical guarantees.</p>"},{"location":"stats/categorical/#overview","title":"Overview","text":"<p>PySuricata treats a categorical variable as any column with string-like values, objects, or low-cardinality integers. Analysis focuses on frequency distributions, diversity metrics, and string characteristics.</p>"},{"location":"stats/categorical/#key-features","title":"Key Features","text":"<ul> <li>Top-k values with frequencies (Misra-Gries algorithm)</li> <li>Distinct count estimation (KMV sketch)</li> <li>Diversity metrics (entropy, Gini impurity, concentration)</li> <li>String statistics (length distribution, empty strings)</li> <li>Variant detection (case-insensitive, trimmed)</li> <li>Memory-efficient streaming algorithms</li> </ul>"},{"location":"stats/categorical/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each categorical column:</p> <ul> <li>Count: non-null values, missing percentage</li> <li>Distinct: unique value count (exact or approximate)</li> <li>Top values: most frequent values with counts and percentages</li> <li>Entropy: Shannon entropy (information content)</li> <li>Gini impurity: concentration measure</li> <li>Diversity ratio: uniqueness measure</li> <li>Most common ratio: dominance of top value</li> <li>String length: mean, p90, distribution</li> <li>Special values: empty strings, whitespace-only</li> <li>Variants: case-insensitive and trimmed unique counts</li> </ul>"},{"location":"stats/categorical/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/categorical/#frequency-and-probability","title":"Frequency and Probability","text":"<p>Let \\(x_1, x_2, \\ldots, x_n\\) be the non-missing categorical values.</p> <p>Frequency of value \\(v\\):</p> \\[ f(v) = |\\{i : x_i = v\\}| \\] <p>Relative frequency (empirical probability):</p> \\[ p(v) = \\frac{f(v)}{n} \\] <p>Distinct count:</p> \\[ d = |\\{v : f(v) &gt; 0\\}| \\]"},{"location":"stats/categorical/#shannon-entropy","title":"Shannon Entropy","text":"<p>Measures the information content or uncertainty in the distribution:</p> \\[ H(X) = -\\sum_{v} p(v) \\log_2 p(v) \\] <p>where the sum is over all distinct values \\(v\\) with \\(p(v) &gt; 0\\).</p> <p>Properties: - \\(H(X) = 0\\) if one value has \\(p=1\\) (no uncertainty) - \\(H(X) = \\log_2 d\\) if all values equally likely (maximum entropy) - Units: bits of information</p> <p>Interpretation: - Low entropy (&lt; 1 bit): highly concentrated, predictable - Medium entropy (1-3 bits): moderate diversity - High entropy (&gt; 3 bits): high diversity, hard to predict</p> <p>Example: - Uniform distribution over 8 values: \\(H = \\log_2 8 = 3\\) bits - One value 90%, others 10%/9: \\(H \\approx 0.57\\) bits</p>"},{"location":"stats/categorical/#gini-impurity","title":"Gini Impurity","text":"<p>Measures the probability of misclassification if labels were assigned randomly according to the distribution:</p> \\[ \\text{Gini}(X) = 1 - \\sum_{v} p(v)^2 \\] <p>Properties: - \\(\\text{Gini}(X) = 0\\) if one value (no impurity) - \\(\\text{Gini}(X) = 1 - 1/d\\) if uniform over \\(d\\) values - Range: \\([0, 1)\\)</p> <p>Interpretation: - Low Gini (&lt; 0.2): concentrated distribution - Medium Gini (0.2-0.6): moderate spread - High Gini (&gt; 0.6): high diversity</p> <p>Use in ML: Decision trees use Gini impurity for splitting criteria.</p>"},{"location":"stats/categorical/#diversity-ratio","title":"Diversity Ratio","text":"<p>Simple measure of uniqueness:</p> \\[ D = \\frac{d}{n} \\] <p>where \\(d\\) = distinct count, \\(n\\) = total count.</p> <p>Interpretation: - \\(D \\to 0\\): low diversity (many repeats) - \\(D \\to 1\\): high diversity (mostly unique)</p> <p>Special cases: - \\(D = 1\\): all values unique (e.g., primary keys) - \\(D = 1/n\\): all values identical</p>"},{"location":"stats/categorical/#concentration-ratio","title":"Concentration Ratio","text":"<p>Fraction of observations in the top \\(k\\) values:</p> \\[ CR_k = \\frac{\\sum_{i=1}^{k} f(v_i)}{n} \\] <p>where \\(v_1, v_2, \\ldots\\) are values sorted by frequency (descending).</p> <p>Example: \\(CR_5 = 0.80\\) means top 5 values account for 80% of data.</p> <p>Interpretation: - High \\(CR_k\\): distribution dominated by few values - Low \\(CR_k\\): distribution spread across many values</p>"},{"location":"stats/categorical/#most-common-ratio","title":"Most Common Ratio","text":"<p>Dominance of the single most frequent value:</p> \\[ MCR = \\frac{f(v_{\\max})}{n} = p(v_{\\max}) \\] <p>Interpretation: - MCR &gt; 0.9: highly dominant category (nearly constant) - MCR &lt; 0.1: no dominant category (high diversity)</p>"},{"location":"stats/categorical/#streaming-algorithms","title":"Streaming Algorithms","text":""},{"location":"stats/categorical/#misra-gries-algorithm-for-top-k","title":"Misra-Gries Algorithm for Top-K","text":"<p>Finds the \\(k\\) most frequent items in a stream using O(k) space with frequency guarantees.</p> <p>Algorithm:</p> <ol> <li>Initialize: empty dictionary \\(M\\) (max size \\(k\\))</li> <li>For each value \\(v\\):</li> <li>If \\(v \\in M\\): increment \\(M[v]\\)</li> <li>Else if \\(|M| &lt; k\\): add \\(M[v] = 1\\)</li> <li>Else: decrement all counts in \\(M\\); remove zeros</li> <li>Output: items in \\(M\\) with estimated counts</li> </ol> <p>Guarantee: For any value \\(v\\) with true frequency \\(f(v)\\): - If \\(f(v) &gt; n/k\\), then \\(v\\) is in output - Estimated frequency within \\(n/k\\) of true frequency</p> <p>Space complexity: \\(O(k)\\) Update complexity: \\(O(k)\\) worst-case, \\(O(1)\\) amortized</p> <p>Mergeable: Yes (sum counters from multiple streams)</p> <p>Example: \\(k=50\\), \\(n=1,000,000\\) - Guaranteed to find all items with frequency &gt; 20,000 - Frequency estimates within \u00b120,000</p> <p>Reference: Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p>"},{"location":"stats/categorical/#kmv-sketch-for-distinct-count","title":"KMV Sketch for Distinct Count","text":"<p>Estimates cardinality using \\(k\\) minimum hash values.</p> <p>Algorithm:</p> <ol> <li>Initialize: empty set \\(S\\) (max size \\(k\\))</li> <li>For each value \\(v\\):</li> <li>Compute hash \\(h(v) \\in [0,1]\\)</li> <li>If \\(|S| &lt; k\\) or \\(h(v) &lt; \\max(S)\\):<ul> <li>Add \\(h(v)\\) to \\(S\\)</li> <li>If \\(|S| &gt; k\\): remove \\(\\max(S)\\)</li> </ul> </li> <li>Estimate:</li> </ol> \\[ \\hat{d} = \\frac{k-1}{x_k} \\] <p>where \\(x_k = \\max(S)\\) is the \\(k\\)-th smallest hash.</p> <p>Error bound:</p> \\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Space: \\(O(k)\\) Update: \\(O(\\log k)\\) (heap operations) Mergeable: Yes (union of sets)</p> <p>Example: \\(k=2048\\) - Error: ~2.2% (95% confidence) - Space: ~16 KB (assuming 64-bit hashes)</p> <p>Reference: Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p>"},{"location":"stats/categorical/#space-saving-algorithm-alternative","title":"Space-Saving Algorithm (Alternative)","text":"<p>Maintains top-k with guaranteed error bounds:</p> <p>Error bound: Estimated frequency within \\(\\epsilon n\\) where \\(\\epsilon = 1/k\\)</p> <p>Advantage over Misra-Gries: Tighter worst-case bounds, better for skewed distributions.</p> <p>Reference: Metwally, A., Agrawal, D., El Abbadi, A. (2005), \"Efficient Computation of Frequent and Top-k Elements in Data Streams\", ICDT.</p>"},{"location":"stats/categorical/#string-analysis","title":"String Analysis","text":""},{"location":"stats/categorical/#length-statistics","title":"Length Statistics","text":"<p>For string values, track:</p> <p>Mean length:</p> \\[ \\bar{L} = \\frac{1}{n} \\sum_{i=1}^{n} |x_i| \\] <p>where \\(|x_i|\\) is the character count of string \\(x_i\\).</p> <p>P90 length: 90th percentile of lengths (via reservoir sampling)</p> <p>Length distribution: Histogram of string lengths</p> <p>Use cases: - Detect outliers (abnormally long strings) - Validate constraints (max length) - Estimate storage requirements</p>"},{"location":"stats/categorical/#empty-strings","title":"Empty Strings","text":"<p>Count of strings that are: - Empty: <code>\"\"</code> - Whitespace-only: match <code>/^\\s*$/</code> - NULL vs. empty distinction</p> <p>Formula:</p> \\[ n_{\\text{empty}} = |\\{i : x_i = \"\" \\text{ or } x_i \\text{ matches } /^\\s*$/\\}| \\]"},{"location":"stats/categorical/#case-variants","title":"Case Variants","text":"<p>Estimate distinct count after case normalization:</p> \\[ d_{\\text{lower}} = |\\{v.\\text{lower}() : v \\in \\text{values}\\}| \\] <p>Interpretation: - \\(d_{\\text{lower}} &lt; d\\): case variants present (e.g., \"USA\", \"usa\") - \\(d_{\\text{lower}} = d\\): no case variants</p>"},{"location":"stats/categorical/#trim-variants","title":"Trim Variants","text":"<p>Estimate distinct count after removing leading/trailing whitespace:</p> \\[ d_{\\text{trim}} = |\\{v.\\text{strip}() : v \\in \\text{values}\\}| \\] <p>Interpretation: - \\(d_{\\text{trim}} &lt; d\\): whitespace variants present - \\(d_{\\text{trim}} = d\\): no trim variants</p>"},{"location":"stats/categorical/#chi-square-uniformity-test","title":"Chi-Square Uniformity Test","text":"<p>Test if the distribution is uniform (all categories equally likely).</p> <p>Null hypothesis: \\(p(v_1) = p(v_2) = \\cdots = p(v_d) = 1/d\\)</p> <p>Test statistic:</p> \\[ \\chi^2 = \\sum_{i=1}^{d} \\frac{(f(v_i) - E)^2}{E} \\] <p>where \\(E = n/d\\) is the expected frequency under uniformity.</p> <p>Distribution under \\(H_0\\): \\(\\chi^2_{d-1}\\) (chi-square with \\(d-1\\) degrees of freedom)</p> <p>P-value: \\(P(\\chi^2_{d-1} &gt; \\chi^2_{\\text{obs}})\\)</p> <p>Interpretation: - Small p-value (&lt; 0.05): reject uniformity (distribution is skewed) - Large p-value: consistent with uniform distribution</p> <p>Not implemented in current version</p> <p>Chi-square test is planned for future release.</p>"},{"location":"stats/categorical/#cardinality-categories","title":"Cardinality Categories","text":"<p>Classify categorical variables by distinct count:</p> Category Distinct Count Examples Boolean-like 2-3 Yes/No, True/False/Unknown Low cardinality 4-20 Status codes, categories Medium cardinality 21-100 US states, countries High cardinality 101-10,000 Zip codes, product IDs Very high cardinality &gt; 10,000 User IDs, URLs, emails <p>Recommended actions: - Low cardinality: Show all values in report - High cardinality: Show top-k only, estimate distinct - Very high cardinality: Consider as identifier (unique key)</p>"},{"location":"stats/categorical/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Misra-Gries \\(O(nk)\\) worst, \\(O(n)\\) amortized \\(O(k)\\) Top-k values KMV distinct \\(O(n \\log k)\\) \\(O(k)\\) Distinct count Entropy \\(O(n + d)\\) \\(O(d)\\) From frequency table String lengths \\(O(n)\\) \\(O(k)\\) Reservoir sample Exact distinct \\(O(n)\\) \\(O(d)\\) Hash set"},{"location":"stats/categorical/#configuration","title":"Configuration","text":"<p>Control categorical analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Top-k size (Misra-Gries)\nconfig.compute.top_k_size = 50  # Default\n\n# Distinct count sketch size (KMV)\nconfig.compute.uniques_sketch_size = 2_048  # Default\n\n# String length sample size\n# (Not separately configurable, uses numeric_sample_size)\n\n# Enable/disable case variants\n# (Always enabled, no toggle)\n\n# Enable/disable trim variants\n# (Always enabled, no toggle)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/categorical/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/categorical/#categoricalaccumulator-class","title":"CategoricalAccumulator Class","text":"<pre><code>class CategoricalAccumulator:\n    def __init__(self, name: str, config: CategoricalConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n\n        # Top-k values\n        self._topk = MisraGries(config.top_k_size)\n\n        # Distinct count\n        self._uniques = KMV(config.uniques_sketch_size)\n        self._uniques_lower = KMV(config.uniques_sketch_size)  # Case-insensitive\n        self._uniques_strip = KMV(config.uniques_sketch_size)  # Trimmed\n\n        # String lengths\n        self._len_sum = 0\n        self._len_n = 0\n        self._len_sample = ReservoirSampler(config.length_sample_size)\n\n        # Special values\n        self._empty_count = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing\n        # Update top-k\n        # Update distinct sketches (original, lower, strip)\n        # Track string lengths\n        # Count empty strings\n        pass\n\n    def finalize(self) -&gt; CategoricalSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Get top values from Misra-Gries\n        # Estimate distinct from KMV\n        # Compute entropy and Gini\n        # Compute string length stats\n        return CategoricalSummary(...)\n</code></pre>"},{"location":"stats/categorical/#validation","title":"Validation","text":"<p>PySuricata validates categorical algorithms:</p> <ul> <li>Exact vs approximate: Compare KMV estimate to exact count (small datasets)</li> <li>Top-k correctness: Verify all items with \\(f &gt; n/k\\) are found</li> <li>Entropy bounds: Check \\(0 \\le H(X) \\le \\log_2 d\\)</li> <li>Gini bounds: Check \\(0 \\le \\text{Gini}(X) &lt; 1\\)</li> <li>Mergeability: Verify merge = concatenate (for Misra-Gries, KMV)</li> </ul>"},{"location":"stats/categorical/#examples","title":"Examples","text":""},{"location":"stats/categorical/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"country\": [\"USA\", \"UK\", \"USA\", \"DE\", None, \"USA\", \"FR\"]\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/categorical/#high-cardinality-column","title":"High-Cardinality Column","text":"<pre><code># Column with 10,000 unique values\ndf = pd.DataFrame({\n    \"user_id\": [f\"user_{i}\" for i in range(100_000)]\n})\n\nconfig = ReportConfig()\nconfig.compute.top_k_size = 100  # Show top 100\nconfig.compute.uniques_sketch_size = 4_096  # More accurate distinct\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/categorical/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\ncountry_stats = stats[\"columns\"][\"country\"]\n\nprint(f\"Distinct: {country_stats['distinct']}\")\nprint(f\"Top value: {country_stats['top_values'][0]}\")\nprint(f\"Entropy: {country_stats['entropy']:.2f} bits\")\nprint(f\"Gini: {country_stats['gini']:.3f}\")\n</code></pre>"},{"location":"stats/categorical/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/categorical/#high-entropy","title":"High Entropy","text":"<p>\\(H(X) &gt; 5\\) bits suggests: - Many distinct values (&gt; 32) - Fairly uniform distribution - High information content - Possibly high cardinality (consider as identifier)</p>"},{"location":"stats/categorical/#low-entropy","title":"Low Entropy","text":"<p>\\(H(X) &lt; 1\\) bit suggests: - Few distinct values (&lt; 4 effective) - Skewed distribution (one value dominates) - Low information content - Consider as low-cardinality categorical</p>"},{"location":"stats/categorical/#high-gini","title":"High Gini","text":"<p>\\(\\text{Gini} &gt; 0.7\\) suggests: - Values well-distributed - No single dominant category - Good for stratification</p>"},{"location":"stats/categorical/#low-gini","title":"Low Gini","text":"<p>\\(\\text{Gini} &lt; 0.2\\) suggests: - One or few values dominate - Imbalanced distribution - Consider as nearly constant column</p>"},{"location":"stats/categorical/#special-cases","title":"Special Cases","text":""},{"location":"stats/categorical/#all-unique-primary-key","title":"All Unique (Primary Key)","text":"<ul> <li>Distinct count \\(d = n\\)</li> <li>Entropy \\(H = \\log_2 n\\) (maximum)</li> <li>Diversity ratio \\(D = 1.0\\)</li> <li>Top-k meaningless (all have count 1)</li> </ul> <p>Recommendation: Flag as identifier, exclude from analysis.</p>"},{"location":"stats/categorical/#nearly-constant","title":"Nearly Constant","text":"<ul> <li>Distinct count \\(d = 2\\) with \\(p_1 &gt; 0.99\\)</li> <li>Entropy \\(H &lt; 0.1\\) bits</li> <li>Gini \\(&lt; 0.02\\)</li> </ul> <p>Recommendation: Consider removing (low variance).</p>"},{"location":"stats/categorical/#many-empty-strings","title":"Many Empty Strings","text":"<ul> <li>Empty count &gt; 10% of non-null</li> </ul> <p>Possible data quality issue: Missing values encoded as empty strings.</p>"},{"location":"stats/categorical/#references","title":"References","text":"<ol> <li> <p>Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p> </li> <li> <p>Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p> </li> <li> <p>Metwally, A., Agrawal, D., El Abbadi, A. (2005), \"Efficient Computation of Frequent and Top-k Elements in Data Streams\", ICDT.</p> </li> <li> <p>Shannon, C.E. (1948), \"A Mathematical Theory of Communication\", Bell System Technical Journal, 27: 379\u2013423.</p> </li> <li> <p>Breiman, L. et al. (1984), Classification and Regression Trees, Wadsworth.</p> </li> <li> <p>Wikipedia: Entropy (information theory) - Link</p> </li> <li> <p>Wikipedia: Decision tree learning - Link</p> </li> </ol>"},{"location":"stats/categorical/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Numeric variables</li> <li>Sketch Algorithms - KMV, Misra-Gries deep dive</li> <li>Data Quality - Quality metrics</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/datetime/","title":"DateTime Variable Analysis","text":"<p>Comprehensive documentation for temporal data analysis in PySuricata, including time distributions, seasonality detection, and gap analysis.</p>"},{"location":"stats/datetime/#overview","title":"Overview","text":"<p>PySuricata treats datetime variables as columns with temporal types (datetime64, timestamp). Analysis focuses on temporal patterns, distributions, and data quality.</p>"},{"location":"stats/datetime/#key-features","title":"Key Features","text":"<ul> <li>Temporal range: min/max timestamps, time span</li> <li>Distribution analysis: hour, day-of-week, month patterns</li> <li>Monotonicity detection: sorted sequences</li> <li>Gap analysis: missing time periods</li> <li>Timeline visualization: temporal coverage</li> <li>Timezone handling: UTC normalization</li> </ul>"},{"location":"stats/datetime/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each datetime column:</p> <ul> <li>Count: non-null timestamps, missing percentage</li> <li>Range: minimum and maximum timestamps</li> <li>Span: total time covered (in days, hours, etc.)</li> <li>Hour distribution: counts by hour (0-23)</li> <li>Day-of-week distribution: counts by weekday (Mon-Sun)</li> <li>Month distribution: counts by month (Jan-Dec)</li> <li>Monotonicity: increasing/decreasing/mixed</li> <li>Timeline chart: visual temporal distribution</li> </ul>"},{"location":"stats/datetime/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/datetime/#temporal-measures","title":"Temporal Measures","text":"<p>Let \\(t_1, t_2, \\ldots, t_n\\) be the non-missing timestamp values (in seconds since epoch or similar).</p> <p>Time span:</p> \\[ \\Delta t = \\max(t) - \\min(t) \\] <p>Typically reported in days, hours, or appropriate units.</p> <p>Sampling rate (average):</p> \\[ r = \\frac{n}{\\Delta t} \\] <p>Average observations per unit time (e.g., rows per day).</p> <p>Time density:</p> \\[ \\rho = \\frac{n}{t_{\\max} - t_{\\min}} \\] <p>Similar to sampling rate; measures temporal concentration.</p>"},{"location":"stats/datetime/#monotonicity-coefficient","title":"Monotonicity Coefficient","text":"<p>Measures how sorted the timestamps are.</p> <p>Strictly increasing pairs:</p> \\[ n_{\\uparrow} = |\\{i : t_i &lt; t_{i+1}\\}| \\] <p>Monotonicity coefficient:</p> \\[ M = \\frac{n_{\\uparrow}}{n - 1} \\] <p>Interpretation: - \\(M = 1\\): strictly increasing (perfectly sorted) - \\(M = 0\\): strictly decreasing (reverse sorted) - \\(M \\approx 0.5\\): random order</p> <p>Use cases: - Detect time-sorted data (logs, time series) - Identify reverse chronological order - Flag shuffled temporal data</p>"},{"location":"stats/datetime/#temporal-entropy","title":"Temporal Entropy","text":"<p>Distribution entropy over time bins:</p> \\[ H_{\\text{time}} = -\\sum_{b \\in \\text{bins}} p_b \\log_2 p_b \\] <p>where \\(p_b\\) is the proportion of timestamps in bin \\(b\\).</p> <p>High entropy: events spread uniformly over time Low entropy: events concentrated in specific periods</p>"},{"location":"stats/datetime/#seasonality-detection","title":"Seasonality Detection","text":"<p>Detect periodic patterns using Fourier analysis or autocorrelation.</p> <p>Autocorrelation at lag \\(\\tau\\):</p> \\[ \\rho(\\tau) = \\frac{\\text{Cov}(X_t, X_{t+\\tau})}{\\text{Var}(X_t)} \\] <p>For count time series \\(X_t\\) (observations per time unit).</p> <p>Significant autocorrelation at lag \\(\\tau\\) suggests periodicity with period \\(\\tau\\).</p> <p>Common periods: - Daily: \\(\\tau = 1\\) day - Weekly: \\(\\tau = 7\\) days - Monthly: \\(\\tau \\approx 30\\) days - Yearly: \\(\\tau = 365\\) days</p> <p>Not fully implemented</p> <p>Seasonality detection via autocorrelation is planned for future release. Current version shows hour/day/month distributions which reveal patterns manually.</p>"},{"location":"stats/datetime/#temporal-distributions","title":"Temporal Distributions","text":""},{"location":"stats/datetime/#hour-distribution","title":"Hour Distribution","text":"<p>Count of observations by hour of day (0-23):</p> \\[ n_h = |\\{i : \\text{hour}(t_i) = h\\}| \\quad \\text{for } h \\in \\{0, 1, \\ldots, 23\\} \\] <p>Use cases: - Detect business hours (9am-5pm peaks) - Identify batch job times (off-hours spikes) - Analyze user activity patterns</p> <p>Visualization: Bar chart showing hourly counts.</p>"},{"location":"stats/datetime/#day-of-week-distribution","title":"Day-of-Week Distribution","text":"<p>Count by day (Monday=0, Sunday=6):</p> \\[ n_d = |\\{i : \\text{weekday}(t_i) = d\\}| \\quad \\text{for } d \\in \\{0, 1, \\ldots, 6\\} \\] <p>Use cases: - Detect weekday vs. weekend patterns - Identify business day data - Analyze periodic behavior</p> <p>Visualization: Bar chart showing daily counts.</p>"},{"location":"stats/datetime/#month-distribution","title":"Month Distribution","text":"<p>Count by month (Jan=1, Dec=12):</p> \\[ n_m = |\\{i : \\text{month}(t_i) = m\\}| \\quad \\text{for } m \\in \\{1, 2, \\ldots, 12\\} \\] <p>Use cases: - Detect seasonal effects - Identify fiscal quarters - Analyze annual patterns</p> <p>Visualization: Bar chart showing monthly counts.</p>"},{"location":"stats/datetime/#timeline-histogram","title":"Timeline Histogram","text":"<p>Temporal histogram showing observation density over time:</p> <ol> <li>Divide time range into \\(k\\) bins</li> <li>Count observations in each bin</li> <li>Display as histogram</li> </ol> <p>Bin width: \\(w = \\Delta t / k\\)</p> <p>Reveals: - Gaps in data collection - Burst periods (high activity) - Data quality issues (missing periods)</p>"},{"location":"stats/datetime/#gap-analysis","title":"Gap Analysis","text":"<p>Detect missing time periods in temporal data.</p> <p>Expected interval:</p> \\[ \\Delta_{\\text{exp}} = \\text{median}(\\{t_{i+1} - t_i : i = 1, \\ldots, n-1\\}) \\] <p>Gap threshold:</p> \\[ \\theta = c \\cdot \\Delta_{\\text{exp}} \\] <p>where \\(c &gt; 1\\) (e.g., \\(c = 2\\) or \\(c = 5\\)).</p> <p>Gaps:</p> \\[ G = \\{(t_i, t_{i+1}) : t_{i+1} - t_i &gt; \\theta\\} \\] <p>Gap statistics: - Number of gaps: \\(|G|\\) - Total missing time: \\(\\sum_{(t_i, t_{i+1}) \\in G} (t_{i+1} - t_i - \\theta)\\) - Longest gap: \\(\\max_{(t_i, t_{i+1}) \\in G} (t_{i+1} - t_i)\\)</p> <p>Not implemented in current version</p> <p>Gap analysis is planned for future release.</p>"},{"location":"stats/datetime/#timezone-handling","title":"Timezone Handling","text":"<p>All timestamps are normalized to UTC for analysis:</p> \\[ t_{\\text{UTC}} = t_{\\text{local}} - \\text{offset} \\] <p>Rationale: - Consistent comparisons across time zones - Avoids DST complications - Standard for distributed systems</p> <p>Reported in UI: Original timezone if available, UTC for calculations.</p>"},{"location":"stats/datetime/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Min/max \\(O(n)\\) \\(O(1)\\) Single pass Hour/day/month counts \\(O(n)\\) \\(O(1)\\) Fixed-size arrays (24, 7, 12) Monotonicity \\(O(n)\\) \\(O(1)\\) Compare adjacent pairs Timeline histogram \\(O(n)\\) \\(O(k)\\) \\(k\\) bins Gap detection \\(O(n \\log n)\\) \\(O(n)\\) Sorting required"},{"location":"stats/datetime/#configuration","title":"Configuration","text":"<p>Control datetime analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Timeline histogram bins\n# (Not separately configurable, uses default 50)\n\n# Gap detection threshold\n# (Not yet implemented)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/datetime/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/datetime/#datetimeaccumulator-class","title":"DatetimeAccumulator Class","text":"<pre><code>class DatetimeAccumulator:\n    def __init__(self, name: str, config: DatetimeConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n\n        # Range tracking\n        self.min_ts = None\n        self.max_ts = None\n\n        # Distribution counters\n        self.hour_counts = [0] * 24\n        self.weekday_counts = [0] * 7\n        self.month_counts = [0] * 12\n\n        # Monotonicity tracking\n        self.prev_ts = None\n        self.monotonic_inc = 0\n        self.monotonic_dec = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of timestamps\"\"\"\n        # Convert to UTC\n        # Update min/max\n        # Count by hour/day/month\n        # Track monotonicity\n        pass\n\n    def finalize(self) -&gt; DatetimeSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute span\n        # Compute monotonicity coefficient\n        # Format distributions\n        # Build timeline\n        return DatetimeSummary(...)\n</code></pre>"},{"location":"stats/datetime/#examples","title":"Examples","text":""},{"location":"stats/datetime/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"timestamp\": pd.date_range(\"2023-01-01\", periods=1000, freq=\"H\")\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/datetime/#time-series-data","title":"Time Series Data","text":"<pre><code># Stock prices\ndf = pd.read_csv(\"stocks.csv\", parse_dates=[\"date\"])\n\nreport = profile(df)\n# Analyze temporal patterns\n</code></pre>"},{"location":"stats/datetime/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nts_stats = stats[\"columns\"][\"timestamp\"]\n\nprint(f\"Min: {ts_stats['min']}\")\nprint(f\"Max: {ts_stats['max']}\")\nprint(f\"Span: {ts_stats['max'] - ts_stats['min']}\")\nprint(f\"Hour distribution: {ts_stats['hour_distribution']}\")\n</code></pre>"},{"location":"stats/datetime/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/datetime/#monotonically-increasing","title":"Monotonically Increasing","text":"<p>\\(M = 1.0\\): Timestamps are sorted (common for logs, time series).</p> <p>Implications: - Data collected in chronological order - Suitable for time series analysis - May enable optimizations (binary search)</p>"},{"location":"stats/datetime/#random-order","title":"Random Order","text":"<p>\\(M \\approx 0.5\\): Timestamps shuffled or unordered.</p> <p>Implications: - Data may need sorting for analysis - Not a true time series - Consider sorting before visualization</p>"},{"location":"stats/datetime/#hourly-patterns","title":"Hourly Patterns","text":"<p>Peak in business hours (9am-5pm): - Typical for user activity data - Web traffic, transactions, etc.</p> <p>Flat distribution: - Automated data collection (24/7 sensors) - No human activity pattern</p>"},{"location":"stats/datetime/#weekly-patterns","title":"Weekly Patterns","text":"<p>Weekday peaks, weekend lows: - Business activity data - Employee-generated events</p> <p>Uniform distribution: - 24/7 operations - Automated systems</p>"},{"location":"stats/datetime/#monthly-patterns","title":"Monthly Patterns","text":"<p>Seasonal variations: - Retail sales (holiday spikes) - Weather data (summer/winter)</p> <p>Uniform distribution: - No seasonal effect - Steady-state process</p>"},{"location":"stats/datetime/#special-cases","title":"Special Cases","text":""},{"location":"stats/datetime/#all-same-timestamp","title":"All Same Timestamp","text":"<ul> <li>Distinct count = 1</li> <li>Span = 0</li> <li>Monotonicity undefined</li> </ul> <p>Possible issue: Snapshot data, not time series.</p>"},{"location":"stats/datetime/#large-gaps","title":"Large Gaps","text":"<p>Long periods without data: - Data collection interruptions - System downtime - Seasonal business (e.g., ski resorts)</p> <p>Recommendation: Investigate gaps, document known outages.</p>"},{"location":"stats/datetime/#future-timestamps","title":"Future Timestamps","text":"<p>Timestamps &gt; current time: - Data quality issue - Incorrect timezone - System clock skew</p> <p>Recommendation: Flag as data quality problem.</p>"},{"location":"stats/datetime/#references","title":"References","text":"<ol> <li> <p>Box, G.E.P., Jenkins, G.M., Reinsel, G.C. (2015), Time Series Analysis: Forecasting and Control, Wiley.</p> </li> <li> <p>Brockwell, P.J., Davis, R.A. (2016), Introduction to Time Series and Forecasting, Springer.</p> </li> <li> <p>Cleveland, R.B. et al. (1990), \"STL: A Seasonal-Trend Decomposition Procedure Based on Loess\", Journal of Official Statistics, 6(1): 3\u201373.</p> </li> <li> <p>Wikipedia: Autocorrelation - Link</p> </li> <li> <p>Wikipedia: Seasonality - Link</p> </li> </ol>"},{"location":"stats/datetime/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - For temporal metrics as numbers</li> <li>Data Quality - Quality checks</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/numeric/","title":"Numeric Variable Analysis","text":"<p>This page provides comprehensive technical documentation for how pysuricata profiles and summarizes numerical (continuous and discrete) columns at scale using proven streaming algorithms with mathematical guarantees.</p> <p>Audience</p> <p>Designed for users who want to understand and trust the numbers in the HTML report, as well as contributors who need to modify or extend the accumulator implementations.</p>"},{"location":"stats/numeric/#overview","title":"Overview","text":"<p>PySuricata treats a numerical variable as any column with machine type among <code>{int8, int16, int32, int64, float32, float64, decimal}</code> (nullable). Values may include <code>NaN</code>, <code>\u00b1Inf</code>, and missing markers, all handled appropriately.</p> <p>All statistics are computed incrementally via stateful accumulators using single-pass streaming algorithms, enabling processing of datasets larger than available RAM.</p>"},{"location":"stats/numeric/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each numeric column, the report includes:</p> <ul> <li>Count: non-null values, missing percentage</li> <li>Central tendency: mean, median</li> <li>Dispersion: variance, standard deviation, IQR, MAD, coefficient of variation</li> <li>Shape: skewness, excess kurtosis, bimodality hints</li> <li>Range: min, max, range</li> <li>Quantiles: configurable set (default: 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99)</li> <li>Outliers: IQR fences, z-score, MAD-based detection</li> <li>Distribution: histogram with adaptive binning</li> <li>Special values: zeros, negatives, infinities</li> <li>Extremes: min/max values with row indices</li> <li>Confidence intervals: 95% CI for mean</li> <li>Correlations: top correlations with other numeric columns (optional)</li> </ul>"},{"location":"stats/numeric/#quality-flags-and-indicators","title":"Quality Flags and Indicators","text":"<p>Numeric variable cards display quality flags (colored chips) that highlight important data characteristics and potential issues. These flags are automatically detected based on statistical thresholds and help you quickly identify data quality concerns or interesting patterns.</p>"},{"location":"stats/numeric/#flag-categories","title":"Flag Categories","text":""},{"location":"stats/numeric/#critical-issues-red-flags","title":"\ud83d\udd34 Critical Issues (Red Flags)","text":"<p>Missing (&gt;20%) - Threshold: More than 20% of values are missing - Impact: Significant data loss may bias analysis and require imputation - Action: Investigate data collection process, consider imputation strategies</p> <p>Has \u221e - Threshold: Any infinite values (\u00b1\u221e) present - Impact: Indicates calculation overflow, division by zero, or data corruption - Action: Investigate source of infinities, may need to exclude or cap values</p> <p>Zero-inflated (\u226550%) - Threshold: 50% or more values are exactly zero - Impact: May indicate sparse data, special zero category, or data quality issue - Action: Consider zero-inflated models or separate analysis of zeros vs non-zeros</p> <p>Constant - Threshold: All non-missing values are identical - Impact: Zero variance, column provides no information for modeling - Action: Consider removing column or investigating data collection</p> <p>Quasi-constant - Threshold: Very low cardinality (typically &gt;95% same value) - Impact: Limited information content, may not be useful for analysis - Action: Review if column adds value to analysis</p> <p>Many outliers - Threshold: High proportion of outliers detected (varies by method) - Impact: Outliers may skew mean, standard deviation, and other statistics - Action: Investigate outliers, consider robust statistics or capping</p> <p>Heavy-tailed - Threshold: High excess kurtosis (typically |kurtosis| &gt; 3) - Impact: More extreme values than normal distribution, classical statistics may be unreliable - Action: Use robust statistics, consider transformations, or non-parametric methods</p>"},{"location":"stats/numeric/#distribution-warnings-orange-flags","title":"\ud83d\udfe0 Distribution Warnings (Orange Flags)","text":"<p>Missing (\u226420%) - Threshold: 0% to 20% missing values - Impact: Minor concern, may introduce slight bias - Action: Monitor data quality, document missingness pattern</p> <p>Has negatives (&gt;10%) - Threshold: More than 10% of values are negative - Impact: May be unexpected for metrics that should be positive (age, price, count, etc.) - Action: Verify this is expected for your data domain</p> <p>Skewed Right - Threshold: Positive skewness detected (typically &gt; 1) - Impact: Long right tail, mean &gt; median, affects parametric assumptions - Action: Consider log transformation or report median instead of mean</p> <p>Skewed Left - Threshold: Negative skewness detected (typically &lt; -1) - Impact: Long left tail, mean &lt; median, affects parametric assumptions - Action: Consider transformation or non-parametric methods</p> <p>Discrete - Threshold: Low unique count relative to sample size - Impact: Column may behave more like categorical than continuous - Action: Consider treating as categorical or ordinal</p> <p>Heaping - Threshold: Values cluster at round numbers (detected via digit analysis) - Impact: May indicate measurement precision issues, rounding, or self-reporting - Action: Be aware of artificial clustering in distribution analysis</p> <p>Possibly bimodal - Threshold: Bimodality coefficient suggests two modes - Impact: May represent two distinct populations or processes - Action: Investigate if data contains meaningful subgroups</p> <p>Some outliers - Threshold: Moderate outliers present (0.3% &lt; outliers \u2264 1%) - Impact: Minor outlier presence may slightly affect statistics - Action: Review outlier values, consider if they're valid extreme values</p> <p>Zero-inflated (&lt;50%) - Threshold: 30-50% zero values (significant but not dominant) - Impact: High zero percentage affects distribution shape - Action: Consider zero-inflated models if modeling zeros separately</p>"},{"location":"stats/numeric/#positive-characteristics-green-flags","title":"\ud83d\udfe2 Positive Characteristics (Green Flags)","text":"<p>Positive-only - Threshold: All values &gt; 0, no zeros or negatives - Impact: Safe for log transformation, geometric mean, etc. - Benefit: Simplifies many analyses and transformations</p> <p>\u2248 Normal (JB) - Threshold: Jarque-Bera test passes (typically JB \u03c7\u00b2 &lt; critical value) - Impact: Data approximately follows normal distribution - Benefit: Parametric tests valid, mean/std are appropriate summaries</p> <p>Log-scale? - Threshold: Log transformation would improve distribution (based on range and skewness) - Impact: Suggests log scale may reveal patterns - Benefit: May linearize relationships, normalize distribution</p> <p>Monotonic \u2191 - Threshold: Values strictly or weakly increase - Impact: May be index, timestamp, cumulative value, or sorted data - Benefit: Indicates ordering or temporal relationship</p> <p>Monotonic \u2193 - Threshold: Values strictly or weakly decrease - Impact: May be countdown, priority, or reverse-sorted data - Benefit: Indicates ordering relationship</p>"},{"location":"stats/numeric/#additional-badges","title":"Additional Badges","text":"<p><code>Numeric</code> Badge - Always present on numeric variable cards - Indicates this is a continuous or discrete numeric column - Distinguished from categorical, datetime, or boolean types</p> <p>Data Type Chip (e.g., <code>int64</code>, <code>float64</code>, <code>uint8</code>) - Shows the specific data type from the source dataframe - Important for understanding precision, range, and memory usage - Helps identify potential overflow or underflow issues</p> <p><code>approx</code> Badge - Appears when statistics use sampling or approximation - Indicates quantiles, unique counts, or other metrics are estimated - Typical when dataset is large and full data scan isn't used - Approximations are still statistically valid with high accuracy</p>"},{"location":"stats/numeric/#interpreting-multiple-flags","title":"Interpreting Multiple Flags","text":"<p>A column can display multiple flags simultaneously. Common combinations:</p> <ul> <li><code>Missing</code> + <code>Skewed Right</code>: Incomplete data with asymmetric distribution</li> <li><code>Zero-inflated</code> + <code>Positive-only</code>: Many zeros, but no negatives</li> <li><code>Heavy-tailed</code> + <code>Many outliers</code>: Extreme values with fat tails</li> <li><code>Discrete</code> + <code>Heaping</code>: Categorical-like integers with rounding</li> </ul> <p>Best Practice: Address flags by priority: 1. \ud83d\udd34 Red flags first (data quality issues) 2. \ud83d\udfe0 Orange flags next (distribution concerns) 3. \ud83d\udfe2 Use green flags to inform analysis approach</p>"},{"location":"stats/numeric/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/numeric/#notation","title":"Notation","text":"<p>Let \\(x_1, x_2, \\ldots, x_n\\) be the non-missing, finite observations for a column.</p>"},{"location":"stats/numeric/#central-tendency","title":"Central Tendency","text":"<p>Mean (arithmetic average):</p> \\[ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i \\] <p>Median:</p> <p>The value \\(m\\) such that at least half the observations are \\(\\le m\\) and at least half are \\(\\ge m\\). For even \\(n\\), typically the average of the two middle values.</p> \\[ \\text{median} = \\begin{cases} x_{(n+1)/2} &amp; \\text{if } n \\text{ odd} \\\\ \\frac{x_{n/2} + x_{n/2+1}}{2} &amp; \\text{if } n \\text{ even} \\end{cases} \\] <p>where \\(x_{(k)}\\) denotes the \\(k\\)-th order statistic (sorted values).</p>"},{"location":"stats/numeric/#dispersion","title":"Dispersion","text":"<p>Sample variance (unbiased):</p> \\[ s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i-\\bar{x})^2 \\] <p>Standard deviation:</p> \\[ s = \\sqrt{s^2} \\] <p>Coefficient of variation (CV):</p> \\[ \\text{CV} = \\frac{s}{|\\bar{x}|} \\times 100\\% \\] <p>Measures relative variability; useful for comparing dispersion across variables with different scales.</p> <p>Interquartile range (IQR):</p> \\[ \\text{IQR} = Q_{0.75} - Q_{0.25} \\] <p>Robust measure of spread, resistant to outliers.</p> <p>Median absolute deviation (MAD):</p> \\[ \\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|) \\] <p>Highly robust measure of variability; often preferred over standard deviation for non-normal data.</p>"},{"location":"stats/numeric/#shape-statistics","title":"Shape Statistics","text":"<p>Skewness (Fisher-Pearson, \\(g_1\\)):</p> <p>Measures asymmetry of the distribution.</p> \\[ g_1 = \\frac{n}{(n-1)(n-2)} \\cdot \\frac{m_3}{s^3} \\] <p>where \\(m_3 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^3\\) is the third central moment.</p> <p>Interpretation: - \\(g_1 = 0\\): symmetric - \\(g_1 &gt; 0\\): right-skewed (long right tail) - \\(g_1 &lt; 0\\): left-skewed (long left tail)</p> <p>Excess kurtosis (\\(g_2\\)):</p> <p>Measures tail heaviness relative to normal distribution.</p> \\[ g_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\cdot \\frac{m_4}{s^4} - \\frac{3(n-1)^2}{(n-2)(n-3)} \\] <p>where \\(m_4 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^4\\) is the fourth central moment.</p> <p>Interpretation: - \\(g_2 = 0\\): normal (mesokurtic) - \\(g_2 &gt; 0\\): heavy tails (leptokurtic) - \\(g_2 &lt; 0\\): light tails (platykurtic)</p>"},{"location":"stats/numeric/#quantiles","title":"Quantiles","text":"<p>Quantile function \\(Q(p)\\):</p> <p>For probability \\(p \\in [0,1]\\), the \\(p\\)-quantile is:</p> \\[ Q(p) = \\inf\\{x : F(x) \\ge p\\} \\] <p>where \\(F(x) = \\mathbb{P}(X \\le x)\\) is the cumulative distribution function.</p> <p>Common quantiles: - \\(Q(0.25)\\): first quartile (Q1) - \\(Q(0.50)\\): median (Q2) - \\(Q(0.75)\\): third quartile (Q3)</p>"},{"location":"stats/numeric/#confidence-intervals","title":"Confidence Intervals","text":"<p>95% confidence interval for mean:</p> <p>Assuming approximate normality (or large \\(n\\) by CLT):</p> \\[ \\text{CI}_{0.95}(\\bar{x}) = \\bar{x} \\pm t_{n-1,0.975} \\cdot \\frac{s}{\\sqrt{n}} \\] <p>where \\(t_{n-1,0.975}\\) is the 97.5th percentile of Student's t-distribution with \\(n-1\\) degrees of freedom.</p> <p>For large \\(n\\), \\(t_{n-1,0.975} \\approx 1.96\\).</p>"},{"location":"stats/numeric/#outlier-detection","title":"Outlier Detection","text":"<p>IQR fences (Tukey's method):</p> <p>Lower fence:</p> \\[ L = Q_{0.25} - 1.5 \\cdot \\text{IQR} \\] <p>Upper fence:</p> \\[ U = Q_{0.75} + 1.5 \\cdot \\text{IQR} \\] <p>Values outside \\([L, U]\\) are flagged as outliers. For \"extreme\" outliers, use multiplier 3.0 instead of 1.5.</p> <p>Modified z-score (robust):</p> \\[ M_i = 0.6745 \\cdot \\frac{x_i - \\text{median}(x)}{\\text{MAD}} \\] <p>Flag if \\(|M_i| &gt; 3.5\\). The constant 0.6745 makes MAD consistent with standard deviation under normality.</p> <p>Classical z-score:</p> \\[ Z_i = \\frac{x_i - \\bar{x}}{s} \\] <p>Flag if \\(|Z_i| &gt; 3\\). Sensitive to outliers themselves (not robust).</p>"},{"location":"stats/numeric/#streaming-algorithms","title":"Streaming Algorithms","text":""},{"location":"stats/numeric/#welfords-online-algorithm","title":"Welford's Online Algorithm","text":"<p>For computing mean and variance in a single pass with O(1) memory and numerical stability.</p> <p>Initialization:</p> \\[ n = 0, \\quad \\mu = 0, \\quad M_2 = 0 \\] <p>Update step: for each new value \\(x\\):</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\mu &amp;\\leftarrow \\mu + \\frac{\\delta}{n} \\\\ \\delta_2 &amp;= x - \\mu \\\\ M_2 &amp;\\leftarrow M_2 + \\delta \\cdot \\delta_2 \\end{aligned} \\] <p>Finalize:</p> \\[ \\text{mean} = \\mu, \\quad \\text{variance} = \\frac{M_2}{n-1} \\] <p>Properties: - Numerical stability: avoids catastrophic cancellation in \\(\\sum x_i^2 - n\\bar{x}^2\\) - Exact: produces same result as two-pass method (up to FP rounding) - Online: updates in O(1) time per value</p> <p>Reference: Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p>"},{"location":"stats/numeric/#higher-moments-skewness-kurtosis","title":"Higher Moments (Skewness, Kurtosis)","text":"<p>Extending Welford to track \\(M_3\\) and \\(M_4\\):</p> <p>Update step:</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\delta_n &amp;= \\frac{\\delta}{n} \\\\ \\delta_n^2 &amp;= \\delta_n^2 \\\\ \\mu &amp;\\leftarrow \\mu + \\delta_n \\\\ M_4 &amp;\\leftarrow M_4 + \\delta \\left(\\delta^3 \\frac{n(n-1)}{n^3} + 6\\delta_n M_2 - 4\\delta_n M_3\\right) \\\\ M_3 &amp;\\leftarrow M_3 + \\delta \\left(\\delta^2 \\frac{n(n-1)}{n^2} - 3\\delta_n M_2\\right) \\\\ M_2 &amp;\\leftarrow M_2 + \\delta(\\delta - \\delta_n) \\end{aligned} \\] <p>Finalize:</p> \\[ \\begin{aligned} s^2 &amp;= \\frac{M_2}{n-1} \\\\ g_1 &amp;= \\frac{n}{(n-1)(n-2)} \\cdot \\frac{M_3/n}{(s^2)^{3/2}} \\\\ g_2 &amp;= \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\cdot \\frac{M_4/n}{(s^2)^2} - \\frac{3(n-1)^2}{(n-2)(n-3)} \\end{aligned} \\]"},{"location":"stats/numeric/#pebays-parallel-merge-formulas","title":"P\u00e9bay's Parallel Merge Formulas","text":"<p>To combine results from multiple chunks or parallel threads, P\u00e9bay's formulas enable exact merging of moments.</p> <p>Given two partial states: - State A: \\((n_a, \\mu_a, M_{2a}, M_{3a}, M_{4a})\\) - State B: \\((n_b, \\mu_b, M_{2b}, M_{3b}, M_{4b})\\)</p> <p>Define:</p> \\[ \\delta = \\mu_b - \\mu_a, \\quad n = n_a + n_b \\] <p>Merged state:</p> \\[ \\begin{aligned} \\mu &amp;= \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\\\ M_2 &amp;= M_{2a} + M_{2b} + \\delta^2 \\cdot \\frac{n_a n_b}{n} \\\\ M_3 &amp;= M_{3a} + M_{3b} + \\delta^3 \\cdot \\frac{n_a n_b (n_a - n_b)}{n^2} + 3\\delta \\cdot \\frac{n_a M_{2b} - n_b M_{2a}}{n} \\\\ M_4 &amp;= M_{4a} + M_{4b} + \\delta^4 \\cdot \\frac{n_a n_b (n_a^2 - n_a n_b + n_b^2)}{n^3} \\\\ &amp;\\quad + 6\\delta^2 \\cdot \\frac{n_a n_b}{n^2}(n_a M_{2b} + n_b M_{2a}) + 4\\delta \\cdot \\frac{n_a M_{3b} - n_b M_{3a}}{n} \\end{aligned} \\] <p>Properties: - Associative: order of merging doesn't matter (up to FP rounding) - Exact: same result as single-pass over concatenated data - Parallelizable: enables multi-core and distributed computation</p> <p>Reference: P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212.</p>"},{"location":"stats/numeric/#quantile-estimation","title":"Quantile Estimation","text":""},{"location":"stats/numeric/#exact-quantiles-reservoir-sampling","title":"Exact Quantiles (Reservoir Sampling)","text":"<p>For exact quantiles, maintain a reservoir sample of size \\(k\\) (default 20,000):</p> <ol> <li>First \\(k\\) values: keep all</li> <li>For value \\(i &gt; k\\): include with probability \\(k/i\\), replacing random existing sample</li> </ol> <p>Uniform guarantee: Every subset of size \\(k\\) has equal probability.</p> <p>Quantile computation: Sort sample and compute using linear interpolation:</p> \\[ Q(p) \\approx x_{(\\lceil p \\cdot k \\rceil)} \\] <p>Pros: Exact for small datasets, unbiased estimator Cons: \\(O(k)\\) memory, \\(O(k \\log k)\\) sort cost</p>"},{"location":"stats/numeric/#approximate-quantiles-kll-sketch","title":"Approximate Quantiles (KLL Sketch)","text":"<p>For massive datasets, use KLL sketch (Karnin, Lang, Liberty):</p> <p>Properties: - Space: \\(O(\\frac{1}{\\epsilon} \\log \\log \\frac{1}{\\delta})\\) - Error bound: \\(\\epsilon\\)-approximate with probability \\(1-\\delta\\) - Mergeable: Combine sketches from multiple streams</p> <p>Example: With \\(\\epsilon=0.01\\), quantiles accurate to \u00b11 percentile using ~1 KB memory.</p> <p>Reference: Karnin, Z., Lang, K., Liberty, E. (2016), \"Optimal Quantile Approximation in Streams\", arXiv:1603.05346.</p>"},{"location":"stats/numeric/#t-digest-alternative","title":"T-Digest (Alternative)","text":"<p>T-digest (Dunning &amp; Ertl) provides excellent tail accuracy:</p> <p>Properties: - Better accuracy for extreme quantiles (P99, P99.9) - Adaptive compression - Mergeable</p> <p>Reference: Dunning, T., Ertl, O. (2019), \"Computing Extremely Accurate Quantiles Using t-Digests\", arXiv:1902.04023.</p>"},{"location":"stats/numeric/#histogram-construction","title":"Histogram Construction","text":""},{"location":"stats/numeric/#freedman-diaconis-rule","title":"Freedman-Diaconis Rule","text":"<p>Optimal bin width for histograms:</p> \\[ h = 2 \\cdot \\frac{\\text{IQR}}{n^{1/3}} \\] <p>Number of bins:</p> \\[ k = \\left\\lceil \\frac{\\max - \\min}{h} \\right\\rceil \\] <p>Rationale: Balances bias and variance; works well for wide variety of distributions.</p>"},{"location":"stats/numeric/#sturges-rule-alternative","title":"Sturges' Rule (Alternative)","text":"\\[ k = \\lceil \\log_2 n \\rceil + 1 \\] <p>Simpler but may undersmooth for large \\(n\\).</p>"},{"location":"stats/numeric/#scotts-rule-alternative","title":"Scott's Rule (Alternative)","text":"\\[ h = 3.5 \\cdot \\frac{s}{n^{1/3}} \\] <p>Assumes normal distribution; similar to Freedman-Diaconis.</p>"},{"location":"stats/numeric/#distinct-count-estimation","title":"Distinct Count Estimation","text":"<p>For numeric columns with many repeated values (e.g., categorical-like integers):</p>"},{"location":"stats/numeric/#kmv-k-minimum-values","title":"KMV (K-Minimum Values)","text":"<p>Maintain the \\(k\\) smallest hash values from column.</p> <p>Estimator:</p> \\[ \\hat{n}_{\\text{distinct}} = \\frac{k-1}{x_k} \\] <p>where \\(x_k\\) is the \\(k\\)-th smallest hash (normalized to [0,1]).</p> <p>Error bound:</p> \\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Example: \\(k=2048\\) \u2192 ~2.2% error (95% confidence)</p> <p>Properties: - Mergeable: Union of two KMV sketches - Space: \\(O(k)\\) per column - Update: \\(O(\\log k)\\) per value</p>"},{"location":"stats/numeric/#hyperloglog-alternative","title":"HyperLogLog (Alternative)","text":"<p>Space: \\(O(\\epsilon^{-2})\\) for relative error \\(\\epsilon\\) Error: Typical 2% with 1.5 KB Standard: Redis, BigQuery, many production systems</p>"},{"location":"stats/numeric/#missing-nan-and-inf-handling","title":"Missing, NaN, and Inf Handling","text":"<ul> <li>Missing (NULL): Excluded from moment calculations; counted separately</li> <li>NaN (Not-a-Number): Treated as missing</li> <li>\u00b1Inf: Excluded from moments; counted under <code>inf_count</code> and surfaced in warnings</li> <li>Type coercion: Strings parsing to numbers counted only if parsing enabled</li> </ul> <p>Edge Cases</p> <ul> <li>All-missing columns: statistics undefined (reported as <code>null</code>)</li> <li>\\(n &lt; 2\\): variance/shape undefined</li> <li>\\(n &lt; 4\\): kurtosis undefined</li> </ul>"},{"location":"stats/numeric/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Moments \\(O(n)\\) total, \\(O(1)\\) per value \\(O(1)\\) Welford/P\u00e9bay Reservoir sampling \\(O(n)\\) total, \\(O(1)\\) amortized \\(O(k)\\) \\(k\\) = sample size Quantiles (exact) \\(O(k \\log k)\\) \\(O(k)\\) Sort sample KLL sketch \\(O(n \\log \\log n)\\) \\(O(\\epsilon^{-1} \\log \\log n)\\) Approximate KMV distinct \\(O(n \\log k)\\) \\(O(k)\\) Heap operations Histogram \\(O(n + k)\\) \\(O(b)\\) \\(b\\) bins"},{"location":"stats/numeric/#configuration","title":"Configuration","text":"<p>Control numeric analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Sample size for quantiles/histograms\nconfig.compute.numeric_sample_size = 20_000  # Default\n\n# Sketch sizes\nconfig.compute.uniques_sketch_size = 2_048  # KMV (default)\nconfig.compute.top_k_size = 50  # Top values (if tracking)\n\n# Quantiles to compute\n# (Not yet configurable, default: [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n\n# Histogram bins\n# (Automatic via Freedman-Diaconis, max 256)\n\n# Outlier detection method\n# (Always computed: IQR, z-score, MAD)\n\n# Random seed for reproducibility\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/numeric/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/numeric/#numericaccumulator-class","title":"NumericAccumulator Class","text":"<pre><code>class NumericAccumulator:\n    def __init__(self, name: str, config: NumericConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n        self.zeros = 0\n        self.negatives = 0\n        self.inf = 0\n\n        # Streaming moments\n        self._moments = StreamingMoments()\n\n        # Reservoir sample for quantiles\n        self._sample = ReservoirSampler(config.sample_size)\n\n        # KMV for distinct count\n        self._uniques = KMV(config.uniques_sketch_size)\n\n        # Min/max tracking\n        self._extremes = ExtremeTracker()\n\n    def update(self, values: np.ndarray):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing/NaN/Inf\n        # Update moments\n        # Update sample\n        # Update KMV\n        # Track extremes\n        pass\n\n    def finalize(self) -&gt; NumericSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute mean, variance, skewness, kurtosis from moments\n        # Compute quantiles from sample\n        # Estimate distinct count from KMV\n        # Build histogram\n        # Detect outliers\n        return NumericSummary(...)\n</code></pre>"},{"location":"stats/numeric/#validation","title":"Validation","text":"<p>PySuricata validates numeric algorithms against reference implementations:</p> <ul> <li>NumPy/SciPy: Cross-check mean, variance, skewness, kurtosis on small datasets</li> <li>Property-based tests: Invariants under concatenation (merge = single pass)</li> <li>Scaling laws: \\(\\text{Var}(aX) = a^2 \\text{Var}(X)\\)</li> <li>Translation laws: \\(\\text{Mean}(X+c) = \\text{Mean}(X) + c\\)</li> <li>Numerical stability: Test with extreme values, large cancellations</li> </ul>"},{"location":"stats/numeric/#examples","title":"Examples","text":""},{"location":"stats/numeric/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\"amount\": [10, 20, 30, None, 50]})\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/numeric/#streaming-large-dataset","title":"Streaming Large Dataset","text":"<pre><code>from pysuricata import profile, ReportConfig\n\ndef read_chunks():\n    for i in range(100):\n        yield pd.read_parquet(f\"data/part-{i}.parquet\")\n\nconfig = ReportConfig()\nconfig.compute.numeric_sample_size = 50_000\nconfig.compute.random_seed = 42\n\nreport = profile(read_chunks(), config=config)\n</code></pre>"},{"location":"stats/numeric/#access-statistics-programmatically","title":"Access Statistics Programmatically","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\namount_stats = stats[\"columns\"][\"amount\"]\n\nprint(f\"Mean: {amount_stats['mean']}\")\nprint(f\"Std: {amount_stats['std']}\")\nprint(f\"Skewness: {amount_stats['skewness']}\")\n</code></pre>"},{"location":"stats/numeric/#references","title":"References","text":"<ol> <li> <p>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p> </li> <li> <p>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212. PDF</p> </li> <li> <p>Karnin, Z., Lang, K., Liberty, E. (2016), \"Optimal Quantile Approximation in Streams\", IEEE FOCS. arXiv:1603.05346</p> </li> <li> <p>Dunning, T., Ertl, O. (2019), \"Computing Extremely Accurate Quantiles Using t-Digests\", arXiv:1902.04023</p> </li> <li> <p>Tukey, J.W. (1977), Exploratory Data Analysis, Addison-Wesley.</p> </li> <li> <p>Freedman, D., Diaconis, P. (1981), \"On the histogram as a density estimator\", Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 57: 453\u2013476.</p> </li> <li> <p>Wikipedia: Algorithms for calculating variance - Link</p> </li> <li> <p>Wikipedia: Skewness - Link</p> </li> <li> <p>Wikipedia: Kurtosis - Link</p> </li> <li> <p>Wikipedia: Median absolute deviation - Link</p> </li> </ol>"},{"location":"stats/numeric/#see-also","title":"See Also","text":"<ul> <li>Categorical Analysis - String/categorical variables</li> <li>DateTime Analysis - Temporal variables</li> <li>Streaming Algorithms - Welford/P\u00e9bay deep dive</li> <li>Sketch Algorithms - KMV, HyperLogLog, KLL</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/overview/","title":"Statistical Methods Overview","text":"<p>PySuricata analyzes four variable types with specialized algorithms for each.</p>"},{"location":"stats/overview/#analysis-by-variable-type","title":"Analysis by Variable Type","text":""},{"location":"stats/overview/#numeric-variables","title":"Numeric Variables","text":"<p>Exact statistics using Welford/P\u00e9bay streaming algorithms: - Mean, variance, standard deviation - Skewness, kurtosis - Min, max, range</p> <p>Approximate statistics using probabilistic data structures: - Quantiles (reservoir sampling) - Distinct count (KMV sketch) - Histograms (adaptive binning)</p> <p>Key formulas: [ \\bar{x} = \\frac{1}{n}\\sum x_i, \\quad s^2 = \\frac{1}{n-1}\\sum (x_i - \\bar{x})^2 ]</p> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#categorical-variables","title":"Categorical Variables","text":"<p>Analysis includes: - Top-k values (Misra-Gries algorithm) - Distinct count (KMV sketch) - Entropy and Gini impurity - String statistics</p> <p>Key formulas: [ H(X) = -\\sum p(x) \\log_2 p(x), \\quad \\text{Gini}(X) = 1 - \\sum p(x)^2 ]</p> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#datetime-variables","title":"DateTime Variables","text":"<p>Temporal analysis: - Hour, day-of-week, month distributions - Monotonicity detection - Timeline visualizations</p> <p>Key formulas: [ \\Delta t = \\max(t) - \\min(t), \\quad M = \\frac{n_{\\text{increasing}}}{n-1} ]</p> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#boolean-variables","title":"Boolean Variables","text":"<p>Binary analysis: - True/false counts and ratios - Entropy calculation - Imbalance detection</p> <p>Key formulas: [ H = -p \\log_2(p) - (1-p)\\log_2(1-p) ]</p> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#advanced-analytics","title":"Advanced Analytics","text":""},{"location":"stats/overview/#correlations","title":"Correlations","text":"<p>Streaming Pearson correlation between numeric columns.</p> \\[ r_{XY} = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2}\\sqrt{\\sum(y_i - \\bar{y})^2}} \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#missing-values","title":"Missing Values","text":"<p>Intelligent missing data analysis with chunk-level tracking.</p> \\[ MR = \\frac{n_{\\text{missing}}}{n_{\\text{total}}} \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#algorithms","title":"Algorithms","text":""},{"location":"stats/overview/#streaming-statistics","title":"Streaming Statistics","text":"<ul> <li>Welford's algorithm: Online mean/variance</li> <li>P\u00e9bay's formulas: Parallel merging</li> </ul> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#sketch-algorithms","title":"Sketch Algorithms","text":"<ul> <li>KMV: Distinct count estimation</li> <li>Misra-Gries: Top-k heavy hitters</li> <li>Reservoir sampling: Uniform sampling</li> </ul> <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#guarantees","title":"Guarantees","text":"Method Type Error Mean, variance Exact Machine precision Skewness, kurtosis Exact Machine precision Distinct (KMV) Approximate ~2% (k=2048) Top-k (Misra-Gries) Guarantee All freq &gt; n/k found Quantiles (reservoir) Exact From uniform sample"},{"location":"stats/overview/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Complete numeric documentation</li> <li>Categorical Analysis - Categorical methods</li> <li>DateTime Analysis - Temporal analysis</li> <li>Boolean Analysis - Binary variables</li> <li>Streaming Algorithms - Algorithm details</li> <li>Sketch Algorithms - Probabilistic structures</li> </ul> <p>Last updated: 2025-10-12</p>"}]}