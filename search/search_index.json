{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pysuricata","title":"PySuricata","text":"<p>Exploratory data analysis for Python, built on streaming algorithms.</p> <p>PySuricata generates self-contained HTML reports for pandas and polars DataFrames. It processes data in chunks using streaming algorithms, so memory usage stays bounded regardless of dataset size.</p> <ul> <li> <p>Quick Start</p> <p>Install PySuricata and generate your first report.</p> <p> Get Started</p> </li> <li> <p>Why PySuricata?</p> <p>Understand the streaming architecture and design decisions.</p> <p> Learn More</p> </li> <li> <p>User Guide</p> <p>Detailed guides for configuration, advanced features, and more.</p> <p> Read the Guide</p> </li> <li> <p>API Reference</p> <p>Full API documentation generated from source code.</p> <p> API Docs</p> </li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Streaming processing \u2014 Data is processed in configurable chunks, keeping memory usage bounded. Useful for datasets that don't fit in RAM.</li> <li>Mathematically grounded \u2014 Uses Welford's algorithm for numerically stable moments, P\u00e9bay's formulas for mergeable statistics, KMV sketches for distinct count estimation, and Misra-Gries for heavy hitters.</li> <li>Pandas and Polars support \u2014 Works natively with both <code>pandas.DataFrame</code> and <code>polars.DataFrame</code> / <code>polars.LazyFrame</code>.</li> <li>Self-contained reports \u2014 Generates a single HTML file with inline CSS, JS, and SVG charts. No external assets or dependencies needed to view.</li> <li>Configurable \u2014 Control chunk sizes, sample sizes, sketch parameters, correlation thresholds, and rendering options via <code>ReportConfig</code>.</li> <li>Reproducible \u2014 Seeded random sampling produces deterministic results across runs.</li> </ul>"},{"location":"#installation","title":"Installation","text":"uv (Recommended)pip <pre><code>uv add pysuricata\n</code></pre> <pre><code>pip install pysuricata\n</code></pre> <p>This installs PySuricata along with its dependencies: pandas, numpy (on Python \u22653.13), markdown, and psutil.</p> <p>To also install polars support:</p> <pre><code>pip install pysuricata[polars]\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load Titanic dataset\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ndf = pd.read_csv(url)\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"titanic_report.html\")\n</code></pre> <p>This is the actual report generated from the code above (Titanic dataset, 891 rows \u00d7 12 columns):</p> <p>Can't see the report? Open in new tab \u2192</p>"},{"location":"#how-it-works","title":"How It Works","text":"<p>PySuricata reads data in chunks and updates lightweight accumulators for each column. This means:</p> Aspect Approach Memory Bounded by chunk size + accumulator state, not dataset size Speed Single pass over the data \u2014 each row is read once Accuracy Exact for moments (mean, variance, skewness, kurtosis); approximate with known error bounds for distinct counts and top-k Mergeability Accumulators can be merged across chunks or machines <p>Reports include per-column statistics, histograms, correlation chips, missing value analysis, outlier detection, and more \u2014 all computed during the single streaming pass.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>New to PySuricata?</p> <p>Start with the Quick Start Guide</p> </li> <li> <p>Want specific examples?</p> <p>Check the Examples Gallery</p> </li> <li> <p>Interested in the algorithms?</p> <p>Explore Statistical Methods</p> </li> <li> <p>Want to contribute?</p> <p>Read the Contributing Guide</p> </li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Discussions</li> <li>Issue Tracker</li> <li>Star on GitHub</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License. See LICENSE for details.</p>"},{"location":"about-suricatas/","title":"About Suricatas","text":""},{"location":"about-suricatas/#the-animal-behind-the-name","title":"The Animal Behind the Name","text":"<p>Suricatas (Suricata suricatta), commonly known as meerkats, are small mongoose species native to the Kalahari Desert in southern Africa. They are social, cooperative, and remarkably efficient \u2014 traits that informed the design of PySuricata.</p> <p>A suricata standing sentinel \u2014 always vigilant, always scanning.</p>"},{"location":"about-suricatas/#why-a-suricata","title":"Why a Suricata?","text":"<p>The name isn't just a mascot \u2014 suricata behavior maps well to how PySuricata processes data:</p> Suricata Behavior PySuricata Design Sentinel duty \u2014 one watches while others forage Streaming architecture \u2014 scan data once, extract everything Cooperative mobs \u2014 20-50 individuals coordinate Mergeable accumulators \u2014 process chunks independently, combine results Desert survival \u2014 thrive with scarce resources Bounded memory \u2014 works regardless of dataset size Quick reactions \u2014 detect threats in milliseconds Single-pass O(n) \u2014 no wasted computation Pattern recognition \u2014 remember hundreds of individuals Statistical analysis \u2014 detect outliers, correlations, distributions Persistent digging \u2014 hours excavating for food Deep profiling \u2014 every column type analyzed thoroughly"},{"location":"about-suricatas/#suricata-facts","title":"Suricata Facts","text":""},{"location":"about-suricatas/#physical-characteristics","title":"Physical Characteristics","text":"<ul> <li>Size: 25\u201335 cm tall (about 1 foot)</li> <li>Weight: 600\u2013900 grams (just over 1 pound)</li> <li>Speed: Up to 32 km/h (20 mph)</li> <li>Lifespan: 12\u201314 years in the wild</li> </ul>"},{"location":"about-suricatas/#notable-abilities","title":"Notable Abilities","text":"<ul> <li>Dark eye patches reduce sun glare, functioning like built-in sunglasses</li> <li>Immune to certain venoms \u2014 can survive scorpion and some snake bites</li> <li>Over 20 distinct vocalizations for communication</li> <li>Active teaching \u2014 adults gradually introduce young to dangerous prey like scorpions</li> <li>Elaborate burrow networks with multiple entrances and exits</li> </ul>"},{"location":"about-suricatas/#social-structure","title":"Social Structure","text":"<p>Suricatas live in highly organized groups called \"mobs\" or \"clans.\" They rotate sentinel duty roughly every hour, cooperatively raise young regardless of parentage, and collectively defend territory. Every member contributes.</p>"},{"location":"about-suricatas/#learn-more","title":"Learn More","text":"<ul> <li>Meerkat \u2014 Wikipedia</li> <li>San Diego Zoo: Meerkats</li> <li>National Geographic: Meerkats</li> </ul>"},{"location":"advanced/","title":"Advanced Features","text":"<p>Advanced techniques for power users.</p>"},{"location":"advanced/#custom-markdown-descriptions","title":"Custom Markdown Descriptions","text":"<p>Add rich descriptions to reports:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.render.description = \"\"\"\n# Q4 2024 Analysis\n\n**Dataset**: Customer transactions  \n**Period**: Oct-Dec 2024  \n**Source**: production.transactions\n\n## Key Findings\n\n- Revenue up 15% YoY\n- Average transaction: $87.50\n- Peak hour: 2pm EST\n\"\"\"\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"advanced/#streaming-from-multiple-sources","title":"Streaming from Multiple Sources","text":"<p>Combine data from multiple sources:</p> <pre><code>def multi_source_generator():\n    # Source 1: CSV files\n    for i in range(10):\n        yield pd.read_csv(f\"batch_{i}.csv\")\n\n    # Source 2: Parquet files\n    for i in range(5):\n        yield pd.read_parquet(f\"archive_{i}.parquet\")\n\n    # Source 3: Database\n    for chunk in pd.read_sql(\"SELECT * FROM logs\", conn, chunksize=100_000):\n        yield chunk\n\nreport = profile(multi_source_generator())\n</code></pre>"},{"location":"advanced/#parallel-processing-with-dask","title":"Parallel Processing with Dask","text":"<pre><code>import dask.dataframe as dd\nfrom pysuricata import profile\n\n# Load with Dask\nddf = dd.read_csv(\"large_*.csv\")\n\n# Convert to generator\ndef dask_generator():\n    for partition in ddf.partitions:\n        yield partition.compute()\n\nreport = profile(dask_generator())\n</code></pre>"},{"location":"advanced/#custom-sampling-strategy","title":"Custom Sampling Strategy","text":"<pre><code># Sample every Nth row for very large datasets\ndef sampled_generator(n=10):\n    for chunk in pd.read_csv(\"huge.csv\", chunksize=100_000):\n        yield chunk[::n]  # Every 10th row\n\nreport = profile(sampled_generator())\n</code></pre>"},{"location":"advanced/#merging-accumulator-states-distributed","title":"Merging Accumulator States (Distributed)","text":"<pre><code>from pysuricata.accumulators import NumericAccumulator\n\n# Worker 1\nacc1 = NumericAccumulator(\"amount\")\nacc1.update(data_partition_1)\n\n# Worker 2\nacc2 = NumericAccumulator(\"amount\")\nacc2.update(data_partition_2)\n\n# Merge on coordinator\nacc1.merge(acc2)\nfinal_stats = acc1.finalize()\n</code></pre>"},{"location":"advanced/#conditional-profiling","title":"Conditional Profiling","text":"<p>Profile only rows meeting criteria:</p> <pre><code>def filtered_generator():\n    for chunk in pd.read_csv(\"data.csv\", chunksize=100_000):\n        # Only active users\n        yield chunk[chunk[\"status\"] == \"active\"]\n\nreport = profile(filtered_generator())\n</code></pre>"},{"location":"advanced/#see-also","title":"See Also","text":"<ul> <li>Configuration - All parameters</li> <li>Performance Tips - Optimization</li> <li>Examples - More use cases</li> </ul>"},{"location":"api/","title":"High-Level API","text":"<p>Two entry points cover most workflows:</p> Function Returns Use case <code>profile(data, config)</code> <code>Report</code> HTML report + statistics <code>summarize(data, config)</code> <code>dict</code> Statistics only (no HTML) <pre><code>from pysuricata import profile, summarize, ReportConfig\n</code></pre>"},{"location":"api/#inputs","title":"Inputs","text":"<p><code>profile()</code> and <code>summarize()</code> accept:</p> <ul> <li><code>pandas.DataFrame</code></li> <li><code>polars.DataFrame</code> or <code>polars.LazyFrame</code> (requires <code>pysuricata[polars]</code>)</li> <li><code>Iterable[pandas.DataFrame]</code> \u2014 a generator yielding chunks</li> </ul>"},{"location":"api/#report-object","title":"Report Object","text":"<pre><code>report = profile(df)\nreport.save_html(\"report.html\")   # Self-contained HTML file\nreport.save_json(\"stats.json\")    # Statistics as JSON\n\n# Jupyter: displays inline automatically\nreport\n</code></pre>"},{"location":"api/#stats-only-path","title":"Stats-Only Path","text":"<p><code>summarize()</code> skips HTML rendering \u2014 useful for CI/CD checks:</p> <pre><code>stats = summarize(df)\n\n# Dataset-level\nprint(stats[\"dataset\"][\"rows_est\"])\nprint(stats[\"dataset\"][\"missing_cells_pct\"])\n\n# Per-column\nprint(stats[\"columns\"][\"age\"][\"mean\"])\n\n# Quality gate\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"api/#configuration","title":"Configuration","text":"<p>All options live in <code>ReportConfig</code>:</p> <pre><code>cfg = ReportConfig()\n\n# Chunking\ncfg.compute.chunk_size = 250_000        # rows per chunk (default: 200_000)\n\n# Sampling\ncfg.compute.numeric_sample_size = 50_000  # reservoir size (default: 20_000)\ncfg.compute.random_seed = 42              # deterministic sampling\n\n# Sketch parameters\ncfg.compute.uniques_sketch_size = 2_048  # KMV sketch size (default: 2_048)\ncfg.compute.top_k_size = 50             # Misra-Gries k (default: 50)\n\n# Correlations\ncfg.compute.compute_correlations = True\ncfg.compute.corr_threshold = 0.5        # minimum |r| to report\n\n# Column selection\ncfg.compute.columns = [\"col_a\", \"col_b\"]  # profile only these\n\n# Render\ncfg.render.title = \"My Report\"\n\nreport = profile(df, config=cfg)\n</code></pre>"},{"location":"api/#streaming-usage","title":"Streaming Usage","text":"<p>Pass a generator to process data larger than RAM:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndef chunks():\n    for path in sorted(Path(\"data/\").glob(\"*.parquet\")):\n        yield pd.read_parquet(path)\n\nreport = profile(chunks())\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"api/#determinism","title":"Determinism","text":"<p>Set <code>random_seed</code> to make reservoir sampling reproducible:</p> <pre><code>cfg = ReportConfig()\ncfg.compute.random_seed = 42\n# Same data + same seed = identical report\n</code></pre>"},{"location":"api/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide \u2014 full parameter reference</li> <li>Basic Usage \u2014 more examples</li> <li>Performance Tips \u2014 tuning for large datasets</li> </ul>"},{"location":"architecture-diagrams/","title":"Architecture Diagrams","text":"<p>Visual reference for PySuricata's processing pipeline, accumulator internals, chunk processing, and rendering \u2014 annotated with algorithmic complexity.</p>"},{"location":"architecture-diagrams/#1-main-processing-pipeline","title":"1. Main Processing Pipeline","text":"<p>End-to-end data flow from user input to final <code>Report</code> object.</p> <pre><code>flowchart TD\n    A[\"User Input\\npd.DataFrame | pl.DataFrame\\npl.LazyFrame | Iterable\"] --&gt;|\"O(1)\"| B[\"api.py \u2014 profile / summarize\\n_coerce_input \u00b7 _to_engine_config\"]\n    B --&gt;|\"O(1)\"| C[\"report.py \u2014 ReportOrchestrator.build_report\"]\n    C --&gt;|\"O(1)\"| D[\"engine.py \u2014 StreamingEngine.process_stream\"]\n    D --&gt;|\"O(1)\"| E[\"EngineManager.select_adapter\\nPandasAdapter | PolarsAdapter\"]\n    E --&gt;|\"O(1)\"| F[\"AdaptiveChunker.chunks_from_source\\nStrategy: ADAPTIVE | FIXED | MEMORY_AWARE\"]\n    F --&gt;|\"O(n/c) chunks\"| G[\"First Chunk \u2014 infer_and_build\\nO(cols): type inference + accumulator creation\"]\n    G --&gt; K{\"Stream Loop\\nO(n/c) iterations\"}\n    K --&gt;|\"O(cols \u00d7 chunk)\"| L[\"consume_chunk\\nPer-column accumulator updates\"]\n    K --&gt;|\"O(m\u00b2 \u00d7 chunk)\"| M[\"update_corr\\nPairwise sums for Pearson r\\nm = numeric cols\"]\n    K --&gt;|\"O(cols \u00d7 chunk)\"| N[\"update_row_kmv\\nRow hash for duplicate detection\"]\n    L --&gt; K\n    M --&gt; K\n    N --&gt; K\n    K --&gt;|\"Done\"| O[\"_build_manifest_inputs\\nO(cols): kinds_map \u00b7 col_order \u00b7 miss_list\"]\n    O --&gt; P[\"_apply_correlation_chips\\nO(cols): attach top correlations\"]\n    P --&gt; Q[\"render_html_snapshot\\nO(cols \u00d7 card_render)\"]\n    Q --&gt; R[\"_build_summary\\nO(cols): acc.finalize per column\"]\n    R --&gt; S[\"Report \u2014 html + stats\"]</code></pre>"},{"location":"architecture-diagrams/#complexity-summary","title":"Complexity Summary","text":"Metric Value Total Time O(n_rows \u00d7 n_cols) + O(m\u00b2 \u00d7 n_rows) for correlations Total Space O(cols \u00d7 (s + k + b)) Peak Memory ~50 MB bounded by streaming architecture <p>Where: <code>n</code> = total rows, <code>c</code> = chunk size (200k default), <code>m</code> = numeric columns, <code>s</code> = reservoir size (20k), <code>k</code> = KMV sketch size (2048), <code>b</code> = histogram bins (25).</p>"},{"location":"architecture-diagrams/#2-accumulator-architecture","title":"2. Accumulator Architecture","text":"<p>Each column type has a specialized accumulator with small, bounded state.</p> <pre><code>flowchart TD\n    subgraph Numeric[\"NumericAccumulator \u2014 Space: O(s + k + b) per column\"]\n        SM[\"StreamingMoments \u2014 Welford/Chan\\nTime: O(n) per chunk | Space: O(1)\\nmean \u00b7 variance \u00b7 skewness \u00b7 kurtosis\"]\n        RS[\"ReservoirSampler \u2014 k = 20 000\\nTime: O(1) add | Space: O(s)\\nquantiles \u00b7 histogram source\"]\n        KMV_N[\"KMV Sketch \u2014 k = 2 048\\nTime: O(log k) add | Space: O(k)\\napprox distinct count\"]\n        ET[\"ExtremeTracker \u2014 k = 5 bounded heaps\\nTime: O(n log k) update | Space: O(k)\\nmin/max with row indices\"]\n        MG_N[\"MisraGries \u2014 k = 50\\nTime: O(1) add | Space: O(k)\\ntop-k values (integer-like cols)\"]\n        SH[\"StreamingHistogram \u2014 bins = 25\\nTime: O(1) add | Space: O(b)\\ntrue distribution approximation\"]\n        OD[\"OutlierDetector\\nTime: O(s) finalize | Space: O(1)\\nIQR + MAD methods\"]\n    end\n\n    subgraph Cat[\"CategoricalAccumulator \u2014 Space: O(k) per column\"]\n        KMV_C[\"KMV Sketch\\nTime: O(log k) add | Space: O(k)\\napprox distinct count\"]\n        MG_C[\"MisraGries\\nTime: O(1) add | Space: O(k)\\ntop-k categories\"]\n        SL[\"String Length Tracker\\nTime: O(1) per value | Space: O(1)\\navg_len \u00b7 p90 \u00b7 empty count\"]\n    end\n\n    subgraph DT[\"DatetimeAccumulator \u2014 Space: O(1) per column\"]\n        DMM[\"Min/Max \u2014 nanosecond timestamps\\nTime: O(1) | Space: O(1)\"]\n        DFC[\"Frequency Counters\\nday-of-week \u00b7 hour \u00b7 month\\nTime: O(1) | Space: O(1)\"]\n    end\n\n    subgraph Bool[\"BooleanAccumulator \u2014 Space: O(1) per column\"]\n        BCT[\"True / False / Missing counters\\nTime: O(1) per value | Space: O(1)\"]\n    end</code></pre>"},{"location":"architecture-diagrams/#per-column-memory-budget","title":"Per-Column Memory Budget","text":"Accumulator Default Config Approx Memory NumericAccumulator s=20k, k=2048, b=25 ~170 KB CategoricalAccumulator k=2048+50 ~20 KB DatetimeAccumulator \u2014 &lt; 1 KB BooleanAccumulator \u2014 &lt; 1 KB"},{"location":"architecture-diagrams/#3-chunk-processing-detail","title":"3. Chunk Processing Detail","text":"<p>What happens inside <code>consume_chunk()</code> for each column in a chunk.</p> <pre><code>flowchart TD\n    A[\"Incoming Chunk\\npd.DataFrame or pl.DataFrame\"] --&gt; B{\"New columns\\nin this chunk?\"}\n    B --&gt;|\"Yes\"| C[\"UnifiedTypeInferrer.infer_series_type\\n+ create accumulator via factory\\nO(sample_size) per new column\"]\n    B --&gt;|\"No\"| D[\"Iterate accs.items \u2014 O(cols)\"]\n    C --&gt; D\n\n    D --&gt; E{\"Column type?\"}\n\n    E --&gt;|\"Numeric\"| F[\"to_numpy(float64)\\nFast path: direct cast\\nSlow path: pd.to_numeric(errors=coerce)\\nO(chunk_size)\"]\n    F --&gt; F1[\"acc.update(arr)\\nmoments + reservoir + histogram\\nO(chunk_size)\"]\n    F1 --&gt; F2[\"KMV.add per value\\nO(chunk_size \u00d7 log k)\"]\n    F2 --&gt; F3[\"ExtremeTracker.update\\nO(chunk_size \u00d7 log k)\\nEvery 5th chunk only (pandas)\"]\n    F3 --&gt; G[\"Next column\"]\n\n    E --&gt;|\"Categorical\"| H[\"s.tolist \u2192 Python list\\nO(chunk_size)\"]\n    H --&gt; H1[\"KMV + MisraGries + string stats\\nO(chunk_size)\"]\n    H1 --&gt; G\n\n    E --&gt;|\"Boolean\"| I[\"Per-value str coercion\\nstr(v).strip().lower()\\nO(chunk_size) \u2014 Python loop\"]\n    I --&gt; I1[\"True/False/Missing counting\\nO(chunk_size)\"]\n    I1 --&gt; G\n\n    E --&gt;|\"Datetime\"| J[\"pd.to_datetime(errors=coerce, utc=True)\\nO(chunk_size)\"]\n    J --&gt; J1[\"Min/max + frequency counters\\nO(chunk_size)\"]\n    J1 --&gt; G\n\n    G --&gt; K{\"More columns?\"}\n    K --&gt;|\"Yes\"| D\n    K --&gt;|\"No\"| L[\"Return to engine loop\\n+ update memory estimate\"]</code></pre>"},{"location":"architecture-diagrams/#per-chunk-bottleneck-analysis","title":"Per-Chunk Bottleneck Analysis","text":"Operation Complexity Notes Numeric KMV loop O(chunk \u00d7 log k) Per-value Python \u2192 batch vectorizable Boolean coercion O(chunk) Python str conversion per value \u2192 vectorizable Correlation update O(m\u00b2 \u00d7 chunk) Pairwise; gated by <code>corr_max_cols=50</code> Row KMV hashing O(chunk \u00d7 cols) Per-row tuple hashing \u2192 vectorizable Type conversion O(chunk) pd.to_numeric / pd.to_datetime"},{"location":"architecture-diagrams/#4-rendering-pipeline","title":"4. Rendering Pipeline","text":"<p>How the final HTML report is assembled from accumulated statistics.</p> <pre><code>flowchart TD\n    A[\"render_html_snapshot\\nrender/html.py\"] --&gt; B[\"Build kinds_map\\nO(cols): name \u2192 (kind, accumulator)\"]\n    B --&gt; C[\"Compute miss_list\\nO(cols): per-column missing percentage\"]\n    C --&gt; D[\"Dataset metrics\\nrow_kmv.approx_duplicates\\nconstant / high-cardinality detection\"]\n    D --&gt; E[\"Card Loop \u2014 O(cols)\"]\n    E --&gt; F[\"acc.finalize(chunk_metadata)\\nquantiles O(s log s)\\nextremes O(k log k)\\noutlier stats O(s)\"]\n    F --&gt; G{\"Card type?\"}\n\n    G --&gt;|\"Numeric\"| G1[\"NumericCardRenderer\\nSVG histogram + stats tables\\n~200 lines HTML per card\"]\n    G --&gt;|\"Categorical\"| G2[\"CategoricalCardRenderer\\nDonut chart SVG + top-k table\"]\n    G --&gt;|\"Datetime\"| G3[\"DatetimeCardRenderer\\nTemporal bar charts + freq tables\"]\n    G --&gt;|\"Boolean\"| G4[\"BooleanCardRenderer\\nTrue/False bar + percentage\"]\n\n    G1 --&gt; H[\"Collect card HTML strings\"]\n    G2 --&gt; H\n    G3 --&gt; H\n    G4 --&gt; H\n\n    H --&gt; I[\"Load + inline static assets\\nstyle.css \u00b7 functionality.js\\nchart.min.js (inlined)\"]\n    I --&gt; J[\"CorrelationsSectionRenderer\\nrender_section \u2014 O(m\u00b2)\"]\n    J --&gt; K_node[\"MissingValuesSectionRenderer\\nrender_section \u2014 O(cols)\"]\n    K_node --&gt; L[\"DonutChartRenderer\\nrender_dtype_donut \u2014 SVG\"]\n    L --&gt; M[\"Template assembly\\nreport_template.html.format\\n~40 template variables\"]\n    M --&gt; N[\"Self-contained HTML\\n~1.2\u20131.6 MB typical\"]</code></pre>"},{"location":"architecture-diagrams/#rendering-cost-breakdown","title":"Rendering Cost Breakdown","text":"Phase Complexity Output Size Finalization O(cols \u00d7 s log s) \u2014 Card rendering O(cols) ~5\u201320 KB per card Asset inlining O(1) ~200 KB (CSS+JS) Correlation section O(m\u00b2) Variable Template assembly O(1) ~1.2\u20131.6 MB total"},{"location":"architecture-diagrams/#5-end-to-end-complexity-table","title":"5. End-to-End Complexity Table","text":"Stage Time Space Key Parameter Input coercion O(1) O(1) \u2014 Chunking O(n/c) O(c \u00d7 cols) <code>chunk_size</code> (200k) Type inference O(sample) O(1) first chunk only Accumulator updates O(n \u00d7 cols) O(cols \u00d7 s) <code>numeric_sample_k</code> (20k) KMV sketching O(n \u00d7 log k) O(cols \u00d7 k) <code>uniques_k</code> (2048) Correlation O(n \u00d7 m\u00b2) O(m\u00b2) <code>corr_max_cols</code> (50) Row deduplication O(n \u00d7 cols) O(k) KMV sketch Finalization O(cols \u00d7 s log s) O(cols) \u2014 HTML rendering O(cols) O(report_size) ~1.5 MB Total O(n \u00d7 cols + n \u00d7 m\u00b2) O(cols \u00d7 s) \u2014"},{"location":"architecture/","title":"Architecture &amp; Internals","text":"<p>How <code>pysuricata</code> profiles data efficiently and renders a self-contained HTML report.</p>"},{"location":"architecture/#high-level-pipeline","title":"High-Level Pipeline","text":"<pre><code>flowchart LR\n    A[\"Data Source\"] --&gt; B[\"Chunk Iterator\"]\n    B --&gt; C[\"Typed Accumulators\"]\n    C --&gt; D[\"Summary Metrics\"]\n    D --&gt; E[\"HTML Renderer\"]\n\n    style A fill:#E8F5E9,stroke:#2E7D32,color:#1B5E20\n    style B fill:#C8E6C9,stroke:#2E7D32,color:#1B5E20\n    style C fill:#A5D6A7,stroke:#2E7D32,color:#1B5E20\n    style D fill:#81C784,stroke:#2E7D32,color:#1B5E20\n    style E fill:#66BB6A,stroke:#2E7D32,color:#fff</code></pre> <p>Data Sources \u2192 pandas DataFrames, polars DataFrames, or any iterable of DataFrames (for streaming).</p> <p>Chunk Iterator \u2192 If a single DataFrame is passed, it is treated as one chunk. Generators are consumed chunk-by-chunk to bound memory.</p> <p>Typed Accumulators \u2192 Each column is assigned a specialized accumulator based on its inferred type. All accumulators are streaming: they accept one chunk at a time and maintain bounded state.</p> <p>Summary Metrics \u2192 After all chunks are consumed, accumulators are finalized and dataset-wide metrics (missingness, duplicates, etc.) are computed.</p> <p>HTML Renderer \u2192 A single-file Jinja2 template with inline CSS/JS produces a portable, self-contained HTML report.</p>"},{"location":"architecture/#accumulator-architecture","title":"Accumulator Architecture","text":"<pre><code>classDiagram\n    class BaseAccumulator {\n        +name: str\n        +count: int\n        +missing: int\n        +update(chunk)\n        +finalize() Summary\n    }\n\n    class NumericAccumulator {\n        +StreamingMoments\n        +ReservoirSampler\n        +KMV sketch\n        +MisraGries top-k\n        +ExtremeTracker\n    }\n\n    class CategoricalAccumulator {\n        +KMV sketch \u00d7 3\n        +MisraGries top-k\n        +String length stats\n    }\n\n    class DatetimeAccumulator {\n        +min/max timestamps\n        +hour/weekday/month counts\n        +monotonicity tracker\n    }\n\n    class BooleanAccumulator {\n        +true_count\n        +false_count\n    }\n\n    BaseAccumulator &lt;|-- NumericAccumulator\n    BaseAccumulator &lt;|-- CategoricalAccumulator\n    BaseAccumulator &lt;|-- DatetimeAccumulator\n    BaseAccumulator &lt;|-- BooleanAccumulator</code></pre> <p>Each accumulator follows the same interface:</p> <ol> <li><code>update(chunk)</code> \u2014 process a batch of values, update internal state</li> <li><code>finalize()</code> \u2014 compute final statistics from accumulated state</li> </ol>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant profile as profile()\n    participant Infer as Type Inference\n    participant Acc as Accumulators\n    participant Corr as Correlations\n    participant Render as HTML Renderer\n\n    User-&gt;&gt;profile: DataFrame / generator\n    profile-&gt;&gt;Infer: First chunk\n    Infer--&gt;&gt;profile: Column types\n    profile-&gt;&gt;Acc: Create typed accumulators\n\n    loop Each chunk\n        profile-&gt;&gt;Acc: update(chunk)\n        profile-&gt;&gt;Corr: update pairs (optional)\n    end\n\n    profile-&gt;&gt;Acc: finalize()\n    Acc--&gt;&gt;profile: Per-column summaries\n    profile-&gt;&gt;Corr: finalize()\n    Corr--&gt;&gt;profile: Correlation matrix\n\n    profile-&gt;&gt;Render: Summaries + config\n    Render--&gt;&gt;User: Report (HTML)</code></pre>"},{"location":"architecture/#streaming-algorithms","title":"Streaming Algorithms","text":"<p>Each accumulator uses algorithms chosen for O(1) per-value update and bounded memory:</p> <pre><code>flowchart TB\n    subgraph Numeric[\"Numeric Accumulator\"]\n        N1[\"Welford/P\u00e9bay&lt;br/&gt;mean, var, skew, kurt&lt;br/&gt;O(1) space\"]\n        N2[\"Reservoir Sampling&lt;br/&gt;quantiles, histograms&lt;br/&gt;O(s) space\"]\n        N3[\"KMV Sketch&lt;br/&gt;distinct count&lt;br/&gt;O(k) space\"]\n        N4[\"Misra-Gries&lt;br/&gt;top-k values&lt;br/&gt;O(k) space\"]\n        N5[\"Extreme Tracker&lt;br/&gt;min/max with indices&lt;br/&gt;O(k) space\"]\n    end\n\n    subgraph Categorical[\"Categorical Accumulator\"]\n        C1[\"KMV \u00d7 3&lt;br/&gt;distinct: original, lower, trimmed\"]\n        C2[\"Misra-Gries&lt;br/&gt;top-k values\"]\n        C3[\"String Length&lt;br/&gt;avg, p90\"]\n    end\n\n    subgraph DateTime[\"DateTime Accumulator\"]\n        D1[\"Min/Max&lt;br/&gt;timestamps\"]\n        D2[\"Counters&lt;br/&gt;hour/weekday/month\"]\n        D3[\"Monotonicity&lt;br/&gt;pair comparison\"]\n    end\n\n    subgraph Boolean[\"Boolean Accumulator\"]\n        B1[\"Counters&lt;br/&gt;true/false/missing\"]\n    end\n\n    style Numeric fill:#E8F5E9,stroke:#2E7D32\n    style Categorical fill:#FFF3E0,stroke:#E65100\n    style DateTime fill:#E3F2FD,stroke:#1565C0\n    style Boolean fill:#F3E5F5,stroke:#6A1B9A</code></pre>"},{"location":"architecture/#rendering-pipeline","title":"Rendering Pipeline","text":"<pre><code>flowchart TB\n    A[\"Finalized Summaries\"] --&gt; B[\"Dataset-Level Metrics\"]\n    B --&gt; C[\"Jinja2 Template\"]\n    C --&gt; D[\"Inline CSS + JS\"]\n    C --&gt; E[\"Summary Cards\"]\n    C --&gt; F[\"Variable Cards\"]\n    C --&gt; G[\"Sample Table\"]\n    D --&gt; H[\"Single HTML File\"]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H\n\n    style H fill:#66BB6A,stroke:#2E7D32,color:#fff</code></pre> <p>The template produces a single portable HTML file \u2014 no external dependencies, no server required.</p> <p>Summary cards show: rows, columns, processed bytes, missing %, duplicates %.</p> <p>Variable cards are rendered per-type with SVG charts, statistics, and quality flags.</p>"},{"location":"architecture/#shared-utilities","title":"Shared Utilities","text":"Module Functions Purpose <code>render/svg_utils.py</code> <code>safe_col_id</code>, <code>nice_ticks</code>, <code>fmt_tick</code>, <code>svg_empty</code> SVG chart helpers <code>render/format_utils.py</code> <code>human_bytes</code>, <code>fmt_num</code>, <code>fmt_compact</code> Number formatting"},{"location":"architecture/#configuration","title":"Configuration","text":"<p><code>ReportConfig</code> controls all behavior:</p> Parameter Default Effect <code>chunk_size</code> 200,000 Rows per chunk <code>numeric_sample_size</code> 20,000 Reservoir size for quantiles <code>uniques_sketch_size</code> 2,048 KMV sketch size <code>top_k_size</code> 50 Misra-Gries capacity <code>compute_correlations</code> <code>True</code> Enable/disable correlation chips <code>corr_threshold</code> 0.5 Minimum |r| to display <code>random_seed</code> <code>None</code> Deterministic sampling <code>include_sample</code> <code>True</code> Include data sample in report"},{"location":"architecture/#security-correctness","title":"Security &amp; Correctness","text":"<ul> <li>HTML escaping \u2014 column names and labels are escaped before rendering</li> <li>Missing/Inf handling \u2014 NaN and \u00b1Inf excluded from moments, reported separately</li> <li>Approximation badges \u2014 estimates marked with <code>(\u2248)</code> or <code>approx</code> badge</li> <li>Reproducibility \u2014 set <code>random_seed</code> for deterministic results</li> </ul>"},{"location":"architecture/#extending","title":"Extending","text":"<ul> <li>Backends \u2014 polars/Arrow/DuckDB can be connected via the chunk iterator interface</li> <li>Quantile sketches \u2014 t-digest or KLL can replace the default reservoir</li> <li>New sections \u2014 drift comparisons, JSON export, CLI wrapper</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to PySuricata are documented here.</p>"},{"location":"changelog/#0016-2026-02-15","title":"[0.0.16] - 2026-02-15","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Polars nested type support \u2014 Structs and Lists are now gracefully handled as categorical variables (with debug warnings) instead of causing inference errors</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Performance optimization \u2014 optimized <code>_safe_compute</code> to use NumPy arrays for type checks, reducing overhead in large datasets</li> </ul>"},{"location":"changelog/#0015-2026-02-14","title":"[0.0.15] - 2026-02-14","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Python 3.14 CI testing \u2014 Added Python 3.14 to CI test matrix</li> <li>Changelog CI check \u2014 PRs now require a changelog entry</li> <li>Mermaid architecture diagrams \u2014 Replaced ASCII art with 5 interactive diagrams</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>MathJax formula rendering \u2014 Fixed <code>ignoreHtmlClass</code> regex that prevented all formula rendering</li> <li>Code/equation styling \u2014 Changed code and math colors from green to standard gray</li> <li>Memory stress test \u2014 Bumped threshold from 200\u2192250 MB for Python 3.14 compatibility</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Dropped Python 3.9 \u2014 Minimum version is now Python 3.10</li> <li>CI runs on PR only \u2014 Tests no longer run on push to main (CD handles releases)</li> <li>Cleaned dev dependencies \u2014 Removed <code>ydata-profiling</code> and <code>ipykernel</code> (not 3.14-compatible)</li> <li>Cleaned examples/ \u2014 Removed benchmark scripts, generated reports, and ydata comparisons</li> <li>Removed <code>.claude/skills</code> \u2014 Cleaned up unused skill symlinks</li> <li>Documentation improvements \u2014 Rewrote API reference, complexity analysis, quality flags (tables), stats overview</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li><code>report_preview.png</code> \u2014 Replaced with link to live interactive report on GitHub Pages</li> <li>Stale dates from stats documentation pages</li> </ul>"},{"location":"changelog/#0014-2026-01-14","title":"[0.0.14] - 2026-01-14","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Polars LazyFrame support \u2014 LazyFrames are now automatically collected before profiling</li> <li>ReportConfig alias \u2014 Added <code>ReportConfig</code> as an alias for <code>ProfileConfig</code> for better API discoverability</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Self-contained HTML reports \u2014 HTML reports no longer depend on external CDN (Chart.js is now inlined)</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Lighter dependencies \u2014 Removed unused dependencies: <code>matplotlib</code>, <code>seaborn</code>, <code>ipywidgets</code></li> </ul>"},{"location":"changelog/#0013-2026-01-02","title":"[0.0.13] - 2026-01-02","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>CLI tool \u2014 New command-line interface with <code>pysuricata profile</code> and <code>pysuricata summarize</code> commands</li> <li>Comprehensive stress tests \u2014 New <code>test_complexity_analysis.py</code> with time/space profiling</li> <li>Python 3.14 support \u2014 Officially supported in package metadata</li> </ul>"},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Memory leak fixes \u2014 Resolved memory leaks in KMV sketch, ExtremeTracker, and chunk metadata</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Realistic benchmarks \u2014 Updated README and docs with measured performance figures</li> </ul>"},{"location":"changelog/#0011-2025-12-xx","title":"[0.0.11] - 2025-12-XX","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Enhanced documentation with mathematical formulas</li> <li>Comprehensive examples gallery</li> <li>Detailed algorithm documentation (Welford, P\u00e9bay, KMV, Misra-Gries)</li> </ul>"},{"location":"changelog/#earlier-versions","title":"Earlier Versions","text":"<p>See GitHub Releases for complete history.</p>"},{"location":"complexity-analysis/","title":"Complexity Analysis","text":""},{"location":"complexity-analysis/#overview","title":"Overview","text":"<p>PySuricata processes data in a single pass using streaming algorithms. All statistics are computed incrementally from small, fixed-size state \u2014 memory usage depends on configuration parameters, not dataset size.</p> <p>Key property: For a dataset with n rows and p columns, PySuricata uses O(p \u00b7 k) memory where k is the sketch/sample size (constant), making total memory independent of n.</p>"},{"location":"complexity-analysis/#summary","title":"Summary","text":"Component Time per value Space Dominant parameter Streaming moments O(1) O(1) \u2014 Reservoir sampling O(1) O(s) <code>numeric_sample_size</code> (default: 20,000) KMV distinct count O(log k) O(k) <code>uniques_sketch_size</code> (default: 2,048) Misra-Gries top-k O(1) amortized O(k) <code>top_k_size</code> (default: 50) Extreme tracking O(log k) O(k) <code>max_extremes</code> (default: 5) Correlations O(p) per value O(p\u00b2) Number of numeric columns <p>Total per numeric column: O(s + k) \u2248 22 KB with defaults</p> <p>Total per categorical column: O(k) \u2248 18 KB with defaults</p> <p>DateTime and Boolean columns: O(1)</p>"},{"location":"complexity-analysis/#algorithms","title":"Algorithms","text":""},{"location":"complexity-analysis/#streaming-moments-welfordpebay","title":"Streaming Moments (Welford/P\u00e9bay)","text":"<p>Purpose: Exact mean, variance, skewness, kurtosis in a single pass.</p> <ul> <li>Update: O(1) per value \u2014 maintains running sums of powers of deviations</li> <li>Merge: O(1) \u2014 P\u00e9bay's formulas combine two partial states exactly</li> <li>Finalize: O(1)</li> <li>Space: O(1) \u2014 six floating-point accumulators (n, \u03bc, M\u2082, M\u2083, M\u2084)</li> </ul> <p>This is numerically stable \u2014 it avoids the catastrophic cancellation of na\u00efve \u03a3x\u00b2 \u2212 n\u03bc\u00b2 formulas.</p> <p>Reference: Welford (1962), P\u00e9bay (2008)</p>"},{"location":"complexity-analysis/#reservoir-sampling","title":"Reservoir Sampling","text":"<p>Purpose: Maintain a uniform random sample for quantile estimation and histograms.</p> <ul> <li>Update: O(1) per value \u2014 include with probability s/i, replacing a random element</li> <li>Merge: O(s) \u2014 weighted combination of two reservoirs</li> <li>Finalize: O(s log s) \u2014 sort for quantile computation</li> <li>Space: O(s) where s = <code>numeric_sample_size</code></li> </ul> <p>Every subset of size s from the stream has equal probability of being the sample.</p> <p>Reference: Vitter (1985)</p>"},{"location":"complexity-analysis/#kmv-k-minimum-values-sketch","title":"KMV (K-Minimum Values) Sketch","text":"<p>Purpose: Approximate distinct count estimation.</p> <ul> <li>Update: O(log k) \u2014 hash value, maintain min-heap of k smallest hashes</li> <li>Merge: O(k log k) \u2014 union two heaps, keep k smallest</li> <li>Estimate: d\u0302 = (k \u2212 1) / x_k where x_k is the k-th smallest hash</li> <li>Space: O(k) where k = <code>uniques_sketch_size</code></li> <li>Error: ~1/\u221ak relative error \u2014 with k=2048, approximately 2.2%</li> </ul> <p>The implementation transitions from exact counting (for low-cardinality columns) to approximate estimation when the number of distinct values exceeds a threshold.</p> <p>Reference: Bar-Yossef et al. (2002)</p>"},{"location":"complexity-analysis/#misra-gries-top-k-frequent-values","title":"Misra-Gries (Top-K Frequent Values)","text":"<p>Purpose: Find the most frequent values in a stream.</p> <ul> <li>Update: O(1) amortized \u2014 increment counter or decrement all</li> <li>Merge: O(k) \u2014 sum counters from two states</li> <li>Finalize: O(k log k) \u2014 sort by frequency</li> <li>Space: O(k) where k = <code>top_k_size</code></li> <li>Guarantee: Any value with true frequency &gt; n/k is included in the output</li> </ul> <p>Frequency estimates are within \u00b1n/k of the true count.</p> <p>Reference: Misra &amp; Gries (1982)</p>"},{"location":"complexity-analysis/#extreme-tracking","title":"Extreme Tracking","text":"<p>Purpose: Track the minimum and maximum values with their row indices.</p> <ul> <li>Update: O(log k) per value \u2014 bounded heap operations</li> <li>Merge: O(k log k) \u2014 merge and truncate heaps</li> <li>Space: O(k) where k = <code>max_extremes</code> (default: 5)</li> </ul> <p>Uses a min-heap for minimums and a negated max-heap for maximums.</p>"},{"location":"complexity-analysis/#correlations","title":"Correlations","text":"<p>Purpose: Streaming Pearson correlation between all pairs of numeric columns.</p> <ul> <li>Update: O(p) per value \u2014 update co-moment for each column pair</li> <li>Finalize: O(p\u00b2) \u2014 compute correlations from co-moments</li> <li>Space: O(p\u00b2) \u2014 stores pairwise co-moments</li> </ul> <p>This is the most expensive component. For datasets with many numeric columns, disable with <code>compute_correlations = False</code> or increase <code>corr_threshold</code>.</p>"},{"location":"complexity-analysis/#accumulator-breakdown","title":"Accumulator Breakdown","text":""},{"location":"complexity-analysis/#numericaccumulator","title":"NumericAccumulator","text":"Component Space StreamingMoments O(1) ReservoirSampler O(s) KMV sketch O(k) ExtremeTracker O(k) MisraGries O(k) Histogram bins O(b) Total O(s + k + b) <p>With defaults: s=20,000, k=2,048, b=25</p>"},{"location":"complexity-analysis/#categoricalaccumulator","title":"CategoricalAccumulator","text":"Component Space KMV sketch (original) O(k) KMV sketch (lowercase) O(k) KMV sketch (trimmed) O(k) MisraGries O(k) String length tracking O(1) Total O(k)"},{"location":"complexity-analysis/#datetimeaccumulator","title":"DatetimeAccumulator","text":"<p>Fixed-size counters: hour (24), weekday (7), month (12), plus min/max tracking.</p> <p>Total: O(1)</p>"},{"location":"complexity-analysis/#booleanaccumulator","title":"BooleanAccumulator","text":"<p>Two counters (true, false) plus missing count.</p> <p>Total: O(1)</p>"},{"location":"complexity-analysis/#configuration-impact","title":"Configuration Impact","text":"Parameter Effect on memory Effect on accuracy <code>chunk_size</code> \u2191 More rows in memory per iteration No effect on final accuracy <code>numeric_sample_size</code> \u2191 Larger reservoir per numeric column Better quantile estimates <code>uniques_sketch_size</code> \u2191 Larger KMV sketch per column Better distinct count accuracy <code>top_k_size</code> \u2191 More frequent values tracked More comprehensive top-k <code>compute_correlations</code> off Saves O(p\u00b2) No correlations reported"},{"location":"complexity-analysis/#performance-optimizations-v0014","title":"Performance Optimizations (v0.0.14)","text":"<p>Several hot paths were vectorized to reduce per-value Python overhead:</p> Optimization Change KMV/MisraGries updates Per-value <code>add()</code> \u2192 batch <code>add_many()</code> with pre-counted values Missing/Inf counting Python loop \u2192 vectorized <code>np.isnan()</code> / <code>np.isinf()</code> Row hashing (duplicates) Per-row tuple + <code>hash()</code> \u2192 vectorized polynomial hash via numpy Boolean coercion Per-value <code>str().lower()</code> \u2192 pandas vectorized <code>str.strip().str.lower()</code> Correlation finite masks Recomputed per pair \u2192 pre-computed once per column, reused"},{"location":"complexity-analysis/#references","title":"References","text":"<ol> <li>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420</li> <li>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212</li> <li>Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM</li> <li>Misra, J. &amp; Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152</li> <li>Vitter, J.S. (1985), \"Random Sampling with a Reservoir\", ACM TOMS, 11(1): 37\u201357</li> </ol>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Complete guide to configuring PySuricata for your specific needs.</p>"},{"location":"configuration/#overview","title":"Overview","text":"<p>PySuricata is highly configurable via the <code>ReportConfig</code> class hierarchy:</p> <pre><code>ReportConfig\n\u251c\u2500\u2500 compute: ComputeOptions  # Analysis parameters\n\u2514\u2500\u2500 render: RenderOptions    # Display parameters\n</code></pre>"},{"location":"configuration/#quick-start","title":"Quick Start","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Create config\nconfig = ReportConfig()\n\n# Customize settings\nconfig.compute.chunk_size = 250_000\nconfig.compute.random_seed = 42\nconfig.compute.compute_correlations = True\n\n# Generate report\nreport = profile(df, config=config)\n</code></pre>"},{"location":"configuration/#computeoptions","title":"ComputeOptions","text":"<p>Control data processing and analysis.</p>"},{"location":"configuration/#basic-parameters","title":"Basic Parameters","text":"<p><code>chunk_size: int = 200_000</code></p> <p>Rows per chunk when processing data.</p> <ul> <li>Larger: Faster processing, more memory</li> <li>Smaller: Less memory, more overhead</li> <li>Recommended: 100K-500K for most datasets</li> </ul> <pre><code>config.compute.chunk_size = 250_000\n</code></pre> <p><code>columns: Optional[List[str]] = None</code></p> <p>Analyze only specific columns. If <code>None</code>, analyze all.</p> <pre><code>config.compute.columns = [\"age\", \"income\", \"city\"]\n</code></pre> <p><code>random_seed: Optional[int] = None</code></p> <p>Random seed for deterministic sampling. Set for reproducibility.</p> <pre><code>config.compute.random_seed = 42  # Same report every run\n</code></pre>"},{"location":"configuration/#numeric-configuration","title":"Numeric Configuration","text":"<p><code>numeric_sample_size: int = 20_000</code></p> <p>Reservoir sample size for quantiles and histograms.</p> <ul> <li>Larger: More accurate quantiles, more memory</li> <li>Smaller: Less memory, slightly less accurate</li> <li>Recommended: 10K-50K</li> </ul> <pre><code>config.compute.numeric_sample_size = 50_000\n</code></pre> <p><code>uniques_sketch_size: int = 2_048</code></p> <p>KMV sketch size for distinct count estimation.</p> <ul> <li>Error: \\(\\approx 1/\\sqrt{k}\\)</li> <li>k=1024: ~3.1% error</li> <li>k=2048: ~2.2% error (default)</li> <li>k=4096: ~1.6% error</li> </ul> <pre><code>config.compute.uniques_sketch_size = 4_096  # More accurate\n</code></pre>"},{"location":"configuration/#categorical-configuration","title":"Categorical Configuration","text":"<p><code>top_k_size: int = 50</code></p> <p>Number of top values to track (Misra-Gries algorithm).</p> <ul> <li>Larger: More top values, more memory</li> <li>Smaller: Fewer top values, less memory</li> <li>Guarantee: All items with frequency &gt; n/k found</li> </ul> <pre><code>config.compute.top_k_size = 100  # Track top 100\n</code></pre>"},{"location":"configuration/#correlation-configuration","title":"Correlation Configuration","text":"<p>Correlation settings are available through the public <code>ComputeOptions</code> API.</p> <p><code>compute_correlations: bool = True</code></p> <p>Enable/disable pairwise correlation computation.</p> <pre><code>from pysuricata import profile, ReportConfig, ComputeOptions\n\nconfig = ReportConfig(compute=ComputeOptions(\n    compute_correlations=False  # Disable for speed\n))\nreport = profile(df, config=config)\n</code></pre> <p><code>corr_threshold: float = 0.5</code></p> <p>Minimum |r| to report.</p> <pre><code>config = ReportConfig(compute=ComputeOptions(\n    corr_threshold=0.7  # Only strong correlations\n))\n</code></pre> <p><code>corr_max_cols: int = 50</code></p> <p>Maximum columns for correlation computation. Skip if exceeded.</p> <pre><code>config = ReportConfig(compute=ComputeOptions(\n    corr_max_cols=100  # Higher limit\n))\n</code></pre> <p><code>corr_max_per_col: int = 10</code></p> <p>Maximum correlations to show per column.</p> <pre><code>config = ReportConfig(compute=ComputeOptions(\n    corr_max_per_col=5  # Show top 5\n))\n</code></pre>"},{"location":"configuration/#renderoptions","title":"RenderOptions","text":"<p>Control report display and formatting.</p>"},{"location":"configuration/#basic-parameters_1","title":"Basic Parameters","text":"<p><code>title: Optional[str] = None</code></p> <p>Custom report title. If <code>None</code>, uses \"Data Profile Report\".</p> <pre><code>config.render.title = \"Customer Data Analysis - Q4 2024\"\n</code></pre> <p><code>description: Optional[str] = None</code></p> <p>Markdown-formatted description shown at top of report.</p> <pre><code>config.render.description = \"\"\"\n# Analysis Overview\n\nDataset contains customer transactions from **2024 Q4**.\n\nKey metrics:\n- 1.5M transactions\n- 50K unique customers\n\"\"\"\n</code></pre> <p><code>include_sample: bool = True</code></p> <p>Include sample rows in report.</p> <pre><code>config.render.include_sample = False  # Exclude sample\n</code></pre> <p><code>sample_rows: int = 10</code></p> <p>Number of sample rows to show (if <code>include_sample=True</code>).</p> <pre><code>config.render.sample_rows = 20  # Show 20 rows\n</code></pre>"},{"location":"configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"configuration/#memory-constrained-environment","title":"Memory-Constrained Environment","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Smaller sketches\nconfig.compute.top_k_size = 20  # Fewer top values\nconfig.compute.compute_correlations = False  # Skip correlations\n</code></pre>"},{"location":"configuration/#maximum-accuracy","title":"Maximum Accuracy","text":"<pre><code>config = ReportConfig()\nconfig.compute.numeric_sample_size = 100_000  # Large samples\nconfig.compute.uniques_sketch_size = 8_192  # Large sketches\nconfig.compute.top_k_size = 200  # Many top values\nconfig.compute.corr_threshold = 0.0  # All correlations\n</code></pre>"},{"location":"configuration/#speed-optimized","title":"Speed Optimized","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 500_000  # Large chunks\nconfig.compute.numeric_sample_size = 10_000  # Small samples\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.compute.top_k_size = 20  # Few top values\n</code></pre>"},{"location":"configuration/#reproducible-reports","title":"Reproducible Reports","text":"<pre><code>config = ReportConfig()\nconfig.compute.random_seed = 42  # Deterministic\nconfig.render.title = f\"Report generated {datetime.now()}\"\n</code></pre>"},{"location":"configuration/#production-data-quality-checks","title":"Production Data Quality Checks","text":"<pre><code># Only check specific columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"customer_id\", \"transaction_amount\", \"timestamp\"]\nconfig.render.include_sample = False  # No PII in reports\n\n# Generate stats only (no HTML)\nfrom pysuricata import summarize\nstats = summarize(df, config=config)\n\n# Assert quality thresholds\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"configuration/#legacy-engineconfig","title":"Legacy EngineConfig","text":"<p>Older versions used <code>EngineConfig</code>. It's still supported but deprecated.</p> <pre><code># Old way (deprecated)\nfrom pysuricata.config import EngineConfig\ncfg = EngineConfig(chunk_size=200_000, sample_k=20_000)\n\n# New way (recommended)\nfrom pysuricata import ReportConfig\nconfig = ReportConfig()\nconfig.compute.chunk_size = 200_000\nconfig.compute.numeric_sample_size = 20_000\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Not currently supported. All configuration via code.</p>"},{"location":"configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Invalid configurations raise <code>ValueError</code>:</p> <pre><code>config = ReportConfig()\nconfig.compute.chunk_size = -1  # Invalid\n# Raises: ValueError: chunk_size must be positive\n</code></pre>"},{"location":"configuration/#performance-impact","title":"Performance Impact","text":"Parameter Increase \u2192 Impact <code>chunk_size</code> \u2191 Faster, more memory <code>numeric_sample_size</code> \u2191 More accurate quantiles, more memory <code>uniques_sketch_size</code> \u2191 More accurate distinct, more memory <code>top_k_size</code> \u2191 More top values, more memory <code>compute_correlations</code> False Much faster, less memory"},{"location":"configuration/#see-also","title":"See Also","text":"<ul> <li>Performance Tips - Optimization strategies</li> <li>Advanced Features - Advanced usage patterns</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"contributing/","title":"Contributing to PySuricata","text":"<p>Thank you for considering contributing to PySuricata! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ </li> <li><code>uv</code> package manager (recommended) or <code>pip</code></li> <li>Git</li> </ul>"},{"location":"contributing/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/alvarodiez20/pysuricata.git\ncd pysuricata\n</code></pre>"},{"location":"contributing/#install-dependencies","title":"Install Dependencies","text":"Using uv (recommended)Using pip <pre><code>uv sync --dev\nuv run python -c \"import pysuricata; print('Success!')\"\n</code></pre> <pre><code>pip install -e \".[dev]\"\npython -c \"import pysuricata; print('Success!')\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=pysuricata --cov-report=html\n\n# Run specific test file\nuv run pytest tests/test_numeric.py\n\n# Run tests in parallel\nuv run pytest -n auto\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>PySuricata uses Ruff for linting and formatting.</p> <pre><code># Format code\nuv run ruff format pysuricata/\n\n# Check linting\nuv run ruff check pysuricata/\n\n# Auto-fix issues\nuv run ruff check --fix pysuricata/\n</code></pre>"},{"location":"contributing/#style-guidelines","title":"Style Guidelines","text":"<ul> <li>Follow PEP 8</li> <li>Line length: 88 characters (Black-style)</li> <li>Use type hints for function signatures</li> <li>Docstrings: Google style</li> </ul> <p>Example:</p> <pre><code>def compute_mean(values: np.ndarray) -&gt; float:\n    \"\"\"Compute arithmetic mean of values.\n\n    Args:\n        values: Array of numeric values\n\n    Returns:\n        Mean value\n\n    Raises:\n        ValueError: If array is empty\n    \"\"\"\n    if len(values) == 0:\n        raise ValueError(\"Cannot compute mean of empty array\")\n    return float(np.mean(values))\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#build-documentation-locally","title":"Build Documentation Locally","text":"<pre><code># Install docs dependencies\nuv sync --dev\n\n# Build docs\nuv run mkdocs serve\n\n# Open http://localhost:8000 in browser\n</code></pre>"},{"location":"contributing/#documentation-style","title":"Documentation Style","text":"<ul> <li>Use clear, concise language</li> <li>Include code examples</li> <li>Add mathematical formulas for algorithms</li> <li>Link to related pages</li> <li>Update relevant sections when changing code</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-feature-branch","title":"1. Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Branch naming: - <code>feature/</code> - New features - <code>fix/</code> - Bug fixes - <code>docs/</code> - Documentation only - <code>refactor/</code> - Code refactoring - <code>test/</code> - Test improvements</p>"},{"location":"contributing/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write tests for new functionality</li> <li>Update documentation</li> <li>Follow code style guidelines</li> <li>Keep commits atomic and well-described</li> </ul>"},{"location":"contributing/#3-run-checks","title":"3. Run Checks","text":"<pre><code># Format\nuv run ruff format pysuricata/\n\n# Lint\nuv run ruff check pysuricata/\n\n# Test\nuv run pytest\n\n# Type check (if using mypy)\nuv run mypy pysuricata/\n\n# Build docs\nuv run mkdocs build --strict\n</code></pre>"},{"location":"contributing/#4-commit-changes","title":"4. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add support for XYZ\"\n</code></pre> <p>Commit message format: - <code>feat:</code> - New feature - <code>fix:</code> - Bug fix - <code>docs:</code> - Documentation - <code>refactor:</code> - Code refactoring - <code>test:</code> - Test updates - <code>chore:</code> - Build/tooling changes</p>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create Pull Request on GitHub with: - Clear description of changes - Link to related issues - Screenshots for UI changes - Checklist of completed items</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#unit-tests","title":"Unit Tests","text":"<p>Test individual functions/classes in isolation.</p> <pre><code>def test_welford_mean():\n    \"\"\"Test Welford mean computation\"\"\"\n    from pysuricata.accumulators.algorithms import StreamingMoments\n\n    moments = StreamingMoments()\n    values = [1.0, 2.0, 3.0, 4.0, 5.0]\n\n    for v in values:\n        moments.update(np.array([v]))\n\n    result = moments.finalize()\n    assert abs(result[\"mean\"] - 3.0) &lt; 1e-10\n</code></pre>"},{"location":"contributing/#integration-tests","title":"Integration Tests","text":"<p>Test components working together.</p> <pre><code>def test_full_profile():\n    \"\"\"Test end-to-end profiling\"\"\"\n    df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [\"a\", \"b\", \"c\"]})\n    report = profile(df)\n\n    assert report.html is not None\n    assert len(report.stats[\"columns\"]) == 2\n</code></pre>"},{"location":"contributing/#property-based-tests","title":"Property-Based Tests","text":"<p>Use hypothesis for randomized testing.</p> <pre><code>from hypothesis import given\nfrom hypothesis.strategies import lists, floats\n\n@given(lists(floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_welford_matches_numpy(values):\n    \"\"\"Welford should match NumPy\"\"\"\n    moments = StreamingMoments()\n    for v in values:\n        moments.update(np.array([v]))\n\n    result = moments.finalize()\n    expected = np.mean(values)\n\n    assert abs(result[\"mean\"] - expected) &lt; 1e-6\n</code></pre>"},{"location":"contributing/#architecture-overview","title":"Architecture Overview","text":"<pre><code>pysuricata/\n\u251c\u2500\u2500 api.py              # Public API (profile, summarize)\n\u251c\u2500\u2500 report.py           # Report generation orchestration\n\u251c\u2500\u2500 config.py           # Configuration classes\n\u251c\u2500\u2500 accumulators/       # Streaming accumulators\n\u2502   \u251c\u2500\u2500 numeric.py      # Numeric statistics\n\u2502   \u251c\u2500\u2500 categorical.py  # Categorical analysis\n\u2502   \u251c\u2500\u2500 datetime.py     # Temporal analysis\n\u2502   \u2514\u2500\u2500 boolean.py      # Boolean analysis\n\u251c\u2500\u2500 compute/            # Data processing\n\u2502   \u251c\u2500\u2500 adapters/       # pandas/polars adapters\n\u2502   \u251c\u2500\u2500 analysis/       # Correlations, metrics\n\u2502   \u2514\u2500\u2500 processing/     # Chunking, inference\n\u251c\u2500\u2500 render/             # HTML generation\n\u2502   \u251c\u2500\u2500 *_card.py       # Variable type cards\n\u2502   \u251c\u2500\u2500 html.py         # Main template\n\u2502   \u2514\u2500\u2500 svg_utils.py    # SVG charts\n\u2514\u2500\u2500 templates/          # HTML templates\n</code></pre>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"contributing/#add-new-statistic-to-numeric-analysis","title":"Add New Statistic to Numeric Analysis","text":"<ol> <li> <p>Update accumulator (<code>pysuricata/accumulators/numeric.py</code>): <pre><code>class NumericAccumulator:\n    def __init__(self, ...):\n        self._new_stat = 0  # Add state\n\n    def update(self, values):\n        # Update new statistic\n        self._new_stat += some_computation(values)\n\n    def finalize(self):\n        return NumericSummary(\n            ...\n            new_stat=self._new_stat  # Include in summary\n        )\n</code></pre></p> </li> <li> <p>Update summary dataclass (<code>pysuricata/accumulators/numeric.py</code>): <pre><code>@dataclass\nclass NumericSummary:\n    ...\n    new_stat: float = 0.0\n</code></pre></p> </li> <li> <p>Update renderer (<code>pysuricata/render/numeric_card.py</code>): <pre><code>def render_card(self, stats):\n    # Add new_stat to HTML\n    html += f\"&lt;div&gt;New Stat: {stats.new_stat:.2f}&lt;/div&gt;\"\n</code></pre></p> </li> <li> <p>Add tests (<code>tests/test_numeric.py</code>): <pre><code>def test_new_stat():\n    acc = NumericAccumulator(\"test\")\n    acc.update(np.array([1, 2, 3]))\n    summary = acc.finalize()\n    assert summary.new_stat == expected_value\n</code></pre></p> </li> <li> <p>Update documentation (<code>docs/stats/numeric.md</code>): <pre><code>### New Statistic\n\nMathematical definition:\n\\[\n\\text{NewStat} = \\sum_{i=1}^{n} f(x_i)\n\\]\n\nInterpretation: ...\n</code></pre></p> </li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>(For maintainers only)</p> <ol> <li>Update version in <code>pyproject.toml</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create git tag: <code>git tag v0.x.y</code></li> <li>Push tag: <code>git push origin v0.x.y</code></li> <li>CI/CD automatically builds and publishes to PyPI</li> </ol>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help others learn and grow</li> <li>Focus on constructive feedback</li> <li>Assume good intentions</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Discussions</li> <li>GitHub Issues</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"examples/","title":"Examples Gallery","text":"<p>Real-world examples showing how to use PySuricata in various scenarios.</p>"},{"location":"examples/#small-dataset-iris","title":"Small Dataset (Iris)","text":"<p>Classic machine learning dataset with 150 rows \u00d7 5 columns.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load Iris dataset\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndf = pd.read_csv(url)\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"iris_report.html\")\n\nprint(f\"Rows: {len(df)}\")\nprint(f\"Columns: {len(df.columns)}\")\n# Output: Rows: 150, Columns: 5\n</code></pre> <p>Expected output: - 4 numeric variables (sepal/petal dimensions) - 1 categorical variable (species) - No missing values - Strong correlations between dimensions</p>"},{"location":"examples/#medium-dataset-titanic","title":"Medium Dataset (Titanic)","text":"<p>Popular dataset with mixed types and missing values.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load Titanic dataset\nurl = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\ndf = pd.read_csv(url)\n\n# Generate report\nreport = profile(df)\nreport.save_html(\"titanic_report.html\")\n</code></pre> <p>Features: - 891 rows \u00d7 12 columns - Numeric: age, fare, siblings/spouses - Categorical: name, ticket, cabin, embarked - Boolean: survived - Missing values in age (~20%), cabin (~77%)</p>"},{"location":"examples/#large-dataset-streaming","title":"Large Dataset (Streaming)","text":"<p>Process multi-GB dataset in bounded memory.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndef read_large_dataset():\n    \"\"\"Generator yielding chunks\"\"\"\n    for i in range(100):\n        yield pd.read_parquet(f\"data/part-{i}.parquet\")\n\n# Configure for large data\nconfig = ReportConfig()\nconfig.compute.chunk_size = 250_000\nconfig.compute.numeric_sample_size = 50_000\nconfig.compute.random_seed = 42\n\n# Profile\nreport = profile(read_large_dataset(), config=config)\nreport.save_html(\"large_dataset_report.html\")\n</code></pre>"},{"location":"examples/#wide-dataset-many-columns","title":"Wide Dataset (Many Columns)","text":"<p>Handle datasets with hundreds of columns.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile, ReportConfig\n\n# Create wide dataset\nn_rows, n_cols = 10_000, 500\ndf = pd.DataFrame(\n    np.random.randn(n_rows, n_cols),\n    columns=[f\"feature_{i}\" for i in range(n_cols)]\n)\n\n# Disable correlations (too expensive for 500 columns)\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False\n\nreport = profile(df, config=config)\nreport.save_html(\"wide_dataset_report.html\")\n</code></pre> <p>Note: For \\(p &gt; 100\\) columns, correlation computation is O(p\u00b2) and may be slow.</p>"},{"location":"examples/#time-series-data","title":"Time Series Data","text":"<p>Analyze temporal patterns in datetime columns.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile\n\n# Generate time series\ndates = pd.date_range(\"2023-01-01\", periods=10_000, freq=\"H\")\ndf = pd.DataFrame({\n    \"timestamp\": dates,\n    \"value\": np.random.randn(10_000).cumsum(),\n    \"category\": np.random.choice([\"A\", \"B\", \"C\"], 10_000)\n})\n\nreport = profile(df)\nreport.save_html(\"timeseries_report.html\")\n</code></pre> <p>Analysis includes: - Hour-of-day distribution - Day-of-week pattern - Month distribution - Monotonicity detection</p>"},{"location":"examples/#high-missing-values","title":"High Missing Values","text":"<p>Dataset with significant missing data.</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pysuricata import profile\n\n# Create dataset with missing values\ndf = pd.DataFrame({\n    \"always_present\": range(1000),\n    \"mostly_present\": [i if i % 10 != 0 else None for i in range(1000)],  # 10% missing\n    \"half_missing\": [i if i % 2 == 0 else None for i in range(1000)],     # 50% missing\n    \"mostly_missing\": [i if i % 10 == 0 else None for i in range(1000)],  # 90% missing\n})\n\nreport = profile(df)\nreport.save_html(\"missing_values_report.html\")\n</code></pre> <p>Report highlights: - Missing percentage per column - Chunk-level distribution - Data completeness visualizations</p>"},{"location":"examples/#all-categorical","title":"All Categorical","text":"<p>Text-heavy dataset (e.g., customer feedback).</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"customer_id\": [f\"CUST_{i:05d}\" for i in range(10_000)],\n    \"product\": np.random.choice([\"Product A\", \"Product B\", \"Product C\"], 10_000),\n    \"rating\": np.random.choice([\"Poor\", \"Fair\", \"Good\", \"Excellent\"], 10_000),\n    \"feedback\": [f\"Comment {i}\" for i in range(10_000)],\n})\n\nreport = profile(df)\nreport.save_html(\"categorical_report.html\")\n</code></pre> <p>Analysis includes: - Top values and frequencies - Distinct counts (KMV sketch) - String length statistics - Entropy and Gini metrics</p>"},{"location":"examples/#polars-dataframe","title":"Polars DataFrame","text":"<p>Use polars instead of pandas.</p> <pre><code>import polars as pl\nfrom pysuricata import profile\n\n# Create polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(1000),\n    \"value\": [float(i) for i in range(1000)],\n    \"category\": [\"A\"] * 500 + [\"B\"] * 500\n})\n\n# Profile works natively with polars\nreport = profile(df)\nreport.save_html(\"polars_report.html\")\n</code></pre>"},{"location":"examples/#polars-lazyframe","title":"Polars LazyFrame","text":"<p>Streaming evaluation with polars.</p> <pre><code>import polars as pl\nfrom pysuricata import profile, ReportConfig\n\n# Create lazy frame\nlf = pl.scan_csv(\"large_file.csv\").filter(pl.col(\"value\") &gt; 0)\n\n# Configure chunk size\nconfig = ReportConfig()\nconfig.compute.chunk_size = 50_000\n\n# Profile lazily evaluated data\nreport = profile(lf, config=config)\nreport.save_html(\"polars_lazy_report.html\")\n</code></pre>"},{"location":"examples/#jupyter-notebook-integration","title":"Jupyter Notebook Integration","text":"<p>Display reports inline in notebooks.</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nreport = profile(df)\n\n# Display inline (automatic)\nreport\n\n# Or with custom size\nreport.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"examples/#programmatic-access-stats-only","title":"Programmatic Access (Stats Only)","text":"<p>Use for data quality checks without HTML.</p> <pre><code>from pysuricata import summarize\n\n# Get statistics only (faster than full report)\nstats = summarize(df)\n\n# Check data quality\nprint(f\"Rows: {stats['dataset']['rows']}\")\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.1f}%\")\nprint(f\"Duplicate rows: {stats['dataset']['duplicate_rows_pct_est']:.1f}%\")\n\n# Per-column stats\nfor col, col_stats in stats[\"columns\"].items():\n    if col_stats.get(\"missing_pct\", 0) &gt; 10:\n        print(f\"{col}: {col_stats['missing_pct']:.1f}% missing\")\n</code></pre>"},{"location":"examples/#cicd-data-quality-gates","title":"CI/CD Data Quality Gates","text":"<p>Enforce quality thresholds in pipelines.</p> <pre><code>from pysuricata import summarize\n\ndef validate_data_quality(df):\n    \"\"\"Validate data quality, raise if fails\"\"\"\n    stats = summarize(df)\n\n    # Check missing data\n    missing_pct = stats[\"dataset\"][\"missing_cells_pct\"]\n    assert missing_pct &lt; 5.0, f\"Too many missing values: {missing_pct:.1f}%\"\n\n    # Check duplicates\n    dup_pct = stats[\"dataset\"][\"duplicate_rows_pct_est\"]\n    assert dup_pct &lt; 1.0, f\"Too many duplicates: {dup_pct:.1f}%\"\n\n    # Check specific columns\n    for col in [\"customer_id\", \"transaction_id\"]:\n        col_stats = stats[\"columns\"][col]\n        assert col_stats[\"distinct\"] == col_stats[\"count\"], \\\n            f\"{col} has duplicates\"\n\n    print(\"\u2713 Data quality checks passed\")\n\n# Use in pipeline\nvalidate_data_quality(df)\n</code></pre>"},{"location":"examples/#custom-column-selection","title":"Custom Column Selection","text":"<p>Profile only specific columns.</p> <pre><code>from pysuricata import profile, ReportConfig\n\n# Large dataset, only analyze key columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"user_id\", \"purchase_amount\", \"timestamp\"]\n\nreport = profile(df, config=config)\nreport.save_html(\"key_columns_report.html\")\n</code></pre>"},{"location":"examples/#reproducible-reports","title":"Reproducible Reports","text":"<p>Generate identical reports across runs.</p> <pre><code>from pysuricata import profile, ReportConfig\nfrom datetime import datetime\n\n# Set random seed\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\n# Add metadata\nconfig.render.title = \"Weekly Data Report\"\nconfig.render.description = f\"\"\"\nReport generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n\nDataset: production.customers\nVersion: 1.2.3\n\"\"\"\n\nreport = profile(df, config=config)\nreport.save_html(f\"report_{datetime.now().strftime('%Y%m%d')}.html\")\n</code></pre>"},{"location":"examples/#memory-constrained-environment","title":"Memory-Constrained Environment","text":"<p>Profile on device with limited RAM.</p> <pre><code>from pysuricata import profile, ReportConfig\n\n# Optimize for low memory\nconfig = ReportConfig()\nconfig.compute.chunk_size = 10_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Small sketches\nconfig.compute.top_k_size = 20  # Few top values\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\nreport.save_html(\"low_memory_report.html\")\n</code></pre>"},{"location":"examples/#export-statistics-as-json","title":"Export Statistics as JSON","text":"<p>Save stats for external processing.</p> <pre><code>from pysuricata import profile\n\nreport = profile(df)\n\n# Save HTML\nreport.save_html(\"report.html\")\n\n# Save JSON\nreport.save_json(\"report.json\")\n\n# Or load JSON for custom analysis\nimport json\nwith open(\"report.json\") as f:\n    stats = json.load(f)\n\n# Custom visualization\nimport matplotlib.pyplot as plt\nmissing = {col: s[\"missing_pct\"] for col, s in stats[\"columns\"].items()}\nplt.bar(missing.keys(), missing.values())\nplt.title(\"Missing Values by Column\")\nplt.xticks(rotation=90)\nplt.tight_layout()\nplt.savefig(\"missing_chart.png\")\n</code></pre>"},{"location":"examples/#combine-multiple-datasets","title":"Combine Multiple Datasets","text":"<p>Compare multiple datasets (manual).</p> <pre><code>from pysuricata import summarize\n\n# Profile multiple datasets\nstats_train = summarize(df_train)\nstats_test = summarize(df_test)\n\n# Compare key metrics\nprint(\"Train vs Test Comparison:\")\nprint(f\"Rows: {stats_train['dataset']['rows']} vs {stats_test['dataset']['rows']}\")\nprint(f\"Missing: {stats_train['dataset']['missing_cells_pct']:.1f}% vs {stats_test['dataset']['missing_cells_pct']:.1f}%\")\n\n# Column-level comparison\nfor col in df_train.columns:\n    train_mean = stats_train[\"columns\"][col].get(\"mean\")\n    test_mean = stats_test[\"columns\"][col].get(\"mean\")\n    if train_mean and test_mean:\n        print(f\"{col} mean: {train_mean:.2f} vs {test_mean:.2f}\")\n</code></pre>"},{"location":"examples/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Configuration for all options</li> <li>See Performance Tips for optimization</li> <li>Check Advanced Features for power user tips</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-pysuricata","title":"What is PySuricata?","text":"<p>PySuricata is a Python library for exploratory data analysis that generates self-contained HTML reports. It uses streaming algorithms to process data in chunks, keeping memory bounded regardless of dataset size.</p>"},{"location":"faq/#is-pysuricata-production-ready","title":"Is PySuricata production-ready?","text":"<p>PySuricata is actively maintained with CI/CD, test coverage tracked via Codecov, and regular releases on PyPI. That said, evaluate it against your own requirements \u2014 it's still a young project.</p>"},{"location":"faq/#installation","title":"Installation","text":""},{"location":"faq/#how-do-i-install-pysuricata","title":"How do I install PySuricata?","text":"<pre><code>pip install pysuricata\n</code></pre> <p>With polars support:</p> <pre><code>pip install pysuricata[polars]\n</code></pre>"},{"location":"faq/#what-are-the-dependencies","title":"What are the dependencies?","text":"<p>Required: pandas, markdown, psutil, numpy (on Python \u22653.13)</p> <p>Optional: polars (install with <code>pip install pysuricata[polars]</code>)</p> <p>PySuricata requires Python 3.10+.</p>"},{"location":"faq/#why-is-my-installation-failing","title":"Why is my installation failing?","text":"<p>Common issues:</p> <ol> <li> <p>Python version \u2014 PySuricata requires Python 3.10+:    <pre><code>python --version\n</code></pre></p> </li> <li> <p>Conflicting packages \u2014 Try a fresh virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate\npip install pysuricata\n</code></pre></p> </li> </ol>"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#how-do-i-generate-a-report","title":"How do I generate a report?","text":"<pre><code>from pysuricata import profile\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"faq/#can-i-profile-only-specific-columns","title":"Can I profile only specific columns?","text":"<p>Yes:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.columns = [\"col1\", \"col2\", \"col3\"]\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"faq/#how-do-i-make-reports-reproducible","title":"How do I make reports reproducible?","text":"<p>Set a random seed:</p> <pre><code>config = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"faq/#can-i-get-statistics-without-generating-html","title":"Can I get statistics without generating HTML?","text":"<p>Yes, use <code>summarize()</code>:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nprint(stats[\"dataset\"])\nprint(stats[\"columns\"][\"my_column\"])\n</code></pre>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#how-much-memory-does-pysuricata-use","title":"How much memory does PySuricata use?","text":"<p>Memory usage depends on configuration, not dataset size. The main factors are:</p> <ul> <li>chunk_size \u2014 rows held in memory per iteration (default: 200,000)</li> <li>numeric_sample_size \u2014 reservoir sample size per numeric column (default: 20,000)</li> <li>uniques_sketch_size \u2014 KMV sketch size per column (default: 2,048)</li> </ul> <p>Processing a 10 GB dataset uses roughly the same memory as processing a 100 MB one.</p>"},{"location":"faq/#my-report-is-slow-how-can-i-speed-it-up","title":"My report is slow. How can I speed it up?","text":"<p>Three quick changes:</p> <pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False    # Skip O(p\u00b2) correlation step\nconfig.compute.numeric_sample_size = 10_000    # Smaller reservoir sample\nconfig.compute.chunk_size = 500_000            # Fewer iterations\n</code></pre> <p>See Performance Tips for more strategies.</p>"},{"location":"faq/#can-pysuricata-handle-very-large-datasets","title":"Can PySuricata handle very large datasets?","text":"<p>Yes, by passing a generator:</p> <pre><code>def read_large_dataset():\n    for file in large_files:\n        yield pd.read_parquet(file)\n\nreport = profile(read_large_dataset())\n</code></pre> <p>Memory stays bounded because only one chunk is in memory at a time.</p>"},{"location":"faq/#why-are-correlations-slow","title":"Why are correlations slow?","text":"<p>Correlation computation is O(p\u00b2) where p is the number of numeric columns. For datasets with many numeric columns, either disable correlations or increase <code>corr_threshold</code> to reduce the number reported.</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#are-the-statistics-exact-or-approximate","title":"Are the statistics exact or approximate?","text":"<p>Exact:</p> <ul> <li>Mean, variance, skewness, kurtosis (Welford/P\u00e9bay algorithms)</li> <li>Min, max, count</li> </ul> <p>Approximate:</p> <ul> <li>Distinct count \u2014 KMV sketch, ~2.2% error with default k=2048</li> <li>Top-k values \u2014 Misra-Gries, guaranteed to find all items with frequency &gt; n/k</li> <li>Quantiles \u2014 computed from a reservoir sample</li> </ul>"},{"location":"faq/#what-algorithms-does-pysuricata-use","title":"What algorithms does PySuricata use?","text":"Algorithm Purpose Reference Welford/P\u00e9bay Exact streaming moments Welford (1962), P\u00e9bay (2008) KMV sketch Distinct count estimation Bar-Yossef et al. (2002) Misra-Gries Top-k frequent values Misra &amp; Gries (1982) Reservoir sampling Uniform random sample Vitter (1985) <p>See Statistical Methods for details.</p>"},{"location":"faq/#does-pysuricata-support-distributed-computing","title":"Does PySuricata support distributed computing?","text":"<p>Accumulators are mergeable \u2014 you can process data on separate machines and combine results. However, PySuricata doesn't include built-in distribution; you'd need to use an external framework.</p>"},{"location":"faq/#data","title":"Data","text":""},{"location":"faq/#does-pysuricata-modify-my-data","title":"Does PySuricata modify my data?","text":"<p>No. PySuricata only reads data, never modifies it.</p>"},{"location":"faq/#what-data-formats-are-supported","title":"What data formats are supported?","text":"<p>Anything that can be loaded into pandas or polars: CSV, Parquet, JSON, Excel, SQL databases. Load into a DataFrame first, then pass it to <code>profile()</code>.</p>"},{"location":"faq/#how-does-pysuricata-handle-missing-values","title":"How does PySuricata handle missing values?","text":"<p>Missing values are excluded from statistical calculations (mean, variance, etc.) and reported separately with count and percentage per column.</p>"},{"location":"faq/#reports","title":"Reports","text":""},{"location":"faq/#why-is-my-html-report-large","title":"Why is my HTML report large?","text":"<p>Report size grows with the number of columns. Each column adds a variable card with statistics and an SVG chart. To reduce size, profile fewer columns or reduce <code>top_k_size</code>.</p>"},{"location":"faq/#can-i-display-reports-in-jupyter","title":"Can I display reports in Jupyter?","text":"<pre><code>report = profile(df)\nreport  # Auto-displays inline\n\n# Or with custom height\nreport.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"faq/#can-i-export-to-pdf","title":"Can I export to PDF?","text":"<p>Not built-in. You can print the HTML report to PDF from your browser, or use a tool like <code>wkhtmltopdf</code>.</p>"},{"location":"faq/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Discussions</li> <li>GitHub Issues</li> </ul> <p>Still have questions? Ask in GitHub Discussions.</p>"},{"location":"install/","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install pysuricata\n</code></pre> <p>Optional: install <code>polars</code> to use polars DataFrames directly:</p> <pre><code>pip install polars\n</code></pre> <p>Verify your installation:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pysuricata import profile\n&gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2, 3]})\n&gt;&gt;&gt; profile(df).html[:15]\n'&lt;!DOCTYPE html&gt;'\n</code></pre>"},{"location":"performance/","title":"Performance Tips","text":"<p>Optimize PySuricata for your specific use case with these strategies.</p>"},{"location":"performance/#quick-wins","title":"Quick Wins","text":""},{"location":"performance/#1-disable-correlations-for-many-columns","title":"1. Disable Correlations for Many Columns","text":"<p>For datasets with &gt; 100 numeric columns, correlation computation is O(p\u00b2) and can be slow.</p> <pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\n</code></pre> <p>Speed improvement: 2-10x for wide datasets</p>"},{"location":"performance/#2-increase-chunk-size","title":"2. Increase Chunk Size","text":"<p>Larger chunks mean fewer iterations and less overhead.</p> <pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 500_000  # Default: 200_000\n\nreport = profile(df, config=config)\n</code></pre> <p>Trade-off: More memory usage</p>"},{"location":"performance/#3-reduce-sample-sizes","title":"3. Reduce Sample Sizes","text":"<p>Smaller samples are faster to process.</p> <pre><code>config = ReportConfig()\nconfig.compute.numeric_sample_size = 10_000  # Default: 20_000\n\nreport = profile(df, config=config)\n</code></pre> <p>Trade-off: Slightly less accurate quantiles</p>"},{"location":"performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"performance/#memory-constrained-environments","title":"Memory-Constrained Environments","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Small chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 1_024  # Small sketches\nconfig.compute.top_k_size = 20  # Fewer top values\nconfig.compute.compute_correlations = False  # Skip correlations\n\nreport = profile(df, config=config)\n</code></pre> <p>Memory usage: ~20-30 MB (vs ~50 MB default)</p>"},{"location":"performance/#monitor-memory-usage","title":"Monitor Memory Usage","text":"<pre><code>import psutil\nimport os\n\nprocess = psutil.Process(os.getpid())\nprint(f\"Memory before: {process.memory_info().rss / 1024**2:.1f} MB\")\n\nreport = profile(df, config=config)\n\nprint(f\"Memory after: {process.memory_info().rss / 1024**2:.1f} MB\")\n</code></pre>"},{"location":"performance/#speed-optimization","title":"Speed Optimization","text":""},{"location":"performance/#profile-only-key-columns","title":"Profile Only Key Columns","text":"<pre><code>config = ReportConfig()\nconfig.compute.columns = [\"user_id\", \"amount\", \"timestamp\"]\n\nreport = profile(df, config=config)\n</code></pre> <p>Speed improvement: Linear in number of columns</p>"},{"location":"performance/#use-polars-for-large-datasets","title":"Use Polars for Large Datasets","text":"<p>Polars can be faster than pandas for certain operations:</p> <pre><code>import polars as pl\n\ndf = pl.read_csv(\"large_file.csv\")\nreport = profile(df)  # Native polars support\n</code></pre>"},{"location":"performance/#parallelize-with-dask-advanced","title":"Parallelize with Dask (Advanced)","text":"<pre><code>import dask.dataframe as dd\n\n# Load with Dask\nddf = dd.read_csv(\"large_file.csv\")\n\n# Convert partitions to generator\ndef partition_generator():\n    for partition in ddf.partitions:\n        yield partition.compute()\n\nreport = profile(partition_generator())\n</code></pre>"},{"location":"performance/#benchmarks","title":"Benchmarks","text":""},{"location":"performance/#performance-by-dataset-size","title":"Performance by Dataset Size","text":"Rows Columns Time Throughput Memory 10K 10 ~3s ~3,000 rows/s 20 MB 100K 10 ~13s ~8,000 rows/s 30 MB 1M 10 ~3 min ~5,500 rows/s 50 MB 10M 10 ~30 min ~5,500 rows/s 50 MB <p>Benchmarks measured on Apple Silicon with Python 3.13. Actual times vary by hardware, data complexity, and configuration.</p>"},{"location":"performance/#scalability","title":"Scalability","text":"<p>PySuricata scales linearly with dataset size (O(n)) thanks to streaming algorithms:</p> <pre><code>Time(n) \u2248 k \u00d7 n\n</code></pre> <p>where k is constant per row processing time.</p>"},{"location":"performance/#streaming-advantage","title":"Streaming Advantage","text":"<p>Because PySuricata processes data in chunks, memory stays bounded regardless of dataset size. Tools that load the full dataset into memory cannot process datasets larger than available RAM.</p>"},{"location":"performance/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"performance/#for-maximum-speed","title":"For Maximum Speed","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 1_000_000  # Large chunks\nconfig.compute.numeric_sample_size = 5_000  # Small samples\nconfig.compute.uniques_sketch_size = 512  # Tiny sketches\nconfig.compute.top_k_size = 10  # Few top values\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.render.include_sample = False  # No sample in report\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"performance/#for-maximum-accuracy","title":"For Maximum Accuracy","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 100_000  # Smaller for better merging\nconfig.compute.numeric_sample_size = 100_000  # Large samples\nconfig.compute.uniques_sketch_size = 8_192  # Large sketches\nconfig.compute.top_k_size = 200  # Many top values\nconfig.compute.corr_threshold = 0.0  # All correlations\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"performance/#profiling-pysuricata","title":"Profiling PySuricata","text":"<p>Use Python's profiler to find bottlenecks:</p> <pre><code>import cProfile\nimport pstats\n\nprofiler = cProfile.Profile()\nprofiler.enable()\n\nreport = profile(df)\n\nprofiler.disable()\nstats = pstats.Stats(profiler)\nstats.sort_stats('cumulative')\nstats.print_stats(20)  # Top 20 functions\n</code></pre>"},{"location":"performance/#common-bottlenecks","title":"Common Bottlenecks","text":""},{"location":"performance/#1-correlation-computation","title":"1. Correlation Computation","text":"<p>Symptom: Slow for &gt; 100 numeric columns Solution: Disable correlations or increase threshold</p>"},{"location":"performance/#2-many-categorical-columns","title":"2. Many Categorical Columns","text":"<p>Symptom: Slow with &gt; 50 categorical columns Solution: Reduce <code>top_k_size</code>, increase <code>chunk_size</code></p>"},{"location":"performance/#3-very-wide-datasets-1000-columns","title":"3. Very Wide Datasets (&gt; 1000 columns)","text":"<p>Symptom: Slow overall Solution: Profile in batches, combine reports manually</p>"},{"location":"performance/#4-small-chunk-size","title":"4. Small Chunk Size","text":"<p>Symptom: Slow despite small dataset Solution: Increase <code>chunk_size</code> to reduce overhead</p>"},{"location":"performance/#production-optimization","title":"Production Optimization","text":""},{"location":"performance/#scheduled-reports","title":"Scheduled Reports","text":"<p>For regular reporting, optimize for speed:</p> <pre><code># Fast config for nightly reports\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False\nconfig.compute.numeric_sample_size = 10_000\nconfig.render.title = f\"Daily Report - {date.today()}\"\n\nreport = profile(df, config=config)\nreport.save_html(f\"reports/daily_{date.today()}.html\")\n</code></pre>"},{"location":"performance/#cicd-quality-checks","title":"CI/CD Quality Checks","text":"<p>Use <code>summarize()</code> for faster stats-only checks:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)  # Faster than profile()\n\n# Check thresholds\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"performance/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide - All configuration options</li> <li>Examples - Real-world use cases</li> <li>Advanced Features - Power user tips</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get started with PySuricata in less than 5 minutes!</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install pysuricata from PyPI:</p> <pre><code>pip install pysuricata\n</code></pre> <p>For polars support (optional):</p> <pre><code>pip install pysuricata polars\n</code></pre>"},{"location":"quickstart/#command-line-usage","title":"Command Line Usage","text":"<p>The fastest way to profile a dataset:</p> <pre><code># Generate an HTML report\npysuricata profile data.csv --output report.html\n\n# Get JSON statistics (no HTML)\npysuricata summarize data.csv --output stats.json\n\n# With options\npysuricata profile data.csv -o report.html --seed 42 --no-correlations\n</code></pre> <p>See <code>pysuricata --help</code> for all options.</p>"},{"location":"quickstart/#your-first-report","title":"Your First Report","text":""},{"location":"quickstart/#step-1-import-and-load-data","title":"Step 1: Import and Load Data","text":"PandasPolarsFrom URL <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load a dataset\ndf = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\n# Load a dataset\ndf = pl.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\")\n</code></pre> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Load directly from URL\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\ndf = pd.read_csv(url)\n</code></pre>"},{"location":"quickstart/#step-2-generate-report","title":"Step 2: Generate Report","text":"<pre><code># Create the profile report\nreport = profile(df)\n\n# Save to HTML\nreport.save_html(\"iris_report.html\")\n</code></pre> <p>That's it! Open <code>iris_report.html</code> in your browser to see a comprehensive analysis.</p>"},{"location":"quickstart/#understanding-your-report","title":"Understanding Your Report","text":"<p>The generated report contains several sections:</p>"},{"location":"quickstart/#1-dataset-overview","title":"1. Dataset Overview","text":"<ul> <li>Number of rows and columns</li> <li>Memory usage (approximate)</li> <li>Missing values summary</li> <li>Duplicate rows estimate</li> <li>Processing time</li> </ul>"},{"location":"quickstart/#2-variables-section","title":"2. Variables Section","text":"<p>For each variable, you'll see:</p>"},{"location":"quickstart/#numeric-variables","title":"Numeric Variables","text":"<ul> <li>Count, missing percentage</li> <li>Mean, median, standard deviation</li> <li>Min, max, range</li> <li>Quantiles (Q1, Q2, Q3)</li> <li>Skewness and kurtosis</li> <li>Histogram visualization</li> </ul>"},{"location":"quickstart/#categorical-variables","title":"Categorical Variables","text":"<ul> <li>Count, missing percentage</li> <li>Number of unique values</li> <li>Top values with frequencies</li> <li>Diversity metrics</li> </ul>"},{"location":"quickstart/#datetime-variables","title":"DateTime Variables","text":"<ul> <li>Temporal range (min/max)</li> <li>Distribution by hour, day of week, month</li> <li>Timeline visualization</li> </ul>"},{"location":"quickstart/#boolean-variables","title":"Boolean Variables","text":"<ul> <li>True/False counts</li> <li>Balance ratio</li> <li>Missing percentage</li> </ul>"},{"location":"quickstart/#3-correlations-for-numeric-columns","title":"3. Correlations (for numeric columns)","text":"<ul> <li>Top correlations for each numeric variable</li> <li>Correlation strength indicators</li> </ul>"},{"location":"quickstart/#working-with-your-data","title":"Working with Your Data","text":""},{"location":"quickstart/#save-as-json-for-programmatic-access","title":"Save as JSON (for programmatic access)","text":"<pre><code># Generate statistics only\nfrom pysuricata import summarize\n\nstats = summarize(df)\nprint(stats[\"dataset\"])  # Dataset-level metrics\nprint(stats[\"columns\"][\"sepal_length\"])  # Per-column stats\n\n# Or save from report\nreport.save_json(\"iris_stats.json\")\n</code></pre>"},{"location":"quickstart/#display-in-jupyter-notebook","title":"Display in Jupyter Notebook","text":"<pre><code># In a Jupyter notebook\nfrom pysuricata import profile\n\nreport = profile(df)\nreport  # Automatically displays inline\n</code></pre> <p>For better display in notebooks:</p> <pre><code>report.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"quickstart/#quick-data-quality-check","title":"Quick Data Quality Check","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\n# Check data quality metrics\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.2f}%\")\nprint(f\"Duplicate rows: {stats['dataset']['duplicate_rows_pct_est']:.2f}%\")\n\n# Assert quality requirements\nassert stats['dataset']['missing_cells_pct'] &lt; 5.0, \"Too many missing values\"\n</code></pre>"},{"location":"quickstart/#profile-specific-columns","title":"Profile Specific Columns","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Select specific columns\nconfig = ReportConfig()\nconfig.compute.columns = [\"sepal_length\", \"sepal_width\", \"species\"]\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#reproducible-reports","title":"Reproducible Reports","text":"<pre><code>from pysuricata import profile, ReportConfig\n\n# Set random seed for deterministic sampling\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n# Same report every time!\n</code></pre>"},{"location":"quickstart/#process-large-files-in-chunks","title":"Process Large Files in Chunks","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# Read and process in chunks\ndef read_chunks():\n    for chunk in pd.read_csv(\"large_file.csv\", chunksize=100_000):\n        yield chunk\n\nreport = profile(read_chunks())\nreport.save_html(\"large_file_report.html\")\n</code></pre>"},{"location":"quickstart/#configuration-basics","title":"Configuration Basics","text":"<p>PySuricata is highly configurable. Here are some common settings:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Adjust chunk size for memory management\nconfig.compute.chunk_size = 200_000  # Default\n\n# Control sample sizes\nconfig.compute.numeric_sample_size = 20_000  # For quantiles/histograms\nconfig.compute.uniques_sketch_size = 2_048   # For distinct counts\nconfig.compute.top_k_size = 50               # For top values\n\n# Enable/disable correlations\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5\n\n# Deterministic sampling\nconfig.compute.random_seed = 42\n\n# Generate report\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#performance-tips","title":"Performance Tips","text":""},{"location":"quickstart/#for-large-datasets-1-gb","title":"For Large Datasets (&gt; 1 GB)","text":"<pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.chunk_size = 250_000  # Larger chunks\nconfig.compute.numeric_sample_size = 10_000  # Smaller samples\nconfig.compute.compute_correlations = False  # Skip if not needed\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#for-memory-constrained-environments","title":"For Memory-Constrained Environments","text":"<pre><code>config = ReportConfig()\nconfig.compute.chunk_size = 50_000  # Smaller chunks\nconfig.compute.numeric_sample_size = 5_000  # Smaller samples\nconfig.compute.uniques_sketch_size = 1_024  # Smaller sketches\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#for-speed","title":"For Speed","text":"<pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Skip correlations\nconfig.compute.top_k_size = 20  # Fewer top values\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've created your first report, explore:</p> <ul> <li>Basic Usage - Detailed usage patterns</li> <li>Configuration - All configuration options</li> <li>Performance Tips - Optimize for your use case</li> <li>Examples Gallery - More real-world examples</li> <li>Statistical Methods - Understand the algorithms</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#report-is-too-large","title":"Report is too large","text":"<ul> <li>Reduce <code>numeric_sample_size</code></li> <li>Skip correlations: <code>config.compute.compute_correlations = False</code></li> <li>Profile fewer columns: <code>config.compute.columns = [\"col1\", \"col2\"]</code></li> </ul>"},{"location":"quickstart/#out-of-memory","title":"Out of memory","text":"<ul> <li>Reduce <code>chunk_size</code></li> <li>Reduce all sample sizes</li> <li>Process columns separately</li> </ul>"},{"location":"quickstart/#report-takes-too-long","title":"Report takes too long","text":"<ul> <li>Increase <code>chunk_size</code> (if memory allows)</li> <li>Disable correlations</li> <li>Reduce <code>top_k_size</code></li> </ul>"},{"location":"quickstart/#want-more-decimal-places","title":"Want more decimal places","text":"<pre><code># Not currently configurable, but stats JSON has full precision\nreport.save_json(\"stats.json\")\n</code></pre>"},{"location":"quickstart/#get-help","title":"Get Help","text":"<ul> <li>\ud83d\udcd6 Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udce7 Contact Maintainer</li> </ul> <p>Ready for more advanced features? Check out the Advanced Guide.</p>"},{"location":"sequence-diagrams-complexity/","title":"Annotated Sequence Diagrams with Complexity Analysis","text":"<p>This document contains sequence diagrams showing the data flow through PySuricata's streaming algorithms, annotated with time and space complexity for each operation.</p>"},{"location":"sequence-diagrams-complexity/#pandas-data-processing-flow","title":"Pandas Data Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Profile\n    participant Adapter as PandasAdapter\n    participant Accumulator as NumericAccumulator\n    participant KMV\n    participant Reservoir as ReservoirSampler\n    participant Extreme as ExtremeTracker\n    participant MG as MisraGries\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(1) - Configuration setup\n\n    Profile-&gt;&gt;Adapter: process_chunk(chunk)\n    Note over Adapter: O(n) - Iterate through rows\n\n    loop For each numeric column\n        Adapter-&gt;&gt;Accumulator: update(values)\n        Note over Accumulator: O(n) - Process n values\n\n        Accumulator-&gt;&gt;KMV: add(value)\n        Note over KMV: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;Reservoir: add(value)\n        Note over Reservoir: O(1) - Constant time&lt;br/&gt;Space: O(s) bounded\n\n        Accumulator-&gt;&gt;Extreme: update(values, indices)\n        Note over Extreme: O(n log k) - Process n values&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;MG: add(value)\n        Note over MG: O(1) - Constant time&lt;br/&gt;Space: O(k) bounded\n    end\n\n    Accumulator-&gt;&gt;Profile: finalize()\n    Note over Accumulator: O(k log k) - Extract results&lt;br/&gt;Space: O(k) bounded\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results</code></pre>"},{"location":"sequence-diagrams-complexity/#polars-data-processing-flow","title":"Polars Data Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Profile\n    participant Adapter as PolarsAdapter\n    participant Accumulator as NumericAccumulator\n    participant KMV\n    participant Reservoir as ReservoirSampler\n    participant Extreme as ExtremeTracker\n    participant MG as MisraGries\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(1) - Configuration setup\n\n    Profile-&gt;&gt;Adapter: process_chunk(chunk)\n    Note over Adapter: O(n) - Iterate through rows\n\n    loop For each numeric column\n        Adapter-&gt;&gt;Accumulator: update(values)\n        Note over Accumulator: O(n) - Process n values\n\n        Accumulator-&gt;&gt;KMV: add(value)\n        Note over KMV: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;Reservoir: add(value)\n        Note over Reservoir: O(1) - Constant time&lt;br/&gt;Space: O(s) bounded\n\n        Accumulator-&gt;&gt;Extreme: update(values, indices)\n        Note over Extreme: O(n log k) - Process n values&lt;br/&gt;Space: O(k) bounded\n\n        Accumulator-&gt;&gt;MG: add(value)\n        Note over MG: O(1) - Constant time&lt;br/&gt;Space: O(k) bounded\n    end\n\n    Accumulator-&gt;&gt;Profile: finalize()\n    Note over Accumulator: O(k log k) - Extract results&lt;br/&gt;Space: O(k) bounded\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results</code></pre>"},{"location":"sequence-diagrams-complexity/#kmv-sketch-memory-optimization","title":"KMV Sketch Memory Optimization","text":"<pre><code>sequenceDiagram\n    participant KMV\n    participant ExactCounter as _exact_counter\n    participant Values as _values\n    participant Hash as _u64\n\n    Note over KMV: Before Fix: O(n) memory growth&lt;br/&gt;After Fix: O(k) bounded memory\n\n    KMV-&gt;&gt;ExactCounter: add(value)\n    Note over ExactCounter: O(1) - Dict lookup&lt;br/&gt;Space: O(min(n, max_exact_tracking))\n\n    alt Exact mode (count &lt; max_exact_tracking)\n        ExactCounter-&gt;&gt;ExactCounter: increment counter\n        Note over ExactCounter: O(1) - Dict update\n    else Transition to approximation mode\n        ExactCounter-&gt;&gt;Values: convert to hashes\n        Note over Values: O(k) - Convert exact values&lt;br/&gt;Space: O(k) bounded\n\n        ExactCounter-&gt;&gt;ExactCounter: clear()\n        Note over ExactCounter: O(1) - Free memory\n\n        Values-&gt;&gt;Values: sort()\n        Note over Values: O(k log k) - Sort hashes\n    end\n\n    alt Approximation mode\n        KMV-&gt;&gt;Hash: _u64(value)\n        Note over Hash: O(1) - Hash computation\n\n        Hash-&gt;&gt;Values: insert if smaller\n        Note over Values: O(log k) - Binary search insert&lt;br/&gt;Space: O(k) bounded\n    end\n\n    KMV-&gt;&gt;Values: estimate()\n    Note over Values: O(1) - Direct calculation&lt;br/&gt;Space: O(k) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#extremetracker-memory-optimization","title":"ExtremeTracker Memory Optimization","text":"<pre><code>sequenceDiagram\n    participant Extreme as ExtremeTracker\n    participant MinHeap as _min_heap\n    participant MaxHeap as _max_heap\n    participant Heapq\n\n    Note over Extreme: Before Fix: O(k \u00d7 chunks) temporary growth&lt;br/&gt;After Fix: O(k) constant space\n\n    Extreme-&gt;&gt;Extreme: update(values, indices)\n    Note over Extreme: O(n log k) - Process n values\n\n    loop For each value\n        Extreme-&gt;&gt;MinHeap: _add_to_min_heap(index, value)\n        Note over MinHeap: O(log k) - Heap insert&lt;br/&gt;Space: O(k) bounded\n\n        Extreme-&gt;&gt;MaxHeap: _add_to_max_heap(index, value)\n        Note over MaxHeap: O(log k) - Heap insert&lt;br/&gt;Space: O(k) bounded\n    end\n\n    alt Min heap not full\n        MinHeap-&gt;&gt;Heapq: heappush(value, index)\n        Note over Heapq: O(log k) - Heap insert\n    else Min heap full\n        MinHeap-&gt;&gt;MinHeap: find largest value\n        Note over MinHeap: O(k) - Linear search\n\n        alt New value smaller\n            MinHeap-&gt;&gt;MinHeap: replace largest\n            Note over MinHeap: O(k) - Find and replace\n            MinHeap-&gt;&gt;Heapq: heapify()\n            Note over Heapq: O(k) - Restore heap property\n        end\n    end\n\n    alt Max heap not full\n        MaxHeap-&gt;&gt;Heapq: heappush(-value, index)\n        Note over Heapq: O(log k) - Heap insert (negated)\n    else Max heap full\n        MaxHeap-&gt;&gt;MaxHeap: find smallest original value\n        Note over MaxHeap: O(k) - Linear search\n\n        alt New value larger\n            MaxHeap-&gt;&gt;MaxHeap: replace smallest\n            Note over MaxHeap: O(k) - Find and replace\n            MaxHeap-&gt;&gt;Heapq: heapify()\n            Note over Heapq: O(k) - Restore heap property\n        end\n    end\n\n    Extreme-&gt;&gt;Extreme: get_extremes()\n    Note over Extreme: O(k log k) - Extract and sort&lt;br/&gt;Space: O(k) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#chunk-metadata-optimization","title":"Chunk Metadata Optimization","text":"<pre><code>sequenceDiagram\n    participant Accumulator as NumericAccumulator\n    participant Config as NumericConfig\n    participant Boundaries as _chunk_boundaries\n    participant Missing as _chunk_missing\n    participant Counter as _chunk_count\n\n    Note over Accumulator: Before Fix: O(num_chunks) unbounded growth&lt;br/&gt;After Fix: O(min(num_chunks, max_chunks)) bounded\n\n    Accumulator-&gt;&gt;Config: check enable_chunk_metadata\n    Note over Config: O(1) - Configuration check\n\n    alt Chunk metadata enabled\n        Accumulator-&gt;&gt;Counter: check _chunk_count &lt; max_chunks\n        Note over Counter: O(1) - Counter check\n\n        alt Under limit\n            Accumulator-&gt;&gt;Boundaries: append(cumulative_rows)\n            Note over Boundaries: O(1) - List append&lt;br/&gt;Space: O(chunk_count)\n\n            Accumulator-&gt;&gt;Missing: append(missing_count)\n            Note over Missing: O(1) - List append&lt;br/&gt;Space: O(chunk_count)\n\n            Accumulator-&gt;&gt;Counter: increment()\n            Note over Counter: O(1) - Counter increment\n        else Over limit\n            Accumulator-&gt;&gt;Config: disable chunk metadata\n            Note over Config: O(1) - Switch to summary mode\n\n            Accumulator-&gt;&gt;Boundaries: stop tracking\n            Note over Boundaries: O(1) - Stop appending&lt;br/&gt;Space: O(max_chunks) bounded\n        end\n    else Chunk metadata disabled\n        Accumulator-&gt;&gt;Accumulator: skip tracking\n        Note over Accumulator: O(1) - No memory usage&lt;br/&gt;Space: O(1) constant\n    end\n\n    Accumulator-&gt;&gt;Accumulator: finalize()\n    Note over Accumulator: O(chunk_count) - Process metadata&lt;br/&gt;Space: O(min(chunk_count, max_chunks)) bounded</code></pre>"},{"location":"sequence-diagrams-complexity/#memory-monitoring-integration","title":"Memory Monitoring Integration","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Tracemalloc\n    participant Psutil\n    participant Process\n    participant Profile\n    participant Accumulator\n\n    User-&gt;&gt;Tracemalloc: start()\n    Note over Tracemalloc: O(1) - Start memory tracking\n\n    User-&gt;&gt;Psutil: Process(os.getpid())\n    Note over Psutil: O(1) - Get process handle\n\n    User-&gt;&gt;Process: memory_info().rss\n    Note over Process: O(1) - Get initial memory\n\n    User-&gt;&gt;Profile: profile(data, config)\n    Note over Profile: O(n) - Process data\n\n    loop During processing\n        Profile-&gt;&gt;Accumulator: update(chunk)\n        Note over Accumulator: O(n) - Process chunk\n\n        User-&gt;&gt;Process: memory_info().rss\n        Note over Process: O(1) - Monitor memory\n\n        User-&gt;&gt;Tracemalloc: get_traced_memory()\n        Note over Tracemalloc: O(1) - Get traced memory\n    end\n\n    Profile-&gt;&gt;User: Report\n    Note over Profile: O(1) - Return results\n\n    User-&gt;&gt;Process: memory_info().rss\n    Note over Process: O(1) - Get final memory\n\n    User-&gt;&gt;Tracemalloc: get_traced_memory()\n    Note over Tracemalloc: O(1) - Get peak memory\n\n    User-&gt;&gt;Tracemalloc: stop()\n    Note over Tracemalloc: O(1) - Stop tracking</code></pre>"},{"location":"sequence-diagrams-complexity/#complexity-summary","title":"Complexity Summary","text":""},{"location":"sequence-diagrams-complexity/#time-complexity","title":"Time Complexity","text":"<ul> <li>Per Element: O(1) for basic operations, O(log k) for heap operations</li> <li>Per Chunk: O(n) where n is chunk size</li> <li>Total: O(N) where N is total dataset size</li> </ul>"},{"location":"sequence-diagrams-complexity/#space-complexity","title":"Space Complexity","text":"<ul> <li>KMV: O(k) bounded (was O(n) unbounded)</li> <li>ExtremeTracker: O(k) bounded (was O(k \u00d7 chunks) temporary)</li> <li>Chunk Metadata: O(min(num_chunks, max_chunks)) bounded (was O(num_chunks) unbounded)</li> <li>Total: O(k + s + c) where k=sketch_size, s=sample_size, c=max_chunks</li> </ul>"},{"location":"sequence-diagrams-complexity/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Before Fixes: O(n) growth for low-cardinality columns</li> <li>After Fixes: O(1) constant growth</li> <li>Memory per Row: &lt;1KB (typically &lt;0.1KB)</li> <li>Peak Memory: &lt;200MB for 1M rows</li> </ul> <p>The memory leak fixes successfully transform PySuricata from a memory-intensive system to a truly streaming system with bounded memory usage.</p>"},{"location":"usage/","title":"Basic Usage","text":""},{"location":"usage/#generating-an-html-report","title":"Generating an HTML Report","text":"<p>The simplest way to use PySuricata is to generate an HTML report from a DataFrame:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <p>Open <code>report.html</code> in any browser. The file is self-contained \u2014 no external assets needed.</p>"},{"location":"usage/#using-polars","title":"Using Polars","text":"<p>PySuricata works natively with polars DataFrames and LazyFrames. Install polars support with:</p> <pre><code>pip install pysuricata[polars]\n</code></pre> <p>Then use it the same way:</p> <pre><code>import polars as pl\nfrom pysuricata import profile\n\n# Eager DataFrame\ndf = pl.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n\n# LazyFrame \u2014 PySuricata collects it in chunks internally\nlf = pl.scan_csv(\"large_file.csv\")\nreport = profile(lf)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"usage/#streaming-large-datasets","title":"Streaming Large Datasets","text":"<p>For datasets that don't fit in memory, pass a generator yielding DataFrame chunks:</p> PandasPandas chunked readerPolars <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndef read_in_chunks():\n    for i in range(10):\n        yield pd.read_csv(f\"data/part-{i}.csv\")\n\nreport = profile(read_in_chunks())\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\n# pandas read_csv has a built-in chunksize parameter\nchunks = pd.read_csv(\"large_file.csv\", chunksize=200_000)\nreport = profile(chunks)\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\ndf = pl.read_parquet(\"large_file.parquet\")\n\n# Manually slice into chunks\nstep = 200_000\nchunks = (df.slice(i, min(step, df.height - i)) for i in range(0, df.height, step))\n\nreport = profile(chunks)\nreport.save_html(\"report.html\")\n</code></pre> <p>Each chunk is processed and discarded, so memory stays bounded regardless of total dataset size.</p>"},{"location":"usage/#getting-statistics-without-html","title":"Getting Statistics Without HTML","text":"<p>Use <code>summarize()</code> to get a dictionary of statistics without generating an HTML report:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\n# Dataset-level statistics\nprint(stats[\"dataset\"])\n# {'rows_est': 891, 'cols': 12, 'missing_cells_pct': 8.7, ...}\n\n# Per-column statistics\nprint(stats[\"columns\"][\"age\"])\n# {'mean': 29.7, 'std': 14.5, 'min': 0.42, 'max': 80.0, ...}\n</code></pre> <p>This is useful for CI/CD quality checks:</p> <pre><code>stats = summarize(df)\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n</code></pre>"},{"location":"usage/#saving-stats-as-json","title":"Saving Stats as JSON","text":"<pre><code>report = profile(df)\nreport.save_json(\"stats.json\")\n</code></pre>"},{"location":"usage/#reproducible-reports","title":"Reproducible Reports","text":"<p>Set <code>random_seed</code> to make histogram sampling deterministic across runs:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n# Same report every time with the same data\n</code></pre>"},{"location":"usage/#end-to-end-example","title":"End-to-End Example","text":"<p>A complete example covering all four column types:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndf = pd.DataFrame({\n    \"amount\": [1.0, 2.5, None, 4.0, 5.5],\n    \"country\": [\"US\", \"US\", \"DE\", None, \"FR\"],\n    \"ts\": pd.to_datetime([\"2021-01-01\", \"2021-01-02\", None, \"2021-01-04\", \"2021-01-05\"]),\n    \"flag\": [True, False, True, None, False],\n})\n\nconfig = ReportConfig()\nconfig.compute.random_seed = 0\n\nreport = profile(df, config=config)\nreport.save_html(\"report.html\")\n</code></pre> <p>This generates a report with:</p> <ul> <li>amount analyzed as numeric (mean, std, histogram, outliers)</li> <li>country analyzed as categorical (top values, distinct count, entropy)</li> <li>ts analyzed as datetime (range, day-of-week distribution)</li> <li>flag analyzed as boolean (true/false ratio, balance score)</li> </ul>"},{"location":"usage/#see-also","title":"See Also","text":"<ul> <li>Configuration Guide \u2014 All available options</li> <li>Advanced Features \u2014 Streaming from multiple sources, distributed processing</li> <li>Examples Gallery \u2014 More real-world use cases</li> </ul>"},{"location":"why-pysuricata/","title":"Why PySuricata?","text":"<p>PySuricata is a Python library for generating HTML data profiling reports from pandas or polars DataFrames. Its main design choice is a streaming architecture: data is processed in chunks, so memory usage stays bounded regardless of dataset size.</p> <p>This page explains the design decisions behind PySuricata and when it might be a good fit for your workflow.</p>"},{"location":"why-pysuricata/#streaming-architecture","title":"Streaming Architecture","text":"<p>Most profiling tools load the entire dataset into memory to compute statistics. PySuricata takes a different approach \u2014 it processes data one chunk at a time, updating lightweight accumulators as it goes.</p> <pre><code>from pysuricata import profile, ReportConfig\n\n# PySuricata processes data in chunks internally\nconfig = ReportConfig()\nconfig.compute.chunk_size = 200_000  # 200k rows per chunk (default)\n\nreport = profile(df, config=config)\n</code></pre> <p>This means:</p> <ul> <li>Memory is bounded by chunk size, not dataset size. A 10 GB dataset uses roughly the same memory as a 100 MB one.</li> <li>Each row is read exactly once \u2014 there's no second pass over the data.</li> <li>Accumulators are mergeable \u2014 statistics computed on separate chunks (or machines) can be combined exactly.</li> </ul> <p>You can also pass a generator of DataFrames for datasets that don't fit in memory at all:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndef read_in_parts():\n    for i in range(100):\n        yield pd.read_parquet(f\"data/part-{i}.parquet\")\n\n# Processes 100 files without loading them all at once\nreport = profile(read_in_parts())\nreport.save_html(\"large_dataset_report.html\")\n</code></pre>"},{"location":"why-pysuricata/#algorithms","title":"Algorithms","text":"<p>PySuricata uses well-known streaming algorithms from the academic literature. Here's what it computes and how:</p>"},{"location":"why-pysuricata/#exact-statistics-welford-pebay","title":"Exact Statistics \u2014 Welford &amp; P\u00e9bay","text":"<p>Mean, variance, skewness, and kurtosis are computed exactly using Welford's online algorithm, extended with P\u00e9bay's merge formulas for combining results across chunks.</p> <pre><code># Conceptually, this is what happens per value:\nn += 1\ndelta = value - mean\nmean += delta / n\nM2 += delta * (value - mean)\n# variance = M2 / (n - 1)\n</code></pre> <p>These formulas are numerically stable (they avoid the catastrophic cancellation that can happen with naive sum-of-squares approaches) and exactly mergeable (combining two partial results gives the same answer as processing all data at once).</p> <p>References:</p> <ul> <li>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics</li> <li>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report</li> </ul>"},{"location":"why-pysuricata/#approximate-statistics-sketches-and-sampling","title":"Approximate Statistics \u2014 Sketches and Sampling","text":"<p>Some statistics can't be computed exactly in a single pass with bounded memory. PySuricata uses probabilistic data structures with known error bounds:</p> Algorithm Purpose Space Error Bound KMV sketch Distinct count estimation O(k), default k=2048 \u03b5 \u2248 1/\u221ak (~2.2%) Misra-Gries Top-k frequent values O(k), default k=50 Finds all items with frequency &gt; n/k Reservoir sampling Uniform random sample O(s), default s=20,000 Exact probability k/n per item <p>These are standard algorithms with well-understood properties. The error bounds are theoretical guarantees, not empirical estimates.</p>"},{"location":"why-pysuricata/#pandas-and-polars-support","title":"Pandas and Polars Support","text":"<p>PySuricata works natively with both pandas and polars DataFrames. The same <code>profile()</code> function handles both:</p> PandasPolarsPolars LazyFrame <pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\ndf = pl.read_csv(\"data.csv\")\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre> <pre><code>import polars as pl\nfrom pysuricata import profile\n\nlf = pl.scan_csv(\"large_file.csv\")\nreport = profile(lf)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"why-pysuricata/#self-contained-reports","title":"Self-Contained Reports","text":"<p>PySuricata generates a single HTML file with everything inlined:</p> <ul> <li>CSS styles embedded in <code>&lt;style&gt;</code> tags</li> <li>JavaScript embedded in <code>&lt;script&gt;</code> tags</li> <li>Charts rendered as inline SVG (no image files)</li> <li>Logo encoded as base64</li> </ul> <p>The resulting file can be opened in any browser, shared via email, hosted on a static server, or committed to a repository. There are no external assets to manage.</p>"},{"location":"why-pysuricata/#configuration","title":"Configuration","text":"<p>All processing parameters are exposed through <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Processing\nconfig.compute.chunk_size = 250_000       # Rows per chunk\nconfig.compute.numeric_sample_size = 50_000  # Sample size for quantiles\nconfig.compute.random_seed = 42           # Deterministic sampling\n\n# Analysis\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5       # Min |r| to display\nconfig.compute.top_k_size = 100           # Top values to track\nconfig.compute.uniques_sketch_size = 4096 # KMV sketch size\n\n# Rendering\nconfig.render.title = \"My Report\"\nconfig.render.description = \"Custom **markdown** description\"\nconfig.render.include_sample = True\nconfig.render.sample_rows = 10\n\nreport = profile(df, config=config)\n</code></pre> <p>Setting <code>random_seed</code> makes reports reproducible \u2014 the same data with the same seed produces the same output.</p>"},{"location":"why-pysuricata/#what-pysuricata-analyzes","title":"What PySuricata Analyzes","text":"<p>PySuricata detects the type of each column and applies specialized analysis:</p> Column Type Statistics Visualization Numeric Mean, variance, skewness, kurtosis, quantiles, IQR/MAD/z-score outliers Histogram (SVG) Categorical Top-k values, distinct count, entropy, Gini impurity, string length stats Donut chart (SVG) DateTime Range, hour/day/month distributions, monotonicity coefficient Timeline (SVG) Boolean True/false ratios, entropy, balance score, imbalance ratio Balance bar (SVG) <p>Additionally, PySuricata computes:</p> <ul> <li>Streaming correlations (Pearson r) between numeric columns</li> <li>Missing value analysis per column and per chunk</li> <li>Duplicate row estimation using KMV hashing</li> </ul>"},{"location":"why-pysuricata/#use-cases","title":"Use Cases","text":""},{"location":"why-pysuricata/#data-quality-checks-in-pipelines","title":"Data Quality Checks in Pipelines","text":"<p>Use <code>summarize()</code> to get statistics as a dictionary, without generating HTML:</p> <pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\n# Assert data quality thresholds\nassert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\nassert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n\n# Access per-column statistics\nprint(f\"Mean age: {stats['columns']['age']['mean']:.1f}\")\nprint(f\"Distinct countries: {stats['columns']['country']['distinct']}\")\n</code></pre>"},{"location":"why-pysuricata/#profiling-large-datasets","title":"Profiling Large Datasets","text":"<p>When your data doesn't fit in memory, pass a generator:</p> <pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\nconfig.compute.chunk_size = 100_000\nconfig.compute.random_seed = 42\n\ndef read_chunks():\n    for chunk in pd.read_csv(\"large_file.csv\", chunksize=100_000):\n        yield chunk\n\nreport = profile(read_chunks(), config=config)\nreport.save_html(\"large_report.html\")\n</code></pre>"},{"location":"why-pysuricata/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Reports render inline in notebooks:</p> <pre><code>from pysuricata import profile\n\nreport = profile(df)\nreport  # Auto-displays inline\n\n# Or with custom height\nreport.display_in_notebook(height=\"800px\")\n</code></pre>"},{"location":"why-pysuricata/#learn-more","title":"Learn More","text":"<ul> <li>Quick Start \u2014 Generate your first report</li> <li>Configuration \u2014 All available options</li> <li>Statistical Methods \u2014 Algorithm details and formulas</li> <li>Architecture Diagrams \u2014 Visual overview of the processing pipeline</li> <li>API Reference \u2014 Function signatures and parameters</li> </ul>"},{"location":"algorithms/sampling/","title":"Reservoir Sampling","text":"<p>Maintain uniform random sample of fixed size \\(k\\) from stream of unknown length.</p>"},{"location":"algorithms/sampling/#algorithm-r-vitter","title":"Algorithm R (Vitter)","text":"<pre><code>class ReservoirSampler:\n    def __init__(self, k):\n        self.k = k\n        self.reservoir = []\n        self.n = 0\n\n    def add(self, item):\n        self.n += 1\n        if len(self.reservoir) &lt; self.k:\n            self.reservoir.append(item)\n        else:\n            j = random.randint(0, self.n - 1)\n            if j &lt; self.k:\n                self.reservoir[j] = item\n</code></pre>"},{"location":"algorithms/sampling/#guarantee","title":"Guarantee","text":"<p>Every element has exactly probability \\(k/n\\) of being in sample.</p>"},{"location":"algorithms/sampling/#proof-sketch","title":"Proof Sketch","text":"<p>Element \\(i\\) enters reservoir with probability \\(\\min(1, k/i)\\).</p> <p>It survives subsequent updates:</p> \\[ P(\\text{survive}) = \\prod_{j=i+1}^{n} \\left(1 - \\frac{1}{j}\\right) = \\frac{i}{n} \\] <p>Total probability:</p> \\[ P(\\text{in sample}) = \\frac{k}{i} \\cdot \\frac{i}{n} = \\frac{k}{n} \\]"},{"location":"algorithms/sampling/#complexity","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Time per element: O(1) amortized</li> <li>Uniform guarantee: Exact</li> </ul>"},{"location":"algorithms/sampling/#use-cases","title":"Use Cases","text":"<ul> <li>Quantile estimation (sort sample)</li> <li>Histogram construction</li> <li>Representative sampling</li> </ul>"},{"location":"algorithms/sampling/#see-also","title":"See Also","text":"<ul> <li>Sketch Algorithms - Other streaming algorithms</li> <li>Numeric Analysis - Using reservoir for quantiles</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"algorithms/sketches/","title":"Sketch Algorithms","text":"<p>Sketch algorithms (probabilistic data structures) enable approximate analytics on massive streams with bounded memory and mathematical error guarantees.</p>"},{"location":"algorithms/sketches/#overview","title":"Overview","text":"<p>Sketches trade perfect accuracy for: - Constant memory: Independent of dataset size - Fast updates: O(1) or O(log k) per element - Mergeability: Combine sketches from parallel streams - Mathematical guarantees: Provable error bounds</p>"},{"location":"algorithms/sketches/#k-minimum-values-kmv","title":"K-Minimum Values (KMV)","text":""},{"location":"algorithms/sketches/#purpose","title":"Purpose","text":"<p>Estimate distinct count (cardinality) of a set.</p>"},{"location":"algorithms/sketches/#algorithm","title":"Algorithm","text":"<ol> <li>Hash each element to [0,1]: \\(h(x) \\sim \\text{Uniform}(0,1)\\)</li> <li>Keep the \\(k\\) smallest hash values</li> <li>Estimate: \\(\\hat{d} = \\frac{k-1}{x_k}\\) where \\(x_k\\) is the \\(k\\)-th smallest hash</li> </ol>"},{"location":"algorithms/sketches/#error-bound","title":"Error Bound","text":"\\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Example: \\(k=2048\\) \u2192 ~2.2% error at 95% confidence</p>"},{"location":"algorithms/sketches/#implementation","title":"Implementation","text":"<pre><code>import heapq, hashlib\n\nclass KMV:\n    def __init__(self, k):\n        self.k = k\n        self.heap = []  # Max-heap of k smallest hashes\n\n    def add(self, value):\n        h = self._hash(value)\n        if len(self.heap) &lt; self.k:\n            heapq.heappush(self.heap, -h)  # Negative for max-heap\n        elif h &lt; -self.heap[0]:\n            heapq.heapreplace(self.heap, -h)\n\n    def estimate(self):\n        if len(self.heap) == 0:\n            return 0\n        if len(self.heap) &lt; self.k:\n            return len(self.heap)\n        kth_min = -self.heap[0]\n        return (self.k - 1) / kth_min if kth_min &gt; 0 else float('inf')\n\n    def _hash(self, value):\n        return int(hashlib.md5(str(value).encode()).hexdigest(), 16) / (2**128)\n</code></pre>"},{"location":"algorithms/sketches/#mergeability","title":"Mergeability","text":"<p>Union of two KMV sketches: merge heaps and keep k smallest.</p> <pre><code>def merge(kmv1, kmv2):\n    combined = KMV(kmv1.k)\n    for h in kmv1.heap + kmv2.heap:\n        combined.heap.append(h)\n    combined.heap = heapq.nlargest(combined.k, combined.heap)\n    heapq.heapify(combined.heap)\n    return combined\n</code></pre>"},{"location":"algorithms/sketches/#misra-gries-algorithm","title":"Misra-Gries Algorithm","text":""},{"location":"algorithms/sketches/#purpose_1","title":"Purpose","text":"<p>Find top-k most frequent items (heavy hitters).</p>"},{"location":"algorithms/sketches/#algorithm_1","title":"Algorithm","text":"<ol> <li>Maintain dictionary of \u2264k items with counts</li> <li>For each new item:</li> <li>If in dictionary: increment count</li> <li>Else if dictionary not full: add with count 1</li> <li>Else: decrement all counts, remove zeros</li> <li>Output: items remaining in dictionary</li> </ol>"},{"location":"algorithms/sketches/#guarantee","title":"Guarantee","text":"<p>For any item with true frequency \\(f &gt; n/k\\): - Guaranteed to appear in output - Estimated frequency within \\(\\pm n/k\\) of true frequency</p>"},{"location":"algorithms/sketches/#implementation_1","title":"Implementation","text":"<pre><code>class MisraGries:\n    def __init__(self, k):\n        self.k = k\n        self.counts = {}\n\n    def add(self, item):\n        if item in self.counts:\n            self.counts[item] += 1\n        elif len(self.counts) &lt; self.k:\n            self.counts[item] = 1\n        else:\n            # Decrement all counts\n            to_remove = []\n            for key in self.counts:\n                self.counts[key] -= 1\n                if self.counts[key] == 0:\n                    to_remove.append(key)\n            for key in to_remove:\n                del self.counts[key]\n\n    def top_k(self):\n        return sorted(self.counts.items(), key=lambda x: -x[1])\n</code></pre>"},{"location":"algorithms/sketches/#complexity","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Update: O(k) worst-case, O(1) amortized</li> <li>Output: O(k log k) for sorting</li> </ul>"},{"location":"algorithms/sketches/#mergeability_1","title":"Mergeability","text":"<p>Sum counts from multiple Misra-Gries structures.</p>"},{"location":"algorithms/sketches/#hyperloglog-hll","title":"HyperLogLog (HLL)","text":""},{"location":"algorithms/sketches/#purpose_2","title":"Purpose","text":"<p>Estimate distinct count with very low memory.</p>"},{"location":"algorithms/sketches/#key-idea","title":"Key Idea","text":"<p>Count leading zeros in binary representation of hashes.</p> <p>Intuition: If max leading zeros = \\(m\\), roughly \\(2^m\\) distinct elements seen.</p>"},{"location":"algorithms/sketches/#algorithm_2","title":"Algorithm","text":"<ol> <li>Hash elements to binary strings</li> <li>Split hash into \\(2^b\\) buckets (first \\(b\\) bits)</li> <li>Track max leading zeros per bucket</li> <li>Combine with harmonic mean</li> </ol>"},{"location":"algorithms/sketches/#estimator","title":"Estimator","text":"\\[ \\hat{d} = \\alpha_m \\cdot m^2 \\cdot \\left(\\sum_{j=1}^{m} 2^{-M_j}\\right)^{-1} \\] <p>where: - \\(m = 2^b\\) = number of buckets - \\(M_j\\) = max leading zeros in bucket \\(j\\) - \\(\\alpha_m\\) = bias correction constant</p>"},{"location":"algorithms/sketches/#error","title":"Error","text":"\\[ \\text{Relative standard error} \\approx \\frac{1.04}{\\sqrt{m}} \\] <p>Example: \\(m=1024\\) (10 KB) \u2192 ~3% error</p>"},{"location":"algorithms/sketches/#properties","title":"Properties","text":"<ul> <li>Space: \\(O(m \\log \\log n)\\) bits</li> <li>Mergeable: Element-wise max of bucket values</li> <li>Production-ready: Used in Redis, BigQuery</li> </ul> <p>Not implemented in current version</p> <p>PySuricata uses KMV instead of HLL. HLL may be added in future for even lower memory.</p>"},{"location":"algorithms/sketches/#reservoir-sampling","title":"Reservoir Sampling","text":""},{"location":"algorithms/sketches/#purpose_3","title":"Purpose","text":"<p>Maintain uniform random sample of fixed size \\(k\\) from stream.</p>"},{"location":"algorithms/sketches/#algorithm-algorithm-r","title":"Algorithm (Algorithm R)","text":"<pre><code>import random\n\nclass ReservoirSampler:\n    def __init__(self, k):\n        self.k = k\n        self.reservoir = []\n        self.n = 0\n\n    def add(self, item):\n        self.n += 1\n        if len(self.reservoir) &lt; self.k:\n            self.reservoir.append(item)\n        else:\n            j = random.randint(0, self.n - 1)\n            if j &lt; self.k:\n                self.reservoir[j] = item\n\n    def get_sample(self):\n        return self.reservoir.copy()\n</code></pre>"},{"location":"algorithms/sketches/#guarantee_1","title":"Guarantee","text":"<p>Every element has exactly probability \\(k/n\\) of being in the sample.</p> <p>Proof sketch: - Element \\(i\\) enters reservoir with probability \\(\\min(1, k/i)\\) - Survives subsequent updates with probability \\(\\prod_{j=i+1}^{n} (1 - 1/j)\\) - Total: \\(k/n\\)</p>"},{"location":"algorithms/sketches/#complexity_1","title":"Complexity","text":"<ul> <li>Space: O(k)</li> <li>Update: O(1)</li> <li>Uniform guarantee: Exact</li> </ul>"},{"location":"algorithms/sketches/#use-cases","title":"Use Cases","text":"<ul> <li>Quantile estimation (sort sample)</li> <li>Histogram construction</li> <li>Outlier detection</li> </ul>"},{"location":"algorithms/sketches/#bloom-filters","title":"Bloom Filters","text":""},{"location":"algorithms/sketches/#purpose_4","title":"Purpose","text":"<p>Test set membership with false positives, no false negatives.</p>"},{"location":"algorithms/sketches/#algorithm_3","title":"Algorithm","text":"<ol> <li>Initialize bit array of size \\(m\\) to 0</li> <li>Use \\(k\\) hash functions</li> <li>Add: set bits at \\(h_1(x), h_2(x), \\ldots, h_k(x)\\) to 1</li> <li>Query: check if all \\(k\\) bits are 1</li> </ol>"},{"location":"algorithms/sketches/#false-positive-rate","title":"False Positive Rate","text":"\\[ P_{\\text{fp}} \\approx \\left(1 - e^{-kn/m}\\right)^k \\]"},{"location":"algorithms/sketches/#optimal-parameters","title":"Optimal Parameters","text":"<p>For desired \\(P_{\\text{fp}}\\) and \\(n\\) elements:</p> \\[ \\begin{aligned} m &amp;= -\\frac{n \\ln P_{\\text{fp}}}{(\\ln 2)^2} \\\\ k &amp;= \\frac{m}{n} \\ln 2 \\end{aligned} \\] <p>Example: \\(n=10^6\\), \\(P_{\\text{fp}}=0.01\\) \u2192 \\(m \\approx 9.6\\) Mb, \\(k=7\\)</p> <p>Not implemented in current version</p> <p>Bloom filters are not currently used in PySuricata but may be added for duplicate detection optimization.</p>"},{"location":"algorithms/sketches/#count-min-sketch","title":"Count-Min Sketch","text":""},{"location":"algorithms/sketches/#purpose_5","title":"Purpose","text":"<p>Estimate frequencies of items in stream.</p>"},{"location":"algorithms/sketches/#algorithm_4","title":"Algorithm","text":"<ol> <li>Initialize \\(d \\times w\\) matrix of counters to 0</li> <li>Use \\(d\\) hash functions mapping to \\([0, w)\\)</li> <li>Add item: increment counters at \\(M[i, h_i(x)]\\) for \\(i=1,\\ldots,d\\)</li> <li>Estimate frequency: \\(\\hat{f}(x) = \\min_i M[i, h_i(x)]\\)</li> </ol>"},{"location":"algorithms/sketches/#error-bound_1","title":"Error Bound","text":"<p>With probability \\(1-\\delta\\):</p> \\[ \\hat{f}(x) \\le f(x) + \\epsilon n \\] <p>where \\(w = \\lceil e / \\epsilon \\rceil\\) and \\(d = \\lceil \\ln(1/\\delta) \\rceil\\).</p>"},{"location":"algorithms/sketches/#comparison-to-misra-gries","title":"Comparison to Misra-Gries","text":"<ul> <li>Count-Min: Point queries, any item</li> <li>Misra-Gries: Top-k queries, only frequent items</li> </ul> <p>Not implemented in current version</p> <p>PySuricata uses Misra-Gries for top-k. Count-Min Sketch may be added for full frequency queries.</p>"},{"location":"algorithms/sketches/#choosing-algorithms","title":"Choosing Algorithms","text":"Need Algorithm Memory Error Distinct count KMV O(k) \\(1/\\sqrt{k}\\) Distinct count (min memory) HyperLogLog O(m log log n) \\(1/\\sqrt{m}\\) Top-k items Misra-Gries O(k) \\(n/k\\) frequency Top-k items (point queries) Count-Min O(dw) \\(\\epsilon n\\) Uniform sample Reservoir O(k) Exact Membership test Bloom filter O(m) \\(P_{\\text{fp}}\\)"},{"location":"algorithms/sketches/#implementation-in-pysuricata","title":"Implementation in PySuricata","text":""},{"location":"algorithms/sketches/#numericaccumulator","title":"NumericAccumulator","text":"<pre><code>self._uniques = KMV(config.uniques_sketch_size)  # Distinct count\nself._sample = ReservoirSampler(config.sample_size)  # Quantiles\n</code></pre>"},{"location":"algorithms/sketches/#categoricalaccumulator","title":"CategoricalAccumulator","text":"<pre><code>self._topk = MisraGries(config.top_k_size)  # Top values\nself._uniques = KMV(config.uniques_sketch_size)  # Distinct count\n</code></pre>"},{"location":"algorithms/sketches/#references","title":"References","text":"<ol> <li> <p>Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p> </li> <li> <p>Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p> </li> <li> <p>Flajolet, P. et al. (2007), \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\", DMTCS, AH: 137\u2013156.</p> </li> <li> <p>Vitter, J.S. (1985), \"Random Sampling with a Reservoir\", ACM TOMS, 11(1): 37\u201357.</p> </li> <li> <p>Bloom, B.H. (1970), \"Space/time trade-offs in hash coding with allowable errors\", CACM, 13(7): 422\u2013426.</p> </li> <li> <p>Cormode, G., Muthukrishnan, S. (2005), \"An Improved Data Stream Summary: The Count-Min Sketch and its Applications\", Journal of Algorithms, 55(1): 58\u201375.</p> </li> </ol>"},{"location":"algorithms/sketches/#see-also","title":"See Also","text":"<ul> <li>Streaming Algorithms - Welford/P\u00e9bay moments</li> <li>Numeric Analysis - Using KMV and reservoir</li> <li>Categorical Analysis - Using Misra-Gries</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"algorithms/streaming/","title":"Streaming Statistics Algorithms","text":"<p>Deep dive into the streaming algorithms that power PySuricata's memory-efficient statistics computation.</p>"},{"location":"algorithms/streaming/#overview","title":"Overview","text":"<p>Streaming algorithms compute statistics in single-pass, constant-memory mode, enabling analysis of datasets larger than RAM.</p>"},{"location":"algorithms/streaming/#key-algorithms","title":"Key Algorithms","text":"<ul> <li>Welford's algorithm: Online mean and variance</li> <li>P\u00e9bay's formulas: Parallel merging of moments</li> <li>Higher moments: Skewness and kurtosis extension</li> <li>Numerical stability: Avoiding catastrophic cancellation</li> </ul>"},{"location":"algorithms/streaming/#welfords-online-algorithm","title":"Welford's Online Algorithm","text":""},{"location":"algorithms/streaming/#the-problem","title":"The Problem","text":"<p>Naive variance formula:</p> \\[ s^2 = \\frac{1}{n-1}\\left(\\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}\\right) \\] <p>Issues: - Requires two passes (one for \\(\\sum x_i\\), one for \\(\\sum x_i^2\\)) - Catastrophic cancellation if \\(\\sum x_i^2 \\approx (\\sum x_i)^2 / n\\) - Poor numerical stability</p>"},{"location":"algorithms/streaming/#welfords-solution","title":"Welford's Solution","text":"<p>State variables: - \\(n\\): count - \\(\\mu\\): running mean - \\(M_2\\): sum of squared deviations from current mean</p> <p>Update formulas:</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\mu &amp;\\leftarrow \\mu + \\frac{\\delta}{n} \\\\ \\delta_2 &amp;= x - \\mu_{\\text{new}} \\\\ M_2 &amp;\\leftarrow M_2 + \\delta \\cdot \\delta_2 \\end{aligned} \\] <p>Finalize:</p> \\[ \\text{variance} = \\frac{M_2}{n-1} \\]"},{"location":"algorithms/streaming/#derivation","title":"Derivation","text":"<p>Starting from the definition:</p> \\[ M_2^{(n)} = \\sum_{i=1}^{n} (x_i - \\mu^{(n)})^2 \\] <p>After adding \\(x_{n+1}\\):</p> \\[ M_2^{(n+1)} = \\sum_{i=1}^{n+1} (x_i - \\mu^{(n+1)})^2 \\] <p>Expand using the mean update:</p> \\[ \\mu^{(n+1)} = \\mu^{(n)} + \\frac{x_{n+1} - \\mu^{(n)}}{n+1} = \\mu^{(n)} + \\frac{\\delta}{n+1} \\] <p>After algebraic manipulation (see Welford 1962):</p> \\[ M_2^{(n+1)} = M_2^{(n)} + \\delta \\cdot (x_{n+1} - \\mu^{(n+1)}) \\] <p>Key insight: Update uses both old and new mean, providing numerical stability.</p>"},{"location":"algorithms/streaming/#pseudocode","title":"Pseudocode","text":"<pre><code>def welford_update(n, mean, M2, x):\n    \"\"\"Update running moments with new value x\"\"\"\n    n_new = n + 1\n    delta = x - mean\n    mean_new = mean + delta / n_new\n    delta2 = x - mean_new\n    M2_new = M2 + delta * delta2\n    return n_new, mean_new, M2_new\n\ndef welford_finalize(n, mean, M2):\n    \"\"\"Compute final statistics\"\"\"\n    if n &lt; 2:\n        return mean, None\n    variance = M2 / (n - 1)\n    return mean, variance\n</code></pre>"},{"location":"algorithms/streaming/#properties","title":"Properties","text":"<ol> <li>Single-pass: Only one scan through data</li> <li>Constant memory: O(1) space (3 numbers)</li> <li>Numerically stable: No catastrophic cancellation</li> <li>Exact: Same result as two-pass (up to FP rounding)</li> <li>Online: Can process streaming data</li> </ol>"},{"location":"algorithms/streaming/#pebays-parallel-merge","title":"P\u00e9bay's Parallel Merge","text":""},{"location":"algorithms/streaming/#the-problem_1","title":"The Problem","text":"<p>How to combine partial results from multiple chunks/threads?</p> <p>Given: - State A: \\((n_a, \\mu_a, M_{2a})\\) - State B: \\((n_b, \\mu_b, M_{2b})\\)</p> <p>Want: Combined state \\((n, \\mu, M_2)\\) equivalent to processing all data together.</p>"},{"location":"algorithms/streaming/#pebays-solution","title":"P\u00e9bay's Solution","text":"<p>Combined state:</p> \\[ \\begin{aligned} n &amp;= n_a + n_b \\\\ \\delta &amp;= \\mu_b - \\mu_a \\\\ \\mu &amp;= \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\\\ M_2 &amp;= M_{2a} + M_{2b} + \\delta^2 \\cdot \\frac{n_a n_b}{n} \\end{aligned} \\]"},{"location":"algorithms/streaming/#derivation_1","title":"Derivation","text":"<p>The combined mean is the weighted average:</p> \\[ \\mu = \\frac{n_a \\mu_a + n_b \\mu_b}{n_a + n_b} = \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\] <p>For \\(M_2\\), use the identity:</p> \\[ M_2 = \\sum (x_i - \\mu)^2 = \\sum (x_i - \\mu_a)^2 - n(\\mu - \\mu_a)^2 \\] <p>Applying to both groups and summing:</p> \\[ \\begin{aligned} M_2 &amp;= M_{2a} + n_a(\\mu_a - \\mu)^2 + M_{2b} + n_b(\\mu_b - \\mu)^2 \\\\ &amp;= M_{2a} + M_{2b} + n_a\\left(-\\delta \\frac{n_b}{n}\\right)^2 + n_b\\left(\\delta \\frac{n_a}{n}\\right)^2 \\\\ &amp;= M_{2a} + M_{2b} + \\delta^2 \\frac{n_a n_b (n_b + n_a)}{n^2} \\\\ &amp;= M_{2a} + M_{2b} + \\delta^2 \\frac{n_a n_b}{n} \\end{aligned} \\]"},{"location":"algorithms/streaming/#pseudocode_1","title":"Pseudocode","text":"<pre><code>def pebay_merge(n_a, mean_a, M2_a, n_b, mean_b, M2_b):\n    \"\"\"Merge two partial states\"\"\"\n    n = n_a + n_b\n    if n == 0:\n        return 0, 0.0, 0.0\n\n    delta = mean_b - mean_a\n    mean = mean_a + delta * n_b / n\n    M2 = M2_a + M2_b + delta**2 * n_a * n_b / n\n\n    return n, mean, M2\n</code></pre>"},{"location":"algorithms/streaming/#properties_1","title":"Properties","text":"<ol> <li>Associative: Order of merging doesn't matter</li> <li>Commutative: A \u222a B = B \u222a A</li> <li>Exact: Same result as single-pass over concatenated data</li> <li>Parallel: Enables multi-threading, distributed computation</li> <li>Fast: O(1) time to merge two states</li> </ol>"},{"location":"algorithms/streaming/#higher-moments-extension","title":"Higher Moments Extension","text":""},{"location":"algorithms/streaming/#third-and-fourth-moments","title":"Third and Fourth Moments","text":"<p>State variables: - \\(n\\), \\(\\mu\\), \\(M_2\\), \\(M_3\\), \\(M_4\\)</p> <p>Where: - \\(M_3 = \\sum (x_i - \\mu)^3\\) - \\(M_4 = \\sum (x_i - \\mu)^4\\)</p>"},{"location":"algorithms/streaming/#online-update","title":"Online Update","text":"<pre><code>def moments_update(n, mean, M2, M3, M4, x):\n    \"\"\"Update all four moments\"\"\"\n    n_new = n + 1\n    delta = x - mean\n    delta_n = delta / n_new\n    delta_n2 = delta_n * delta_n\n    term1 = delta * delta_n * n\n\n    mean_new = mean + delta_n\n    M4_new = M4 + term1 * delta_n2 * (n_new*n_new - 3*n_new + 3) + 6*delta_n2*M2 - 4*delta_n*M3\n    M3_new = M3 + term1 * delta_n * (n_new - 2) - 3*delta_n*M2\n    M2_new = M2 + term1\n\n    return n_new, mean_new, M2_new, M3_new, M4_new\n</code></pre>"},{"location":"algorithms/streaming/#pebay-merge-for-higher-moments","title":"P\u00e9bay Merge for Higher Moments","text":"<pre><code>def pebay_merge_moments(n_a, mean_a, M2_a, M3_a, M4_a,\n                        n_b, mean_b, M2_b, M3_b, M4_b):\n    \"\"\"Merge higher moments\"\"\"\n    n = n_a + n_b\n    if n == 0:\n        return 0, 0.0, 0.0, 0.0, 0.0\n\n    delta = mean_b - mean_a\n    delta2 = delta * delta\n    delta3 = delta2 * delta\n    delta4 = delta3 * delta\n\n    mean = mean_a + delta * n_b / n\n\n    M2 = M2_a + M2_b + delta2 * n_a * n_b / n\n\n    M3 = M3_a + M3_b + \\\n         delta3 * n_a * n_b * (n_a - n_b) / (n * n) + \\\n         3 * delta * (n_a * M2_b - n_b * M2_a) / n\n\n    M4 = M4_a + M4_b + \\\n         delta4 * n_a * n_b * (n_a*n_a - n_a*n_b + n_b*n_b) / (n * n * n) + \\\n         6 * delta2 * (n_a*n_a * M2_b + n_b*n_b * M2_a) / (n * n) + \\\n         4 * delta * (n_a * M3_b - n_b * M3_a) / n\n\n    return n, mean, M2, M3, M4\n</code></pre>"},{"location":"algorithms/streaming/#computing-skewness-and-kurtosis","title":"Computing Skewness and Kurtosis","text":"<pre><code>def compute_shape(n, M2, M3, M4):\n    \"\"\"Compute skewness and excess kurtosis\"\"\"\n    if n &lt; 3:\n        return None, None\n\n    variance = M2 / (n - 1)\n    if variance == 0:\n        return None, None\n\n    # Skewness (g1)\n    g1 = (n / ((n-1) * (n-2))) * (M3 / n) / (variance ** 1.5)\n\n    if n &lt; 4:\n        return g1, None\n\n    # Excess kurtosis (g2)\n    g2 = ((n * (n+1)) / ((n-1) * (n-2) * (n-3))) * (M4 / n) / (variance ** 2) - \\\n         (3 * (n-1) ** 2) / ((n-2) * (n-3))\n\n    return g1, g2\n</code></pre>"},{"location":"algorithms/streaming/#numerical-stability-analysis","title":"Numerical Stability Analysis","text":""},{"location":"algorithms/streaming/#why-naive-formula-fails","title":"Why Naive Formula Fails","text":"<p>Consider \\(x_i \\approx 10^9 + \\epsilon_i\\) where \\(|\\epsilon_i| \\ll 10^9\\).</p> <p>Naive formula:</p> \\[ \\sum x_i^2 \\approx n \\cdot 10^{18}, \\quad \\left(\\sum x_i\\right)^2 / n \\approx n \\cdot 10^{18} \\] <p>Subtraction loses precision (catastrophic cancellation).</p> <p>Welford's formula:</p> <p>Works with deviations \\(x_i - \\mu\\), which are \\(O(\\epsilon)\\), avoiding large intermediate values.</p>"},{"location":"algorithms/streaming/#condition-number","title":"Condition Number","text":"<p>For variance computation, Welford's algorithm has condition number \\(\\kappa \\approx 1\\), while naive formula has \\(\\kappa \\approx \\frac{\\bar{x}^2}{\\sigma^2}\\) (can be huge).</p>"},{"location":"algorithms/streaming/#parallelization","title":"Parallelization","text":""},{"location":"algorithms/streaming/#mapreduce-pattern","title":"MapReduce Pattern","text":"<pre><code># Map phase: compute partial moments per chunk\ndef map_chunk(chunk):\n    n, mean, M2, M3, M4 = 0, 0.0, 0.0, 0.0, 0.0\n    for x in chunk:\n        n, mean, M2, M3, M4 = moments_update(n, mean, M2, M3, M4, x)\n    return n, mean, M2, M3, M4\n\n# Reduce phase: merge all partial results\ndef reduce_moments(states):\n    result = (0, 0.0, 0.0, 0.0, 0.0)\n    for state in states:\n        result = pebay_merge_moments(*result, *state)\n    return result\n\n# Usage\npartial_states = [map_chunk(chunk) for chunk in chunks]\nfinal_state = reduce_moments(partial_states)\n</code></pre>"},{"location":"algorithms/streaming/#multi-threading","title":"Multi-threading","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef parallel_moments(data, n_threads=4):\n    chunks = np.array_split(data, n_threads)\n\n    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n        states = list(executor.map(map_chunk, chunks))\n\n    return reduce_moments(states)\n</code></pre>"},{"location":"algorithms/streaming/#implementation-in-pysuricata","title":"Implementation in PySuricata","text":""},{"location":"algorithms/streaming/#streamingmoments-class","title":"StreamingMoments Class","text":"<pre><code>class StreamingMoments:\n    def __init__(self):\n        self.n = 0\n        self.mean = 0.0\n        self.M2 = 0.0\n        self.M3 = 0.0\n        self.M4 = 0.0\n\n    def update(self, values: np.ndarray):\n        \"\"\"Update with array of values\"\"\"\n        for x in values:\n            if not np.isfinite(x):\n                continue\n            self.n, self.mean, self.M2, self.M3, self.M4 = \\\n                moments_update(self.n, self.mean, self.M2, self.M3, self.M4, x)\n\n    def merge(self, other: 'StreamingMoments'):\n        \"\"\"Merge with another moments object\"\"\"\n        self.n, self.mean, self.M2, self.M3, self.M4 = \\\n            pebay_merge_moments(\n                self.n, self.mean, self.M2, self.M3, self.M4,\n                other.n, other.mean, other.M2, other.M3, other.M4\n            )\n\n    def finalize(self):\n        \"\"\"Compute final statistics\"\"\"\n        if self.n &lt; 2:\n            return {\"mean\": self.mean, \"variance\": None, \"skewness\": None, \"kurtosis\": None}\n\n        variance = self.M2 / (self.n - 1)\n        std = math.sqrt(variance)\n        skewness, kurtosis = compute_shape(self.n, self.M2, self.M3, self.M4)\n\n        return {\n            \"count\": self.n,\n            \"mean\": self.mean,\n            \"variance\": variance,\n            \"std\": std,\n            \"skewness\": skewness,\n            \"kurtosis\": kurtosis\n        }\n</code></pre>"},{"location":"algorithms/streaming/#validation","title":"Validation","text":""},{"location":"algorithms/streaming/#test-properties","title":"Test Properties","text":"<pre><code>def test_welford_equivalence():\n    \"\"\"Verify Welford = two-pass\"\"\"\n    data = np.random.randn(10000)\n\n    # Welford\n    n, mean, M2 = 0, 0.0, 0.0\n    for x in data:\n        n, mean, M2 = welford_update(n, mean, M2, x)\n    var_welford = M2 / (n - 1)\n\n    # Two-pass\n    mean_twopass = np.mean(data)\n    var_twopass = np.var(data, ddof=1)\n\n    assert np.isclose(mean, mean_twopass)\n    assert np.isclose(var_welford, var_twopass)\n\ndef test_pebay_merge():\n    \"\"\"Verify merge = concatenate\"\"\"\n    data_a = np.random.randn(5000)\n    data_b = np.random.randn(3000)\n\n    # Separate\n    state_a = compute_moments(data_a)\n    state_b = compute_moments(data_b)\n    merged = pebay_merge_moments(*state_a, *state_b)\n\n    # Combined\n    data_combined = np.concatenate([data_a, data_b])\n    combined = compute_moments(data_combined)\n\n    assert np.allclose(merged, combined)\n</code></pre>"},{"location":"algorithms/streaming/#references","title":"References","text":"<ol> <li> <p>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p> </li> <li> <p>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212.</p> </li> <li> <p>Chan, T.F., Golub, G.H., LeVeque, R.J. (1983), \"Algorithms for Computing the Sample Variance: Analysis and Recommendations\", The American Statistician, 37(3): 242\u2013247.</p> </li> <li> <p>West, D.H.D. (1979), \"Updating Mean and Variance Estimates: An Improved Method\", Communications of the ACM, 22(9): 532\u2013535.</p> </li> <li> <p>Wikipedia: Algorithms for calculating variance - Link</p> </li> </ol>"},{"location":"algorithms/streaming/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Application of these algorithms</li> <li>Sketch Algorithms - Other streaming algorithms</li> <li>Performance Tips - Optimization strategies</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/correlations/","title":"Correlation Analysis","text":"<p>PySuricata computes pairwise correlations between numeric columns using streaming algorithms that operate in bounded memory.</p>"},{"location":"analytics/correlations/#overview","title":"Overview","text":"<p>Correlation analysis reveals linear relationships between numeric variables, helping identify: - Redundant features (highly correlated) - Related measurements (positively/negatively correlated) - Independent variables (near-zero correlation)</p>"},{"location":"analytics/correlations/#key-features","title":"Key Features","text":"<ul> <li>Streaming computation: O(p\u00b2) space for p numeric columns</li> <li>Single-pass algorithm: No need to store full data</li> <li>Exact Pearson correlation: Not approximate</li> <li>Configurable threshold: Only report significant correlations</li> <li>Per-column top-k: Show most correlated pairs</li> </ul>"},{"location":"analytics/correlations/#pearson-correlation-coefficient","title":"Pearson Correlation Coefficient","text":""},{"location":"analytics/correlations/#definition","title":"Definition","text":"<p>For two numeric variables X and Y with observations \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\):</p> \\[ r_{XY} = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\] <p>where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means.</p>"},{"location":"analytics/correlations/#alternative-formula","title":"Alternative Formula","text":"\\[ r_{XY} = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n\\sum y_i^2 - (\\sum y_i)^2}} \\] <p>This form enables streaming computation by maintaining sufficient statistics.</p>"},{"location":"analytics/correlations/#properties","title":"Properties","text":"<ul> <li>Range: \\(r \\in [-1, 1]\\)</li> <li>Interpretation:</li> <li>\\(r = 1\\): perfect positive linear relationship</li> <li>\\(r = -1\\): perfect negative linear relationship</li> <li>\\(r = 0\\): no linear relationship</li> <li>\\(0 &lt; r &lt; 1\\): positive correlation</li> <li>\\(-1 &lt; r &lt; 0\\): negative correlation</li> </ul>"},{"location":"analytics/correlations/#strength-guidelines","title":"Strength Guidelines","text":"<p>| \\(|r|\\) Range | Strength | |---------------|----------| | 0.0 - 0.2 | Very weak | | 0.2 - 0.4 | Weak | | 0.4 - 0.6 | Moderate | | 0.6 - 0.8 | Strong | | 0.8 - 1.0 | Very strong |</p>"},{"location":"analytics/correlations/#streaming-algorithm","title":"Streaming Algorithm","text":""},{"location":"analytics/correlations/#sufficient-statistics","title":"Sufficient Statistics","text":"<p>To compute \\(r_{XY}\\) without storing all data, maintain:</p> \\[ \\begin{aligned} n &amp;= \\text{count of pairs} \\\\ S_x &amp;= \\sum x_i \\\\ S_y &amp;= \\sum y_i \\\\ S_{xx} &amp;= \\sum x_i^2 \\\\ S_{yy} &amp;= \\sum y_i^2 \\\\ S_{xy} &amp;= \\sum x_i y_i \\end{aligned} \\]"},{"location":"analytics/correlations/#update-step","title":"Update Step","text":"<p>For each new pair \\((x, y)\\):</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ S_x &amp;\\leftarrow S_x + x \\\\ S_y &amp;\\leftarrow S_y + y \\\\ S_{xx} &amp;\\leftarrow S_{xx} + x^2 \\\\ S_{yy} &amp;\\leftarrow S_{yy} + y^2 \\\\ S_{xy} &amp;\\leftarrow S_{xy} + xy \\end{aligned} \\]"},{"location":"analytics/correlations/#finalize","title":"Finalize","text":"<p>Compute correlation:</p> \\[ r = \\frac{nS_{xy} - S_x S_y}{\\sqrt{nS_{xx} - S_x^2} \\sqrt{nS_{yy} - S_y^2}} \\]"},{"location":"analytics/correlations/#missing-values","title":"Missing Values","text":"<p>Only pairs with both values present are included:</p> <pre><code>mask = ~(isnan(x) | isnan(y) | isinf(x) | isinf(y))\nx_valid = x[mask]\ny_valid = y[mask]\n# Update with valid pairs only\n</code></pre>"},{"location":"analytics/correlations/#statistical-significance","title":"Statistical Significance","text":""},{"location":"analytics/correlations/#t-test-for-correlation","title":"t-test for Correlation","text":"<p>Test \\(H_0: \\rho = 0\\) (no correlation in population).</p> <p>Test statistic:</p> \\[ t = r \\sqrt{\\frac{n-2}{1-r^2}} \\] <p>Under \\(H_0\\), \\(t \\sim t_{n-2}\\) (Student's t-distribution with \\(n-2\\) degrees of freedom).</p> <p>P-value:</p> \\[ p = 2 \\cdot P(T_{n-2} &gt; |t|) \\] <p>Reject \\(H_0\\) if \\(p &lt; \\alpha\\) (e.g., \\(\\alpha = 0.05\\)).</p> <p>Not implemented in current version</p> <p>Significance tests are planned for future release. Current version reports raw correlations.</p>"},{"location":"analytics/correlations/#multiple-testing-correction","title":"Multiple Testing Correction","text":"<p>When testing \\(m = \\binom{p}{2}\\) pairs, use Bonferroni correction:</p> \\[ \\alpha_{\\text{adj}} = \\frac{\\alpha}{m} \\] <p>Or False Discovery Rate (FDR) control via Benjamini-Hochberg procedure.</p> <p>Example: 50 columns \u2192 1,225 pairs - Bonferroni: \\(\\alpha_{\\text{adj}} = 0.05/1225 \\approx 0.00004\\) - Very conservative</p>"},{"location":"analytics/correlations/#implementation","title":"Implementation","text":""},{"location":"analytics/correlations/#streamingcorr-class","title":"StreamingCorr Class","text":"<pre><code>class StreamingCorr:\n    def __init__(self, columns: List[str]):\n        self.cols = columns\n        self.pairs = {}  # (col1, col2) -&gt; {n, sx, sy, sxx, syy, sxy}\n\n    def update(self, df: pd.DataFrame):\n        \"\"\"Update with chunk of data\"\"\"\n        for i, col1 in enumerate(self.cols):\n            for j in range(i+1, len(self.cols)):\n                col2 = self.cols[j]\n\n                # Extract values\n                x = df[col1].to_numpy()\n                y = df[col2].to_numpy()\n\n                # Filter valid pairs\n                mask = np.isfinite(x) &amp; np.isfinite(y)\n                x_valid = x[mask]\n                y_valid = y[mask]\n\n                if len(x_valid) == 0:\n                    continue\n\n                # Update sufficient statistics\n                key = (col1, col2)\n                if key not in self.pairs:\n                    self.pairs[key] = {\n                        'n': 0, 'sx': 0, 'sy': 0,\n                        'sxx': 0, 'syy': 0, 'sxy': 0\n                    }\n\n                stats = self.pairs[key]\n                stats['n'] += len(x_valid)\n                stats['sx'] += float(np.sum(x_valid))\n                stats['sy'] += float(np.sum(y_valid))\n                stats['sxx'] += float(np.sum(x_valid ** 2))\n                stats['syy'] += float(np.sum(y_valid ** 2))\n                stats['sxy'] += float(np.sum(x_valid * y_valid))\n\n    def finalize(self, threshold: float = 0.0) -&gt; Dict:\n        \"\"\"Compute final correlations\"\"\"\n        results = {}\n\n        for (col1, col2), stats in self.pairs.items():\n            n = stats['n']\n            if n &lt; 2:\n                continue\n\n            # Compute correlation\n            num = n * stats['sxy'] - stats['sx'] * stats['sy']\n            den1 = n * stats['sxx'] - stats['sx'] ** 2\n            den2 = n * stats['syy'] - stats['sy'] ** 2\n\n            if den1 &lt;= 0 or den2 &lt;= 0:\n                continue\n\n            r = num / (math.sqrt(den1) * math.sqrt(den2))\n\n            # Filter by threshold\n            if abs(r) &gt;= threshold:\n                results[(col1, col2)] = r\n\n        return results\n</code></pre>"},{"location":"analytics/correlations/#complexity","title":"Complexity","text":""},{"location":"analytics/correlations/#space-complexity","title":"Space Complexity","text":"<p>For \\(p\\) numeric columns: - Number of pairs: \\(m = \\binom{p}{2} = \\frac{p(p-1)}{2} = O(p^2)\\) - Space per pair: O(1) (6 floating-point values) - Total space: O(p\u00b2)</p> <p>Example: - 10 columns \u2192 45 pairs \u2192 ~2 KB - 50 columns \u2192 1,225 pairs \u2192 ~50 KB - 100 columns \u2192 4,950 pairs \u2192 ~200 KB</p>"},{"location":"analytics/correlations/#time-complexity","title":"Time Complexity","text":"<p>Per chunk with \\(n\\) rows and \\(p\\) columns: - Iterate over \\(O(p^2)\\) pairs - For each pair: \\(O(n)\\) to compute valid mask and sums - Total per chunk: O(n p\u00b2)</p> <p>For dataset with \\(N\\) total rows: - Total time: O(N p\u00b2)</p>"},{"location":"analytics/correlations/#when-to-disable","title":"When to Disable","text":"<p>For large \\(p\\) (many columns), correlation computation can be expensive:</p> <ul> <li>\\(p &gt; 100\\): Consider disabling or using sampling</li> <li>\\(p &gt; 500\\): Strongly recommend disabling</li> </ul> <p>Configuration:</p> <pre><code>config = ReportConfig()\nconfig.compute.compute_correlations = False  # Disable\n</code></pre>"},{"location":"analytics/correlations/#configuration","title":"Configuration","text":"<p>Control correlation analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ProfileConfig, ComputeOptions\n\n# Using the public API\nconfig = ProfileConfig(compute=ComputeOptions(\n    compute_correlations=True,  # Default\n    corr_threshold=0.5,  # Default (only |r| &gt;= 0.5)\n    corr_max_cols=50,  # Default (skip if &gt; 50 cols)\n    corr_max_per_col=10  # Default (top 10 per column)\n))\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"analytics/correlations/#interpretation","title":"Interpretation","text":""},{"location":"analytics/correlations/#high-positive-correlation-r-08","title":"High Positive Correlation (r &gt; 0.8)","text":"<p>Interpretation: Variables move together strongly.</p> <p>Examples: - Height and weight (r \u2248 0.7-0.8) - Temperature in \u00b0F and \u00b0C (r = 1.0, exact conversion) - Revenue and profit (r \u2248 0.8-0.9)</p> <p>Actionable insights: - Potential redundancy (consider removing one feature) - Useful for imputation (predict one from other) - Check for derived features (one computed from other)</p>"},{"location":"analytics/correlations/#high-negative-correlation-r-08","title":"High Negative Correlation (r &lt; -0.8)","text":"<p>Interpretation: Variables move in opposite directions.</p> <p>Examples: - Latitude and temperature (r \u2248 -0.5 to -0.7) - Altitude and air pressure (r \u2248 -0.9) - Discount and profit margin (r \u2248 -0.6)</p> <p>Actionable insights: - Substitutes or inverse relationships - Consider composite features (sum, ratio)</p>"},{"location":"analytics/correlations/#low-correlation-r-02","title":"Low Correlation (|r| &lt; 0.2)","text":"<p>Interpretation: Little to no linear relationship.</p> <p>Note: Variables may still have nonlinear relationships (e.g., quadratic, exponential).</p> <p>Actionable insights: - Independent features (good for model diversity) - May need nonlinear analysis (polynomial features, interactions)</p>"},{"location":"analytics/correlations/#limitations","title":"Limitations","text":""},{"location":"analytics/correlations/#linear-relationships-only","title":"Linear Relationships Only","text":"<p>Pearson correlation measures linear association only.</p> <p>Example: Quadratic relationship \\(y = x^2\\) - Correlation: \\(r \\approx 0\\) (if x spans negative and positive) - But strong nonlinear relationship exists</p> <p>Solutions: - Use Spearman rank correlation (monotonic relationships) - Plot scatter plots - Use mutual information (any dependency)</p>"},{"location":"analytics/correlations/#sensitive-to-outliers","title":"Sensitive to Outliers","text":"<p>Single extreme value can dominate correlation.</p> <p>Solutions: - Use Spearman instead (rank-based, robust) - Remove outliers before computing - Use robust correlation measures (MAD-based)</p>"},{"location":"analytics/correlations/#correlation-causation","title":"Correlation \u2260 Causation","text":"<p>High correlation does not imply causation.</p> <p>Example: Ice cream sales and drowning deaths (r \u2248 0.9) - Spurious correlation (confounded by temperature/summer)</p>"},{"location":"analytics/correlations/#alternatives","title":"Alternatives","text":""},{"location":"analytics/correlations/#spearman-rank-correlation","title":"Spearman Rank Correlation","text":"<p>Measures monotonic (not necessarily linear) relationships.</p> \\[ \\rho_s = 1 - \\frac{6\\sum d_i^2}{n(n^2 - 1)} \\] <p>where \\(d_i\\) is the rank difference for observation \\(i\\).</p> <p>Advantages: - Captures monotonic nonlinear relationships - Robust to outliers - No distribution assumptions</p> <p>Disadvantages: - Requires sorting (more expensive) - Not streamable (needs ranks)</p> <p>Not implemented in current version</p> <p>Spearman correlation is planned for future release.</p>"},{"location":"analytics/correlations/#kendall-tau","title":"Kendall Tau","text":"<p>Another rank-based correlation measure.</p> <p>Advantages: - More robust than Spearman - Better for small samples</p> <p>Disadvantages: - Even more expensive to compute (O(n log n) or O(n\u00b2))</p>"},{"location":"analytics/correlations/#mutual-information","title":"Mutual Information","text":"<p>Measures any dependency (linear or nonlinear).</p> \\[ MI(X, Y) = \\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)} \\] <p>Advantages: - Detects any relationship - Information-theoretic</p> <p>Disadvantages: - Requires binning (continuous \u2192 discrete) - Harder to interpret than correlation</p>"},{"location":"analytics/correlations/#examples","title":"Examples","text":""},{"location":"analytics/correlations/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile, ReportConfig\n\ndf = pd.DataFrame({\n    \"x\": range(100),\n    \"y\": [2*i + 1 for i in range(100)],  # y = 2x + 1\n    \"z\": [100 - i for i in range(100)]    # z = 100 - x\n})\n\nconfig = ReportConfig()\nconfig.compute.compute_correlations = True\nconfig.compute.corr_threshold = 0.5\n\nreport = profile(df, config=config)\n# Expect: r(x,y) \u2248 1.0, r(x,z) \u2248 -1.0, r(y,z) \u2248 -1.0\n</code></pre>"},{"location":"analytics/correlations/#access-correlations-programmatically","title":"Access Correlations Programmatically","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\n\nx_stats = stats[\"columns\"][\"x\"]\ncorrelations = x_stats.get(\"corr_top\", [])\n\nfor col, r in correlations:\n    print(f\"x vs {col}: r = {r:.3f}\")\n</code></pre>"},{"location":"analytics/correlations/#high-dimensional-data","title":"High-Dimensional Data","text":"<pre><code># Many columns: disable correlations\nconfig = ReportConfig()\nconfig.compute.compute_correlations = False  # Too expensive\n\nreport = profile(wide_df, config=config)\n</code></pre>"},{"location":"analytics/correlations/#references","title":"References","text":"<ol> <li> <p>Pearson, K. (1895), \"Notes on regression and inheritance in the case of two parents\", Proceedings of the Royal Society of London, 58: 240\u2013242.</p> </li> <li> <p>Rodgers, J.L., Nicewander, W.A. (1988), \"Thirteen Ways to Look at the Correlation Coefficient\", The American Statistician, 42(1): 59\u201366.</p> </li> <li> <p>Spearman, C. (1904), \"The proof and measurement of association between two things\", American Journal of Psychology, 15: 72\u2013101.</p> </li> <li> <p>Benjamini, Y., Hochberg, Y. (1995), \"Controlling the false discovery rate: a practical and powerful approach to multiple testing\", Journal of the Royal Statistical Society B, 57(1): 289\u2013300.</p> </li> <li> <p>Wikipedia: Pearson correlation coefficient - Link</p> </li> <li> <p>Wikipedia: Spearman's rank correlation - Link</p> </li> </ol>"},{"location":"analytics/correlations/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Univariate numeric statistics</li> <li>Streaming Algorithms - Streaming computation techniques</li> <li>Configuration Guide - All parameters</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/duplicates/","title":"Duplicate Detection","text":"<p>PySuricata estimates duplicate rows using memory-efficient hash-based algorithms.</p>"},{"location":"analytics/duplicates/#duplicate-rate","title":"Duplicate Rate","text":"<p>For dataset with \\(n\\) total rows and \\(d\\) distinct rows:</p> \\[ DR = \\frac{n - d}{n} = 1 - \\frac{d}{n} \\]"},{"location":"analytics/duplicates/#detection-method","title":"Detection Method","text":"<p>Uses KMV sketch on row hashes for approximate distinct count:</p> <pre><code># Conceptual algorithm\nfor row in dataset:\n    row_hash = hash(tuple(row))\n    kmv.add(row_hash)\n\nn_distinct = kmv.estimate()\nduplicate_rate = (n_total - n_distinct) / n_total\n</code></pre>"},{"location":"analytics/duplicates/#exact-vs-approximate","title":"Exact vs Approximate","text":"<p>Exact (for small datasets): <pre><code>exact_duplicates = df.duplicated().sum()\ndup_pct = (exact_duplicates / len(df)) * 100\n</code></pre></p> <p>Approximate (PySuricata for large datasets): - Uses KMV sketch - ~2% error with default settings - Constant memory</p>"},{"location":"analytics/duplicates/#see-also","title":"See Also","text":"<ul> <li>Sketch Algorithms - KMV details</li> <li>Data Quality - Quality metrics</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/missing-values/","title":"Missing Values Analysis","text":"<p>Comprehensive guide to PySuricata's intelligent missing values analysis with adaptive display and chunk-level distribution tracking.</p>"},{"location":"analytics/missing-values/#overview","title":"Overview","text":"<p>Missing data is ubiquitous in real-world datasets. PySuricata provides:</p> <ul> <li>Intelligent display: Adaptive limits based on dataset size</li> <li>Chunk-level tracking: See missing data distribution across chunks</li> <li>Pattern detection: Identify systematic missingness</li> <li>Smart filtering: Show only significant missing columns</li> <li>Expandable UI: Progressive disclosure for many columns</li> </ul>"},{"location":"analytics/missing-values/#missing-data-mechanisms","title":"Missing Data Mechanisms","text":""},{"location":"analytics/missing-values/#mar-mcar-mnar","title":"MAR, MCAR, MNAR","text":"<p>Missing Completely At Random (MCAR): - Missingness independent of observed/unobserved data - \\(P(\\text{missing} | X, Y) = P(\\text{missing})\\) - Example: Sensor randomly fails</p> <p>Missing At Random (MAR): - Missingness depends on observed data only - \\(P(\\text{missing} | X, Y_{\\text{obs}}) = P(\\text{missing} | X)\\) - Example: Older patients skip optional questions</p> <p>Missing Not At Random (MNAR): - Missingness depends on unobserved values - Example: High earners don't report income</p> <p>Detection not automated</p> <p>Determining mechanism requires domain knowledge. PySuricata shows patterns to help investigation.</p>"},{"location":"analytics/missing-values/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"analytics/missing-values/#missing-rate","title":"Missing Rate","text":"<p>For column with \\(n_{\\text{total}}\\) observations:</p> \\[ MR = \\frac{n_{\\text{missing}}}{n_{\\text{total}}} \\]"},{"location":"analytics/missing-values/#missing-pattern-entropy","title":"Missing Pattern Entropy","text":"<p>For \\(k\\) different missing patterns (combinations of missing columns):</p> \\[ H_{\\text{pattern}} = -\\sum_{i=1}^{k} p_i \\log_2 p_i \\] <p>where \\(p_i\\) is the proportion of rows with pattern \\(i\\).</p> <p>High entropy: Many different patterns (complex missingness) Low entropy: Few patterns (systematic missingness)</p> <p>Not implemented</p> <p>Pattern entropy computation is planned for future release.</p>"},{"location":"analytics/missing-values/#intelligent-display-system","title":"Intelligent Display System","text":""},{"location":"analytics/missing-values/#dynamic-limits","title":"Dynamic Limits","text":"<p>Limits adapt to dataset size:</p> Dataset Size Initial Display Expanded Display \u226410 columns All All 11-50 columns 10 25 51-200 columns 12 25 &gt;200 columns 15 25"},{"location":"analytics/missing-values/#smart-filtering","title":"Smart Filtering","text":"<p>Threshold: Only show columns with &gt;\\(t\\)% missing (default \\(t=0.5\\)%)</p> <p>Rationale: Columns with &lt;0.5% missing are usually not concerning.</p>"},{"location":"analytics/missing-values/#expandable-ui","title":"Expandable UI","text":"<p>For datasets with many missing columns: 1. Initial view: Show top \\(n\\) columns 2. Expand button: Reveal up to 25 total 3. Smooth animation: JavaScript-powered transition</p>"},{"location":"analytics/missing-values/#chunk-level-distribution","title":"Chunk-Level Distribution","text":"<p>Track missing data per chunk to identify: - Temporal patterns (early vs. late data) - Batch patterns (certain files have more missing) - System issues (outages, collection failures)</p>"},{"location":"analytics/missing-values/#visualization","title":"Visualization","text":"<p>Horizontal bar showing missing percentage per chunk:</p> <pre><code>Chunk 1  \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591  40%\nChunk 2  \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591  20%\nChunk 3  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591   0%\nChunk 4  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591  70%\n</code></pre> <p>Reveals chunk 4 has data quality issue.</p>"},{"location":"analytics/missing-values/#configuration","title":"Configuration","text":"<pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Missing columns display threshold (default 0.5%)\n# (Not yet configurable in current version)\n\n# Maximum initial display (default: dynamic based on dataset size)\n# (Not yet configurable in current version)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"analytics/missing-values/#implementation","title":"Implementation","text":""},{"location":"analytics/missing-values/#missingcolumnsanalyzer","title":"MissingColumnsAnalyzer","text":"<pre><code>class MissingColumnsAnalyzer:\n    MIN_THRESHOLD_PCT = 0.5\n    MAX_INITIAL_DISPLAY = 8\n    MAX_EXPANDED_DISPLAY = 25\n\n    def analyze_missing_columns(self, miss_list, n_cols, n_rows):\n        \"\"\"Analyze and filter missing columns\"\"\"\n        # Filter significant missing\n        significant = [\n            item for item in miss_list \n            if item[1] &gt;= self.MIN_THRESHOLD_PCT\n        ]\n\n        # Determine limits\n        initial_limit = self._get_initial_display_limit(n_cols, n_rows)\n        expanded_limit = self._get_expanded_display_limit(n_cols, n_rows)\n\n        # Build result\n        return MissingColumnsResult(\n            initial_columns=significant[:initial_limit],\n            expanded_columns=significant[:expanded_limit],\n            needs_expandable=len(significant) &gt; initial_limit,\n            total_significant=len(significant),\n            total_insignificant=len(miss_list) - len(significant)\n        )\n</code></pre>"},{"location":"analytics/missing-values/#interpreting-results","title":"Interpreting Results","text":""},{"location":"analytics/missing-values/#high-missing-percentage-50","title":"High Missing Percentage (&gt;50%)","text":"<p>Possible causes: - Optional field (by design) - Data collection issue - Recent column (added midway) - Rare event (e.g., \"error_message\" only on errors)</p> <p>Actions: - Verify if intentional - Consider imputation or exclusion - Check data pipeline</p>"},{"location":"analytics/missing-values/#systematic-patterns","title":"Systematic Patterns","text":"<p>Multiple columns missing together:</p> <p>Possible causes: - Related optional section (e.g., address fields) - Batch import failure - Survey skip logic</p> <p>Actions: - Analyze co-occurrence - Check data source - Document business logic</p>"},{"location":"analytics/missing-values/#increasing-over-time","title":"Increasing Over Time","text":"<p>More missing in later chunks:</p> <p>Possible causes: - Degrading data quality - System malfunction - Intentional change</p> <p>Actions: - Investigate recent changes - Alert data engineering team</p>"},{"location":"analytics/missing-values/#littles-mcar-test","title":"Little's MCAR Test","text":"<p>Statistical test for MCAR assumption.</p> <p>Null hypothesis: Data is MCAR</p> <p>Test statistic: Compare means of subgroups defined by missing patterns</p> <p>P-value interpretation: - Large p-value: Consistent with MCAR - Small p-value: Reject MCAR (MAR or MNAR)</p> <p>Not implemented</p> <p>Little's test is planned for future release. Currently, users must perform external analysis.</p> <p>Reference: Little, R.J.A. (1988), \"A Test of Missing Completely at Random for Multivariate Data with Missing Values\", JASA, 83(404): 1198\u20131202.</p>"},{"location":"analytics/missing-values/#imputation-considerations","title":"Imputation Considerations","text":""},{"location":"analytics/missing-values/#meanmedian-imputation","title":"Mean/Median Imputation","text":"\\[ x_{\\text{imputed}} = \\begin{cases} x &amp; \\text{if observed} \\\\ \\bar{x} &amp; \\text{if missing} \\end{cases} \\] <p>Pros: Simple, fast Cons: Reduces variance, distorts correlations</p>"},{"location":"analytics/missing-values/#multiple-imputation","title":"Multiple Imputation","text":"<p>Generate \\(m\\) complete datasets with different imputations, analyze separately, combine results.</p> <p>Pros: Preserves uncertainty Cons: Complex, computationally expensive</p>"},{"location":"analytics/missing-values/#model-based","title":"Model-Based","text":"<p>Use ML model to predict missing values from other columns.</p> <p>Pros: Can capture complex relationships Cons: Requires training, may introduce bias</p> <p>PySuricata does not impute</p> <p>PySuricata is a profiling tool, not a preprocessing tool. Imputation should be done separately based on domain knowledge.</p>"},{"location":"analytics/missing-values/#best-practices","title":"Best Practices","text":"<ol> <li>Document missingness: Record why data is missing</li> <li>Distinguish NULL types: NULL vs. empty string vs. \"N/A\"</li> <li>Set thresholds: Define acceptable missing percentages</li> <li>Monitor trends: Track missing rates over time</li> <li>Investigate patterns: Look for systematic missingness</li> </ol>"},{"location":"analytics/missing-values/#examples","title":"Examples","text":""},{"location":"analytics/missing-values/#basic-usage","title":"Basic Usage","text":"<pre><code>from pysuricata import profile\n\n# Dataset with missing values\ndf = pd.DataFrame({\n    \"age\": [25, 30, None, 45, 50],\n    \"income\": [50000, None, None, 80000, 90000],\n    \"city\": [\"NYC\", \"LA\", None, \"Chicago\", None]\n})\n\nreport = profile(df)\n# Report shows missing percentages and patterns\n</code></pre>"},{"location":"analytics/missing-values/#access-missing-statistics","title":"Access Missing Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nprint(f\"Missing cells: {stats['dataset']['missing_cells_pct']:.1f}%\")\n\nfor col, col_stats in stats[\"columns\"].items():\n    missing_pct = col_stats.get(\"missing_pct\", 0)\n    if missing_pct &gt; 10:\n        print(f\"{col}: {missing_pct:.1f}% missing\")\n</code></pre>"},{"location":"analytics/missing-values/#references","title":"References","text":"<ol> <li> <p>Little, R.J.A., Rubin, D.B. (2019), Statistical Analysis with Missing Data, 3rd ed., Wiley.</p> </li> <li> <p>Rubin, D.B. (1976), \"Inference and Missing Data\", Biometrika, 63(3): 581\u2013592.</p> </li> <li> <p>Schafer, J.L., Graham, J.W. (2002), \"Missing Data: Our View of the State of the Art\", Psychological Methods, 7(2): 147\u2013177.</p> </li> <li> <p>Wikipedia: Missing data - Link</p> </li> </ol>"},{"location":"analytics/missing-values/#see-also","title":"See Also","text":"<ul> <li>Data Quality - Overall quality metrics</li> <li>Numeric Analysis - Handling missing in numeric columns</li> <li>Configuration - Display settings</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"analytics/quality/","title":"Data Quality Metrics","text":"<p>PySuricata computes several data quality metrics automatically.</p>"},{"location":"analytics/quality/#dataset-level-metrics","title":"Dataset-Level Metrics","text":""},{"location":"analytics/quality/#missing-cells-percentage","title":"Missing Cells Percentage","text":"\\[ \\text{Missing\\%} = \\frac{\\sum_{\\text{cols}} n_{\\text{missing}}}{\\text{rows} \\times \\text{cols}} \\times 100 \\] <p>Thresholds: - &lt; 5%: Good quality - 5-20%: Moderate issues - &gt; 20%: Significant problems</p>"},{"location":"analytics/quality/#duplicate-rows-approximate","title":"Duplicate Rows (Approximate)","text":"\\[ \\text{Dup\\%} = \\left(1 - \\frac{n_{\\text{distinct}}}{n_{\\text{total}}}\\right) \\times 100 \\]"},{"location":"analytics/quality/#constant-columns","title":"Constant Columns","text":"<p>Columns with single unique value (zero variance).</p>"},{"location":"analytics/quality/#highly-correlated-pairs","title":"Highly Correlated Pairs","text":"<p>Pairs with |r| &gt; 0.95 may indicate redundancy.</p>"},{"location":"analytics/quality/#column-level-metrics","title":"Column-Level Metrics","text":""},{"location":"analytics/quality/#completeness","title":"Completeness","text":"\\[ \\text{Completeness} = \\frac{n_{\\text{present}}}{n_{\\text{total}}} \\times 100 \\]"},{"location":"analytics/quality/#cardinality","title":"Cardinality","text":"<ul> <li>Very low (&lt; 10): Consider as categorical</li> <li>Very high (&gt; 0.9n): Consider as identifier</li> </ul>"},{"location":"analytics/quality/#outliers","title":"Outliers","text":"<p>Percentage of values outside acceptable ranges.</p>"},{"location":"analytics/quality/#quality-checks-in-cicd","title":"Quality Checks in CI/CD","text":"<pre><code>from pysuricata import summarize\n\ndef check_quality(df):\n    stats = summarize(df)\n\n    # Assertions\n    assert stats[\"dataset\"][\"missing_cells_pct\"] &lt; 5.0\n    assert stats[\"dataset\"][\"duplicate_rows_pct_est\"] &lt; 1.0\n\n    for col, col_stats in stats[\"columns\"].items():\n        if \"unique\" in col.lower():\n            # Expect high cardinality for ID columns\n            assert col_stats[\"distinct\"] == col_stats[\"count\"]\n</code></pre>"},{"location":"analytics/quality/#see-also","title":"See Also","text":"<ul> <li>Missing Values - Missing data analysis</li> <li>Duplicates - Duplicate detection</li> </ul> <p>Last updated: 2025-10-12</p>"},{"location":"stats/boolean/","title":"Boolean Variable Analysis","text":"<p>Comprehensive documentation for analyzing boolean (True/False) variables in PySuricata with information-theoretic measures.</p>"},{"location":"stats/boolean/#overview","title":"Overview","text":"<p>PySuricata treats boolean variables as columns with two distinct values (True/False, 1/0, Yes/No). Analysis focuses on balance, information content, and missing patterns.</p>"},{"location":"stats/boolean/#key-features","title":"Key Features","text":"<ul> <li>True/False counts with percentages</li> <li>Balance ratio (distribution symmetry)</li> <li>Entropy (information content)</li> <li>Information per value (bits)</li> <li>Imbalance detection (skewed distributions)</li> <li>Missing value handling</li> </ul>"},{"location":"stats/boolean/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each boolean column:</p> <ul> <li>Count: total non-null values</li> <li>True count: number of True values</li> <li>False count: number of False values</li> <li>Missing count: number of missing/null values</li> <li>True percentage: \\(p = n_{\\text{true}} / n\\)</li> <li>False percentage: \\(1 - p\\)</li> <li>Missing percentage: \\(n_{\\text{missing}} / n_{\\text{total}}\\)</li> <li>Entropy: Shannon entropy in bits</li> <li>Balance score: measure of distribution symmetry</li> <li>Imbalance ratio: deviation from 50/50 split</li> </ul>"},{"location":"stats/boolean/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/boolean/#basic-counts","title":"Basic Counts","text":"<p>Let the boolean column have: - \\(n_{\\text{true}}\\) = count of True values - \\(n_{\\text{false}}\\) = count of False values - \\(n_{\\text{missing}}\\) = count of missing/null values - \\(n = n_{\\text{true}} + n_{\\text{false}}\\) = non-null count - \\(n_{\\text{total}} = n + n_{\\text{missing}}\\) = total observations</p>"},{"location":"stats/boolean/#probability","title":"Probability","text":"<p>Probability of True:</p> \\[ p = \\frac{n_{\\text{true}}}{n} \\] <p>Probability of False:</p> \\[ q = 1 - p = \\frac{n_{\\text{false}}}{n} \\]"},{"location":"stats/boolean/#truefalse-ratio","title":"True/False Ratio","text":"\\[ R = \\frac{n_{\\text{true}}}{n_{\\text{false}}} \\] <p>Interpretation: - \\(R = 1\\): perfectly balanced (50/50) - \\(R &gt; 1\\): more True than False - \\(R &lt; 1\\): more False than True - \\(R \\to \\infty\\): nearly all True - \\(R \\to 0\\): nearly all False</p>"},{"location":"stats/boolean/#imbalance-ratio","title":"Imbalance Ratio","text":"<p>Measures deviation from balanced distribution:</p> \\[ I = \\frac{|n_{\\text{true}} - n_{\\text{false}}|}{n} = |2p - 1| \\] <p>Properties: - \\(I = 0\\): perfectly balanced (\\(p = 0.5\\)) - \\(I = 1\\): completely imbalanced (\\(p = 0\\) or \\(p = 1\\)) - Range: \\([0, 1]\\)</p> <p>Interpretation: - \\(I &lt; 0.2\\): well balanced (40/60 to 60/40) - \\(0.2 \\le I &lt; 0.6\\): moderately imbalanced - \\(I \\ge 0.6\\): severely imbalanced - \\(I &gt; 0.9\\): nearly constant</p>"},{"location":"stats/boolean/#balance-score","title":"Balance Score","text":"<p>Alternative measure of balance:</p> \\[ B = 1 - |0.5 - p| \\] <p>Properties: - \\(B = 1\\): perfectly balanced (\\(p = 0.5\\)) - \\(B = 0.5\\): completely imbalanced (\\(p = 0\\) or \\(p = 1\\)) - Range: \\([0.5, 1]\\)</p> <p>Interpretation: - \\(B &gt; 0.9\\): well balanced - \\(0.7 &lt; B \\le 0.9\\): moderately balanced - \\(B \\le 0.7\\): imbalanced</p>"},{"location":"stats/boolean/#shannon-entropy","title":"Shannon Entropy","text":"<p>Measures the information content or uncertainty in the boolean distribution:</p> \\[ H = -p \\log_2(p) - (1-p) \\log_2(1-p) \\] <p>By convention, \\(0 \\log_2(0) = 0\\).</p> <p>Properties: - \\(H = 0\\) bits if \\(p = 0\\) or \\(p = 1\\) (no uncertainty, deterministic) - \\(H = 1\\) bit if \\(p = 0.5\\) (maximum uncertainty, uniformly random) - Range: \\([0, 1]\\) bits</p> <p>Interpretation: - \\(H &lt; 0.5\\): low information content, predictable - \\(H \\approx 1.0\\): high information content, unpredictable - \\(H = 1.0\\): fair coin flip</p> <p>Entropy vs. Probability:</p> \\(p\\) \\(H\\) (bits) Interpretation 0.0 0.00 No information (constant False) 0.1 0.47 Low entropy, mostly False 0.5 1.00 Maximum entropy, balanced 0.9 0.47 Low entropy, mostly True 1.0 0.00 No information (constant True)"},{"location":"stats/boolean/#information-content-per-true-value","title":"Information Content per True Value","text":"<p>Average information conveyed by each True observation:</p> \\[ IC_{\\text{true}} = -\\log_2(p) \\text{ bits} \\] <p>Example: - \\(p = 0.5\\): \\(IC = 1\\) bit (unsurprising) - \\(p = 0.1\\): \\(IC = 3.32\\) bits (rare event, informative) - \\(p = 0.01\\): \\(IC = 6.64\\) bits (very rare, very informative)</p> <p>Use case: In imbalanced classification, rare class has higher information content.</p>"},{"location":"stats/boolean/#information-content-per-false-value","title":"Information Content per False Value","text":"\\[ IC_{\\text{false}} = -\\log_2(1 - p) \\text{ bits} \\]"},{"location":"stats/boolean/#statistical-tests","title":"Statistical Tests","text":""},{"location":"stats/boolean/#binomial-test-for-balance","title":"Binomial Test for Balance","text":"<p>Test if \\(p = 0.5\\) (balanced distribution).</p> <p>Null hypothesis: \\(H_0: p = 0.5\\)</p> <p>Test statistic:</p> \\[ Z = \\frac{\\hat{p} - 0.5}{\\sqrt{0.5 \\cdot 0.5 / n}} \\] <p>Under \\(H_0\\) and large \\(n\\), \\(Z \\sim N(0, 1)\\).</p> <p>P-value (two-tailed):</p> \\[ \\text{p-value} = 2 \\cdot \\Phi(-|Z|) \\] <p>where \\(\\Phi\\) is the standard normal CDF.</p> <p>Interpretation: - Small p-value (&lt; 0.05): reject balance hypothesis (distribution is skewed) - Large p-value: consistent with balanced distribution</p> <p>Not implemented in current version</p> <p>Statistical tests are planned for future release.</p>"},{"location":"stats/boolean/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Count True/False \\(O(n)\\) \\(O(1)\\) Single pass Entropy \\(O(1)\\) \\(O(1)\\) From counts All metrics \\(O(n)\\) \\(O(1)\\) Single pass <p>Boolean analysis is extremely efficient: O(1) space, O(n) time.</p>"},{"location":"stats/boolean/#configuration","title":"Configuration","text":"<p>Control boolean analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Boolean-specific config\n# (Currently no boolean-specific parameters)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/boolean/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/boolean/#booleanaccumulator-class","title":"BooleanAccumulator Class","text":"<pre><code>class BooleanAccumulator:\n    def __init__(self, name: str, config: BooleanConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n        self.true_count = 0\n        self.false_count = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing\n        # Count True values\n        # Count False values\n        pass\n\n    def finalize(self) -&gt; BooleanSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute percentages\n        # Compute entropy\n        # Compute balance scores\n        # Detect imbalance\n        return BooleanSummary(\n            count=self.count,\n            missing=self.missing,\n            true_count=self.true_count,\n            false_count=self.false_count,\n            true_pct=self.true_count / max(1, self.count),\n            entropy=self._compute_entropy(),\n            balance_score=self._compute_balance(),\n            imbalance_ratio=self._compute_imbalance(),\n        )\n\n    def _compute_entropy(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        p = self.true_count / self.count\n        if p == 0.0 or p == 1.0:\n            return 0.0\n        return -(p * math.log2(p) + (1-p) * math.log2(1-p))\n\n    def _compute_balance(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        p = self.true_count / self.count\n        return 1.0 - abs(0.5 - p)\n\n    def _compute_imbalance(self) -&gt; float:\n        if self.count == 0:\n            return 0.0\n        return abs(self.true_count - self.false_count) / self.count\n</code></pre>"},{"location":"stats/boolean/#examples","title":"Examples","text":""},{"location":"stats/boolean/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"is_active\": [True, False, True, True, None, False]\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/boolean/#imbalanced-boolean","title":"Imbalanced Boolean","text":"<pre><code># Highly imbalanced (10% True)\ndf = pd.DataFrame({\n    \"is_fraud\": [False] * 90 + [True] * 10\n})\n\nreport = profile(df)\n# Will show low entropy, high imbalance\n</code></pre>"},{"location":"stats/boolean/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nactive_stats = stats[\"columns\"][\"is_active\"]\n\nprint(f\"True count: {active_stats['true_count']}\")\nprint(f\"True %: {active_stats['true_pct']:.1%}\")\nprint(f\"Entropy: {active_stats['entropy']:.2f} bits\")\nprint(f\"Balance: {active_stats['balance_score']:.2f}\")\n</code></pre>"},{"location":"stats/boolean/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/boolean/#well-balanced-p-05","title":"Well-Balanced (p \u2248 0.5)","text":"<ul> <li>Entropy \u2248 1.0 bit</li> <li>Balance score &gt; 0.9</li> <li>Imbalance ratio &lt; 0.2</li> </ul> <p>Implications: - High information content - Good for binary classification (no class imbalance) - Unpredictable values</p> <p>Example: Fair coin flip, A/B test with even split.</p>"},{"location":"stats/boolean/#imbalanced-p-05-or-p-05","title":"Imbalanced (p &lt;&lt; 0.5 or p &gt;&gt; 0.5)","text":"<ul> <li>Entropy &lt; 0.5 bits</li> <li>Balance score &lt; 0.7</li> <li>Imbalance ratio &gt; 0.6</li> </ul> <p>Implications: - Low information content - May need rebalancing for ML - Predictable values</p> <p>Example: Fraud detection (1% positive), rare disease (0.1% positive).</p>"},{"location":"stats/boolean/#nearly-constant-p-001-or-p-099","title":"Nearly Constant (p &lt; 0.01 or p &gt; 0.99)","text":"<ul> <li>Entropy &lt; 0.1 bits</li> <li>Balance score \u2248 0.5</li> <li>Imbalance ratio &gt; 0.98</li> </ul> <p>Implications: - Almost no information - Consider removing column - May indicate data quality issue</p> <p>Example: \"is_deleted\" flag in active records table (all False).</p>"},{"location":"stats/boolean/#use-in-machine-learning","title":"Use in Machine Learning","text":""},{"location":"stats/boolean/#class-imbalance","title":"Class Imbalance","text":"<p>For binary classification with boolean target:</p> <p>Balanced (\\(0.4 &lt; p &lt; 0.6\\)): - Standard algorithms work well - Use accuracy as metric</p> <p>Moderately imbalanced (\\(0.1 &lt; p &lt; 0.4\\) or \\(0.6 &lt; p &lt; 0.9\\)): - Consider class weights - Use F1-score, AUC-ROC - Try SMOTE for oversampling</p> <p>Severely imbalanced (\\(p &lt; 0.1\\) or \\(p &gt; 0.9\\)): - Must use rebalancing techniques - Precision-recall curve essential - Consider anomaly detection instead</p>"},{"location":"stats/boolean/#entropy-as-feature-quality","title":"Entropy as Feature Quality","text":"<p>High entropy boolean features (\\(H \\approx 1\\)): - Good discriminative power - Worth including in model</p> <p>Low entropy boolean features (\\(H &lt; 0.5\\)): - Low information content - May not help model - Consider interaction terms</p>"},{"location":"stats/boolean/#special-cases","title":"Special Cases","text":""},{"location":"stats/boolean/#all-true-or-all-false","title":"All True or All False","text":"<ul> <li>Entropy = 0 (no information)</li> <li>Balance score = 0.5 (worst)</li> <li>Imbalance ratio = 1.0 (complete)</li> </ul> <p>Recommendation: Remove column (constant value).</p>"},{"location":"stats/boolean/#all-missing","title":"All Missing","text":"<ul> <li>No non-null values</li> <li>Statistics undefined</li> </ul> <p>Recommendation: Remove column or investigate data source.</p>"},{"location":"stats/boolean/#three-valued-boolean","title":"Three-Valued Boolean","text":"<p>Columns with True, False, and many NULLs:</p> <p>Interpretation: May be ternary (True/False/Unknown) rather than binary.</p> <p>Recommendation:  - Report missing percentage - Consider as categorical instead - Imputation may not be appropriate</p>"},{"location":"stats/boolean/#references","title":"References","text":"<ol> <li> <p>Shannon, C.E. (1948), \"A Mathematical Theory of Communication\", Bell System Technical Journal, 27: 379\u2013423.</p> </li> <li> <p>Cover, T.M., Thomas, J.A. (2006), Elements of Information Theory, 2nd ed., Wiley.</p> </li> <li> <p>Chawla, N.V. et al. (2002), \"SMOTE: Synthetic Minority Over-sampling Technique\", JAIR, 16: 321\u2013357.</p> </li> <li> <p>He, H., Garcia, E.A. (2009), \"Learning from Imbalanced Data\", IEEE TKDE, 21(9): 1263\u20131284.</p> </li> <li> <p>Wikipedia: Entropy (information theory) - Link</p> </li> <li> <p>Wikipedia: Binary classification - Link</p> </li> </ol>"},{"location":"stats/boolean/#see-also","title":"See Also","text":"<ul> <li>Categorical Analysis - For multi-class variables</li> <li>Data Quality - Quality metrics</li> <li>Configuration Guide - All parameters</li> </ul>"},{"location":"stats/categorical/","title":"Categorical Variable Analysis","text":"<p>This page provides comprehensive documentation for how PySuricata analyzes categorical (string, object) variables using scalable streaming algorithms with mathematical guarantees.</p>"},{"location":"stats/categorical/#overview","title":"Overview","text":"<p>PySuricata treats a categorical variable as any column with string-like values, objects, or low-cardinality integers. Analysis focuses on frequency distributions, diversity metrics, and string characteristics.</p>"},{"location":"stats/categorical/#key-features","title":"Key Features","text":"<ul> <li>Top-k values with frequencies (Misra-Gries algorithm)</li> <li>Distinct count estimation (KMV sketch)</li> <li>Diversity metrics (entropy, Gini impurity, concentration)</li> <li>String statistics (length distribution, empty strings)</li> <li>Variant detection (case-insensitive, trimmed)</li> <li>Memory-efficient streaming algorithms</li> </ul>"},{"location":"stats/categorical/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each categorical column:</p> <ul> <li>Count: non-null values, missing percentage</li> <li>Distinct: unique value count (exact or approximate)</li> <li>Top values: most frequent values with counts and percentages</li> <li>Entropy: Shannon entropy (information content)</li> <li>Gini impurity: concentration measure</li> <li>Diversity ratio: uniqueness measure</li> <li>Most common ratio: dominance of top value</li> <li>String length: mean, p90, distribution</li> <li>Special values: empty strings, whitespace-only</li> <li>Variants: case-insensitive and trimmed unique counts</li> </ul>"},{"location":"stats/categorical/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/categorical/#frequency-and-probability","title":"Frequency and Probability","text":"<p>Let \\(x_1, x_2, \\ldots, x_n\\) be the non-missing categorical values.</p> <p>Frequency of value \\(v\\):</p> \\[ f(v) = |\\{i : x_i = v\\}| \\] <p>Relative frequency (empirical probability):</p> \\[ p(v) = \\frac{f(v)}{n} \\] <p>Distinct count:</p> \\[ d = |\\{v : f(v) &gt; 0\\}| \\]"},{"location":"stats/categorical/#shannon-entropy","title":"Shannon Entropy","text":"<p>Measures the information content or uncertainty in the distribution:</p> \\[ H(X) = -\\sum_{v} p(v) \\log_2 p(v) \\] <p>where the sum is over all distinct values \\(v\\) with \\(p(v) &gt; 0\\).</p> <p>Properties: - \\(H(X) = 0\\) if one value has \\(p=1\\) (no uncertainty) - \\(H(X) = \\log_2 d\\) if all values equally likely (maximum entropy) - Units: bits of information</p> <p>Interpretation: - Low entropy (&lt; 1 bit): highly concentrated, predictable - Medium entropy (1-3 bits): moderate diversity - High entropy (&gt; 3 bits): high diversity, hard to predict</p> <p>Example: - Uniform distribution over 8 values: \\(H = \\log_2 8 = 3\\) bits - One value 90%, others 10%/9: \\(H \\approx 0.57\\) bits</p>"},{"location":"stats/categorical/#gini-impurity","title":"Gini Impurity","text":"<p>Measures the probability of misclassification if labels were assigned randomly according to the distribution:</p> \\[ \\text{Gini}(X) = 1 - \\sum_{v} p(v)^2 \\] <p>Properties: - \\(\\text{Gini}(X) = 0\\) if one value (no impurity) - \\(\\text{Gini}(X) = 1 - 1/d\\) if uniform over \\(d\\) values - Range: \\([0, 1)\\)</p> <p>Interpretation: - Low Gini (&lt; 0.2): concentrated distribution - Medium Gini (0.2-0.6): moderate spread - High Gini (&gt; 0.6): high diversity</p> <p>Use in ML: Decision trees use Gini impurity for splitting criteria.</p>"},{"location":"stats/categorical/#diversity-ratio","title":"Diversity Ratio","text":"<p>Simple measure of uniqueness:</p> \\[ D = \\frac{d}{n} \\] <p>where \\(d\\) = distinct count, \\(n\\) = total count.</p> <p>Interpretation: - \\(D \\to 0\\): low diversity (many repeats) - \\(D \\to 1\\): high diversity (mostly unique)</p> <p>Special cases: - \\(D = 1\\): all values unique (e.g., primary keys) - \\(D = 1/n\\): all values identical</p>"},{"location":"stats/categorical/#concentration-ratio","title":"Concentration Ratio","text":"<p>Fraction of observations in the top \\(k\\) values:</p> \\[ CR_k = \\frac{\\sum_{i=1}^{k} f(v_i)}{n} \\] <p>where \\(v_1, v_2, \\ldots\\) are values sorted by frequency (descending).</p> <p>Example: \\(CR_5 = 0.80\\) means top 5 values account for 80% of data.</p> <p>Interpretation: - High \\(CR_k\\): distribution dominated by few values - Low \\(CR_k\\): distribution spread across many values</p>"},{"location":"stats/categorical/#most-common-ratio","title":"Most Common Ratio","text":"<p>Dominance of the single most frequent value:</p> \\[ MCR = \\frac{f(v_{\\max})}{n} = p(v_{\\max}) \\] <p>Interpretation: - MCR &gt; 0.9: highly dominant category (nearly constant) - MCR &lt; 0.1: no dominant category (high diversity)</p>"},{"location":"stats/categorical/#streaming-algorithms","title":"Streaming Algorithms","text":""},{"location":"stats/categorical/#misra-gries-algorithm-for-top-k","title":"Misra-Gries Algorithm for Top-K","text":"<p>Finds the \\(k\\) most frequent items in a stream using O(k) space with frequency guarantees.</p> <p>Algorithm:</p> <ol> <li>Initialize: empty dictionary \\(M\\) (max size \\(k\\))</li> <li>For each value \\(v\\):</li> <li>If \\(v \\in M\\): increment \\(M[v]\\)</li> <li>Else if \\(|M| &lt; k\\): add \\(M[v] = 1\\)</li> <li>Else: decrement all counts in \\(M\\); remove zeros</li> <li>Output: items in \\(M\\) with estimated counts</li> </ol> <p>Guarantee: For any value \\(v\\) with true frequency \\(f(v)\\): - If \\(f(v) &gt; n/k\\), then \\(v\\) is in output - Estimated frequency within \\(n/k\\) of true frequency</p> <p>Space complexity: \\(O(k)\\) Update complexity: \\(O(k)\\) worst-case, \\(O(1)\\) amortized</p> <p>Mergeable: Yes (sum counters from multiple streams)</p> <p>Example: \\(k=50\\), \\(n=1,000,000\\) - Guaranteed to find all items with frequency &gt; 20,000 - Frequency estimates within \u00b120,000</p> <p>Reference: Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p>"},{"location":"stats/categorical/#kmv-sketch-for-distinct-count","title":"KMV Sketch for Distinct Count","text":"<p>Estimates cardinality using \\(k\\) minimum hash values.</p> <p>Algorithm:</p> <ol> <li>Initialize: empty set \\(S\\) (max size \\(k\\))</li> <li>For each value \\(v\\):</li> <li>Compute hash \\(h(v) \\in [0,1]\\)</li> <li>If \\(|S| &lt; k\\) or \\(h(v) &lt; \\max(S)\\):<ul> <li>Add \\(h(v)\\) to \\(S\\)</li> <li>If \\(|S| &gt; k\\): remove \\(\\max(S)\\)</li> </ul> </li> <li>Estimate:</li> </ol> \\[ \\hat{d} = \\frac{k-1}{x_k} \\] <p>where \\(x_k = \\max(S)\\) is the \\(k\\)-th smallest hash.</p> <p>Error bound:</p> \\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Space: \\(O(k)\\) Update: \\(O(\\log k)\\) (heap operations) Mergeable: Yes (union of sets)</p> <p>Example: \\(k=2048\\) - Error: ~2.2% (95% confidence) - Space: ~16 KB (assuming 64-bit hashes)</p> <p>Reference: Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p>"},{"location":"stats/categorical/#space-saving-algorithm-alternative","title":"Space-Saving Algorithm (Alternative)","text":"<p>Maintains top-k with guaranteed error bounds:</p> <p>Error bound: Estimated frequency within \\(\\epsilon n\\) where \\(\\epsilon = 1/k\\)</p> <p>Advantage over Misra-Gries: Tighter worst-case bounds, better for skewed distributions.</p> <p>Reference: Metwally, A., Agrawal, D., El Abbadi, A. (2005), \"Efficient Computation of Frequent and Top-k Elements in Data Streams\", ICDT.</p>"},{"location":"stats/categorical/#string-analysis","title":"String Analysis","text":""},{"location":"stats/categorical/#length-statistics","title":"Length Statistics","text":"<p>For string values, track:</p> <p>Mean length:</p> \\[ \\bar{L} = \\frac{1}{n} \\sum_{i=1}^{n} |x_i| \\] <p>where \\(|x_i|\\) is the character count of string \\(x_i\\).</p> <p>P90 length: 90th percentile of lengths (via reservoir sampling)</p> <p>Length distribution: Histogram of string lengths</p> <p>Use cases: - Detect outliers (abnormally long strings) - Validate constraints (max length) - Estimate storage requirements</p>"},{"location":"stats/categorical/#empty-strings","title":"Empty Strings","text":"<p>Count of strings that are: - Empty: <code>\"\"</code> - Whitespace-only: match <code>/^\\s*$/</code> - NULL vs. empty distinction</p> <p>Formula:</p> \\[ n_{\\text{empty}} = |\\{i : x_i = \"\" \\text{ or } x_i \\text{ matches } /^\\s*$/\\}| \\]"},{"location":"stats/categorical/#case-variants","title":"Case Variants","text":"<p>Estimate distinct count after case normalization:</p> \\[ d_{\\text{lower}} = |\\{v.\\text{lower}() : v \\in \\text{values}\\}| \\] <p>Interpretation: - \\(d_{\\text{lower}} &lt; d\\): case variants present (e.g., \"USA\", \"usa\") - \\(d_{\\text{lower}} = d\\): no case variants</p>"},{"location":"stats/categorical/#trim-variants","title":"Trim Variants","text":"<p>Estimate distinct count after removing leading/trailing whitespace:</p> \\[ d_{\\text{trim}} = |\\{v.\\text{strip}() : v \\in \\text{values}\\}| \\] <p>Interpretation: - \\(d_{\\text{trim}} &lt; d\\): whitespace variants present - \\(d_{\\text{trim}} = d\\): no trim variants</p>"},{"location":"stats/categorical/#chi-square-uniformity-test","title":"Chi-Square Uniformity Test","text":"<p>Test if the distribution is uniform (all categories equally likely).</p> <p>Null hypothesis: \\(p(v_1) = p(v_2) = \\cdots = p(v_d) = 1/d\\)</p> <p>Test statistic:</p> \\[ \\chi^2 = \\sum_{i=1}^{d} \\frac{(f(v_i) - E)^2}{E} \\] <p>where \\(E = n/d\\) is the expected frequency under uniformity.</p> <p>Distribution under \\(H_0\\): \\(\\chi^2_{d-1}\\) (chi-square with \\(d-1\\) degrees of freedom)</p> <p>P-value: \\(P(\\chi^2_{d-1} &gt; \\chi^2_{\\text{obs}})\\)</p> <p>Interpretation: - Small p-value (&lt; 0.05): reject uniformity (distribution is skewed) - Large p-value: consistent with uniform distribution</p> <p>Not implemented in current version</p> <p>Chi-square test is planned for future release.</p>"},{"location":"stats/categorical/#cardinality-categories","title":"Cardinality Categories","text":"<p>Classify categorical variables by distinct count:</p> Category Distinct Count Examples Boolean-like 2-3 Yes/No, True/False/Unknown Low cardinality 4-20 Status codes, categories Medium cardinality 21-100 US states, countries High cardinality 101-10,000 Zip codes, product IDs Very high cardinality &gt; 10,000 User IDs, URLs, emails <p>Recommended actions: - Low cardinality: Show all values in report - High cardinality: Show top-k only, estimate distinct - Very high cardinality: Consider as identifier (unique key)</p>"},{"location":"stats/categorical/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Misra-Gries \\(O(nk)\\) worst, \\(O(n)\\) amortized \\(O(k)\\) Top-k values KMV distinct \\(O(n \\log k)\\) \\(O(k)\\) Distinct count Entropy \\(O(n + d)\\) \\(O(d)\\) From frequency table String lengths \\(O(n)\\) \\(O(k)\\) Reservoir sample Exact distinct \\(O(n)\\) \\(O(d)\\) Hash set"},{"location":"stats/categorical/#configuration","title":"Configuration","text":"<p>Control categorical analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Top-k size (Misra-Gries)\nconfig.compute.top_k_size = 50  # Default\n\n# Distinct count sketch size (KMV)\nconfig.compute.uniques_sketch_size = 2_048  # Default\n\n# String length sample size\n# (Not separately configurable, uses numeric_sample_size)\n\n# Enable/disable case variants\n# (Always enabled, no toggle)\n\n# Enable/disable trim variants\n# (Always enabled, no toggle)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/categorical/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/categorical/#categoricalaccumulator-class","title":"CategoricalAccumulator Class","text":"<pre><code>class CategoricalAccumulator:\n    def __init__(self, name: str, config: CategoricalConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n\n        # Top-k values\n        self._topk = MisraGries(config.top_k_size)\n\n        # Distinct count\n        self._uniques = KMV(config.uniques_sketch_size)\n        self._uniques_lower = KMV(config.uniques_sketch_size)  # Case-insensitive\n        self._uniques_strip = KMV(config.uniques_sketch_size)  # Trimmed\n\n        # String lengths\n        self._len_sum = 0\n        self._len_n = 0\n        self._len_sample = ReservoirSampler(config.length_sample_size)\n\n        # Special values\n        self._empty_count = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing\n        # Update top-k\n        # Update distinct sketches (original, lower, strip)\n        # Track string lengths\n        # Count empty strings\n        pass\n\n    def finalize(self) -&gt; CategoricalSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Get top values from Misra-Gries\n        # Estimate distinct from KMV\n        # Compute entropy and Gini\n        # Compute string length stats\n        return CategoricalSummary(...)\n</code></pre>"},{"location":"stats/categorical/#validation","title":"Validation","text":"<p>PySuricata validates categorical algorithms:</p> <ul> <li>Exact vs approximate: Compare KMV estimate to exact count (small datasets)</li> <li>Top-k correctness: Verify all items with \\(f &gt; n/k\\) are found</li> <li>Entropy bounds: Check \\(0 \\le H(X) \\le \\log_2 d\\)</li> <li>Gini bounds: Check \\(0 \\le \\text{Gini}(X) &lt; 1\\)</li> <li>Mergeability: Verify merge = concatenate (for Misra-Gries, KMV)</li> </ul>"},{"location":"stats/categorical/#examples","title":"Examples","text":""},{"location":"stats/categorical/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"country\": [\"USA\", \"UK\", \"USA\", \"DE\", None, \"USA\", \"FR\"]\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/categorical/#high-cardinality-column","title":"High-Cardinality Column","text":"<pre><code># Column with 10,000 unique values\ndf = pd.DataFrame({\n    \"user_id\": [f\"user_{i}\" for i in range(100_000)]\n})\n\nconfig = ReportConfig()\nconfig.compute.top_k_size = 100  # Show top 100\nconfig.compute.uniques_sketch_size = 4_096  # More accurate distinct\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/categorical/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\ncountry_stats = stats[\"columns\"][\"country\"]\n\nprint(f\"Distinct: {country_stats['distinct']}\")\nprint(f\"Top value: {country_stats['top_values'][0]}\")\nprint(f\"Entropy: {country_stats['entropy']:.2f} bits\")\nprint(f\"Gini: {country_stats['gini']:.3f}\")\n</code></pre>"},{"location":"stats/categorical/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/categorical/#high-entropy","title":"High Entropy","text":"<p>\\(H(X) &gt; 5\\) bits suggests: - Many distinct values (&gt; 32) - Fairly uniform distribution - High information content - Possibly high cardinality (consider as identifier)</p>"},{"location":"stats/categorical/#low-entropy","title":"Low Entropy","text":"<p>\\(H(X) &lt; 1\\) bit suggests: - Few distinct values (&lt; 4 effective) - Skewed distribution (one value dominates) - Low information content - Consider as low-cardinality categorical</p>"},{"location":"stats/categorical/#high-gini","title":"High Gini","text":"<p>\\(\\text{Gini} &gt; 0.7\\) suggests: - Values well-distributed - No single dominant category - Good for stratification</p>"},{"location":"stats/categorical/#low-gini","title":"Low Gini","text":"<p>\\(\\text{Gini} &lt; 0.2\\) suggests: - One or few values dominate - Imbalanced distribution - Consider as nearly constant column</p>"},{"location":"stats/categorical/#special-cases","title":"Special Cases","text":""},{"location":"stats/categorical/#all-unique-primary-key","title":"All Unique (Primary Key)","text":"<ul> <li>Distinct count \\(d = n\\)</li> <li>Entropy \\(H = \\log_2 n\\) (maximum)</li> <li>Diversity ratio \\(D = 1.0\\)</li> <li>Top-k meaningless (all have count 1)</li> </ul> <p>Recommendation: Flag as identifier, exclude from analysis.</p>"},{"location":"stats/categorical/#nearly-constant","title":"Nearly Constant","text":"<ul> <li>Distinct count \\(d = 2\\) with \\(p_1 &gt; 0.99\\)</li> <li>Entropy \\(H &lt; 0.1\\) bits</li> <li>Gini \\(&lt; 0.02\\)</li> </ul> <p>Recommendation: Consider removing (low variance).</p>"},{"location":"stats/categorical/#many-empty-strings","title":"Many Empty Strings","text":"<ul> <li>Empty count &gt; 10% of non-null</li> </ul> <p>Possible data quality issue: Missing values encoded as empty strings.</p>"},{"location":"stats/categorical/#references","title":"References","text":"<ol> <li> <p>Misra, J., Gries, D. (1982), \"Finding repeated elements\", Science of Computer Programming, 2(2): 143\u2013152.</p> </li> <li> <p>Bar-Yossef, Z. et al. (2002), \"Counting Distinct Elements in a Data Stream\", RANDOM.</p> </li> <li> <p>Metwally, A., Agrawal, D., El Abbadi, A. (2005), \"Efficient Computation of Frequent and Top-k Elements in Data Streams\", ICDT.</p> </li> <li> <p>Shannon, C.E. (1948), \"A Mathematical Theory of Communication\", Bell System Technical Journal, 27: 379\u2013423.</p> </li> <li> <p>Breiman, L. et al. (1984), Classification and Regression Trees, Wadsworth.</p> </li> <li> <p>Wikipedia: Entropy (information theory) - Link</p> </li> <li> <p>Wikipedia: Decision tree learning - Link</p> </li> </ol>"},{"location":"stats/categorical/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - Numeric variables</li> <li>Sketch Algorithms - KMV, Misra-Gries deep dive</li> <li>Data Quality - Quality metrics</li> <li>Configuration Guide - All parameters</li> </ul>"},{"location":"stats/datetime/","title":"DateTime Variable Analysis","text":"<p>Comprehensive documentation for temporal data analysis in PySuricata, including time distributions, seasonality detection, and gap analysis.</p>"},{"location":"stats/datetime/#overview","title":"Overview","text":"<p>PySuricata treats datetime variables as columns with temporal types (datetime64, timestamp). Analysis focuses on temporal patterns, distributions, and data quality.</p>"},{"location":"stats/datetime/#key-features","title":"Key Features","text":"<ul> <li>Temporal range: min/max timestamps, time span</li> <li>Distribution analysis: hour, day-of-week, month patterns</li> <li>Monotonicity detection: sorted sequences</li> <li>Gap analysis: missing time periods</li> <li>Timeline visualization: temporal coverage</li> <li>Timezone handling: UTC normalization</li> </ul>"},{"location":"stats/datetime/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each datetime column:</p> <ul> <li>Count: non-null timestamps, missing percentage</li> <li>Range: minimum and maximum timestamps</li> <li>Span: total time covered (in days, hours, etc.)</li> <li>Hour distribution: counts by hour (0-23)</li> <li>Day-of-week distribution: counts by weekday (Mon-Sun)</li> <li>Month distribution: counts by month (Jan-Dec)</li> <li>Monotonicity: increasing/decreasing/mixed</li> <li>Timeline chart: visual temporal distribution</li> </ul>"},{"location":"stats/datetime/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/datetime/#temporal-measures","title":"Temporal Measures","text":"<p>Let \\(t_1, t_2, \\ldots, t_n\\) be the non-missing timestamp values (in seconds since epoch or similar).</p> <p>Time span:</p> \\[ \\Delta t = \\max(t) - \\min(t) \\] <p>Typically reported in days, hours, or appropriate units.</p> <p>Sampling rate (average):</p> \\[ r = \\frac{n}{\\Delta t} \\] <p>Average observations per unit time (e.g., rows per day).</p> <p>Time density:</p> \\[ \\rho = \\frac{n}{t_{\\max} - t_{\\min}} \\] <p>Similar to sampling rate; measures temporal concentration.</p>"},{"location":"stats/datetime/#monotonicity-coefficient","title":"Monotonicity Coefficient","text":"<p>Measures how sorted the timestamps are.</p> <p>Strictly increasing pairs:</p> \\[ n_{\\uparrow} = |\\{i : t_i &lt; t_{i+1}\\}| \\] <p>Monotonicity coefficient:</p> \\[ M = \\frac{n_{\\uparrow}}{n - 1} \\] <p>Interpretation: - \\(M = 1\\): strictly increasing (perfectly sorted) - \\(M = 0\\): strictly decreasing (reverse sorted) - \\(M \\approx 0.5\\): random order</p> <p>Use cases: - Detect time-sorted data (logs, time series) - Identify reverse chronological order - Flag shuffled temporal data</p>"},{"location":"stats/datetime/#temporal-entropy","title":"Temporal Entropy","text":"<p>Distribution entropy over time bins:</p> \\[ H_{\\text{time}} = -\\sum_{b \\in \\text{bins}} p_b \\log_2 p_b \\] <p>where \\(p_b\\) is the proportion of timestamps in bin \\(b\\).</p> <p>High entropy: events spread uniformly over time Low entropy: events concentrated in specific periods</p>"},{"location":"stats/datetime/#seasonality-detection","title":"Seasonality Detection","text":"<p>Detect periodic patterns using Fourier analysis or autocorrelation.</p> <p>Autocorrelation at lag \\(\\tau\\):</p> \\[ \\rho(\\tau) = \\frac{\\text{Cov}(X_t, X_{t+\\tau})}{\\text{Var}(X_t)} \\] <p>For count time series \\(X_t\\) (observations per time unit).</p> <p>Significant autocorrelation at lag \\(\\tau\\) suggests periodicity with period \\(\\tau\\).</p> <p>Common periods: - Daily: \\(\\tau = 1\\) day - Weekly: \\(\\tau = 7\\) days - Monthly: \\(\\tau \\approx 30\\) days - Yearly: \\(\\tau = 365\\) days</p> <p>Not fully implemented</p> <p>Seasonality detection via autocorrelation is planned for future release. Current version shows hour/day/month distributions which reveal patterns manually.</p>"},{"location":"stats/datetime/#temporal-distributions","title":"Temporal Distributions","text":""},{"location":"stats/datetime/#hour-distribution","title":"Hour Distribution","text":"<p>Count of observations by hour of day (0-23):</p> \\[ n_h = |\\{i : \\text{hour}(t_i) = h\\}| \\quad \\text{for } h \\in \\{0, 1, \\ldots, 23\\} \\] <p>Use cases: - Detect business hours (9am-5pm peaks) - Identify batch job times (off-hours spikes) - Analyze user activity patterns</p> <p>Visualization: Bar chart showing hourly counts.</p>"},{"location":"stats/datetime/#day-of-week-distribution","title":"Day-of-Week Distribution","text":"<p>Count by day (Monday=0, Sunday=6):</p> \\[ n_d = |\\{i : \\text{weekday}(t_i) = d\\}| \\quad \\text{for } d \\in \\{0, 1, \\ldots, 6\\} \\] <p>Use cases: - Detect weekday vs. weekend patterns - Identify business day data - Analyze periodic behavior</p> <p>Visualization: Bar chart showing daily counts.</p>"},{"location":"stats/datetime/#month-distribution","title":"Month Distribution","text":"<p>Count by month (Jan=1, Dec=12):</p> \\[ n_m = |\\{i : \\text{month}(t_i) = m\\}| \\quad \\text{for } m \\in \\{1, 2, \\ldots, 12\\} \\] <p>Use cases: - Detect seasonal effects - Identify fiscal quarters - Analyze annual patterns</p> <p>Visualization: Bar chart showing monthly counts.</p>"},{"location":"stats/datetime/#timeline-histogram","title":"Timeline Histogram","text":"<p>Temporal histogram showing observation density over time:</p> <ol> <li>Divide time range into \\(k\\) bins</li> <li>Count observations in each bin</li> <li>Display as histogram</li> </ol> <p>Bin width: \\(w = \\Delta t / k\\)</p> <p>Reveals: - Gaps in data collection - Burst periods (high activity) - Data quality issues (missing periods)</p>"},{"location":"stats/datetime/#gap-analysis","title":"Gap Analysis","text":"<p>Detect missing time periods in temporal data.</p> <p>Expected interval:</p> \\[ \\Delta_{\\text{exp}} = \\text{median}(\\{t_{i+1} - t_i : i = 1, \\ldots, n-1\\}) \\] <p>Gap threshold:</p> \\[ \\theta = c \\cdot \\Delta_{\\text{exp}} \\] <p>where \\(c &gt; 1\\) (e.g., \\(c = 2\\) or \\(c = 5\\)).</p> <p>Gaps:</p> \\[ G = \\{(t_i, t_{i+1}) : t_{i+1} - t_i &gt; \\theta\\} \\] <p>Gap statistics: - Number of gaps: \\(|G|\\) - Total missing time: \\(\\sum_{(t_i, t_{i+1}) \\in G} (t_{i+1} - t_i - \\theta)\\) - Longest gap: \\(\\max_{(t_i, t_{i+1}) \\in G} (t_{i+1} - t_i)\\)</p> <p>Not implemented in current version</p> <p>Gap analysis is planned for future release.</p>"},{"location":"stats/datetime/#timezone-handling","title":"Timezone Handling","text":"<p>All timestamps are normalized to UTC for analysis:</p> \\[ t_{\\text{UTC}} = t_{\\text{local}} - \\text{offset} \\] <p>Rationale: - Consistent comparisons across time zones - Avoids DST complications - Standard for distributed systems</p> <p>Reported in UI: Original timezone if available, UTC for calculations.</p>"},{"location":"stats/datetime/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Min/max \\(O(n)\\) \\(O(1)\\) Single pass Hour/day/month counts \\(O(n)\\) \\(O(1)\\) Fixed-size arrays (24, 7, 12) Monotonicity \\(O(n)\\) \\(O(1)\\) Compare adjacent pairs Timeline histogram \\(O(n)\\) \\(O(k)\\) \\(k\\) bins Gap detection \\(O(n \\log n)\\) \\(O(n)\\) Sorting required"},{"location":"stats/datetime/#configuration","title":"Configuration","text":"<p>Control datetime analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Timeline histogram bins\n# (Not separately configurable, uses default 50)\n\n# Gap detection threshold\n# (Not yet implemented)\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/datetime/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/datetime/#datetimeaccumulator-class","title":"DatetimeAccumulator Class","text":"<pre><code>class DatetimeAccumulator:\n    def __init__(self, name: str, config: DatetimeConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n\n        # Range tracking\n        self.min_ts = None\n        self.max_ts = None\n\n        # Distribution counters\n        self.hour_counts = [0] * 24\n        self.weekday_counts = [0] * 7\n        self.month_counts = [0] * 12\n\n        # Monotonicity tracking\n        self.prev_ts = None\n        self.monotonic_inc = 0\n        self.monotonic_dec = 0\n\n    def update(self, values: pd.Series):\n        \"\"\"Update with chunk of timestamps\"\"\"\n        # Convert to UTC\n        # Update min/max\n        # Count by hour/day/month\n        # Track monotonicity\n        pass\n\n    def finalize(self) -&gt; DatetimeSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute span\n        # Compute monotonicity coefficient\n        # Format distributions\n        # Build timeline\n        return DatetimeSummary(...)\n</code></pre>"},{"location":"stats/datetime/#examples","title":"Examples","text":""},{"location":"stats/datetime/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\n    \"timestamp\": pd.date_range(\"2023-01-01\", periods=1000, freq=\"H\")\n})\n\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/datetime/#time-series-data","title":"Time Series Data","text":"<pre><code># Stock prices\ndf = pd.read_csv(\"stocks.csv\", parse_dates=[\"date\"])\n\nreport = profile(df)\n# Analyze temporal patterns\n</code></pre>"},{"location":"stats/datetime/#access-statistics","title":"Access Statistics","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\nts_stats = stats[\"columns\"][\"timestamp\"]\n\nprint(f\"Min: {ts_stats['min']}\")\nprint(f\"Max: {ts_stats['max']}\")\nprint(f\"Span: {ts_stats['max'] - ts_stats['min']}\")\nprint(f\"Hour distribution: {ts_stats['hour_distribution']}\")\n</code></pre>"},{"location":"stats/datetime/#interpreting-results","title":"Interpreting Results","text":""},{"location":"stats/datetime/#monotonically-increasing","title":"Monotonically Increasing","text":"<p>\\(M = 1.0\\): Timestamps are sorted (common for logs, time series).</p> <p>Implications: - Data collected in chronological order - Suitable for time series analysis - May enable optimizations (binary search)</p>"},{"location":"stats/datetime/#random-order","title":"Random Order","text":"<p>\\(M \\approx 0.5\\): Timestamps shuffled or unordered.</p> <p>Implications: - Data may need sorting for analysis - Not a true time series - Consider sorting before visualization</p>"},{"location":"stats/datetime/#hourly-patterns","title":"Hourly Patterns","text":"<p>Peak in business hours (9am-5pm): - Typical for user activity data - Web traffic, transactions, etc.</p> <p>Flat distribution: - Automated data collection (24/7 sensors) - No human activity pattern</p>"},{"location":"stats/datetime/#weekly-patterns","title":"Weekly Patterns","text":"<p>Weekday peaks, weekend lows: - Business activity data - Employee-generated events</p> <p>Uniform distribution: - 24/7 operations - Automated systems</p>"},{"location":"stats/datetime/#monthly-patterns","title":"Monthly Patterns","text":"<p>Seasonal variations: - Retail sales (holiday spikes) - Weather data (summer/winter)</p> <p>Uniform distribution: - No seasonal effect - Steady-state process</p>"},{"location":"stats/datetime/#special-cases","title":"Special Cases","text":""},{"location":"stats/datetime/#all-same-timestamp","title":"All Same Timestamp","text":"<ul> <li>Distinct count = 1</li> <li>Span = 0</li> <li>Monotonicity undefined</li> </ul> <p>Possible issue: Snapshot data, not time series.</p>"},{"location":"stats/datetime/#large-gaps","title":"Large Gaps","text":"<p>Long periods without data: - Data collection interruptions - System downtime - Seasonal business (e.g., ski resorts)</p> <p>Recommendation: Investigate gaps, document known outages.</p>"},{"location":"stats/datetime/#future-timestamps","title":"Future Timestamps","text":"<p>Timestamps &gt; current time: - Data quality issue - Incorrect timezone - System clock skew</p> <p>Recommendation: Flag as data quality problem.</p>"},{"location":"stats/datetime/#references","title":"References","text":"<ol> <li> <p>Box, G.E.P., Jenkins, G.M., Reinsel, G.C. (2015), Time Series Analysis: Forecasting and Control, Wiley.</p> </li> <li> <p>Brockwell, P.J., Davis, R.A. (2016), Introduction to Time Series and Forecasting, Springer.</p> </li> <li> <p>Cleveland, R.B. et al. (1990), \"STL: A Seasonal-Trend Decomposition Procedure Based on Loess\", Journal of Official Statistics, 6(1): 3\u201373.</p> </li> <li> <p>Wikipedia: Autocorrelation - Link</p> </li> <li> <p>Wikipedia: Seasonality - Link</p> </li> </ol>"},{"location":"stats/datetime/#see-also","title":"See Also","text":"<ul> <li>Numeric Analysis - For temporal metrics as numbers</li> <li>Data Quality - Quality checks</li> <li>Configuration Guide - All parameters</li> </ul>"},{"location":"stats/numeric/","title":"Numeric Variable Analysis","text":"<p>This page provides comprehensive technical documentation for how pysuricata profiles and summarizes numerical (continuous and discrete) columns at scale using proven streaming algorithms with mathematical guarantees.</p> <p>Audience</p> <p>Designed for users who want to understand and trust the numbers in the HTML report, as well as contributors who need to modify or extend the accumulator implementations.</p>"},{"location":"stats/numeric/#overview","title":"Overview","text":"<p>PySuricata treats a numerical variable as any column with machine type among <code>{int8, int16, int32, int64, float32, float64, decimal}</code> (nullable). Values may include <code>NaN</code>, <code>\u00b1Inf</code>, and missing markers, all handled appropriately.</p> <p>All statistics are computed incrementally via stateful accumulators using single-pass streaming algorithms, enabling processing of datasets larger than available RAM.</p>"},{"location":"stats/numeric/#summary-statistics-provided","title":"Summary Statistics Provided","text":"<p>For each numeric column, the report includes:</p> <ul> <li>Count: non-null values, missing percentage</li> <li>Central tendency: mean, median</li> <li>Dispersion: variance, standard deviation, IQR, MAD, coefficient of variation</li> <li>Shape: skewness, excess kurtosis, bimodality hints</li> <li>Range: min, max, range</li> <li>Quantiles: configurable set (default: 0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99)</li> <li>Outliers: IQR fences, z-score, MAD-based detection</li> <li>Distribution: histogram with adaptive binning</li> <li>Special values: zeros, negatives, infinities</li> <li>Extremes: min/max values with row indices</li> <li>Confidence intervals: 95% CI for mean</li> <li>Correlations: top correlations with other numeric columns (optional)</li> </ul>"},{"location":"stats/numeric/#quality-flags","title":"Quality Flags","text":"<p>Numeric variable cards display quality flags (colored chips) to highlight data characteristics. Flags are detected automatically based on statistical thresholds.</p>"},{"location":"stats/numeric/#critical-issues","title":"\ud83d\udd34 Critical Issues","text":"Flag Threshold Impact Action Missing (&gt;20%) &gt;20% values missing Bias in analysis, imputation needed Investigate data collection Has \u221e Any \u00b1\u221e present Overflow, division by zero, or corruption Exclude or cap infinite values Zero-inflated (\u226550%) \u226550% zeros Sparse data or data quality issue Use zero-inflated models Constant All values identical Zero variance, no information Remove column Quasi-constant &gt;95% same value Limited information content Review column utility Many outliers High outlier proportion Skewed mean and std Robust statistics or capping Heavy-tailed |kurtosis| &gt; 3 Extreme values, unreliable classical stats Robust methods or transformations"},{"location":"stats/numeric/#distribution-warnings","title":"\ud83d\udfe0 Distribution Warnings","text":"Flag Threshold Impact Action Missing (\u226420%) 0\u201320% missing Minor bias risk Monitor, document pattern Has negatives (&gt;10%) &gt;10% negative Unexpected for some metrics (age, price) Verify domain expectations Skewed Right Skewness &gt; 1 Long right tail, mean &gt; median Log transform or report median Skewed Left Skewness &lt; \u22121 Long left tail, mean &lt; median Transform or non-parametric methods Discrete Low unique count Behaves like categorical Consider as categorical/ordinal Heaping Clustering at round numbers Rounding or self-reporting Aware of artificial clustering Possibly bimodal Bimodality coefficient high Two populations or processes Investigate subgroups Some outliers 0.3\u20131% outliers Minor effect on statistics Review if valid extremes Zero-inflated (&lt;50%) 30\u201350% zeros Affects distribution shape Consider zero-inflated models"},{"location":"stats/numeric/#positive-characteristics","title":"\ud83d\udfe2 Positive Characteristics","text":"Flag Threshold Benefit Positive-only All values &gt; 0 Safe for log transform, geometric mean \u2248 Normal (JB) Jarque-Bera test passes Parametric tests valid Log-scale? Range + skewness suggest log May reveal hidden patterns Monotonic \u2191 Weakly increasing Index, timestamp, or cumulative Monotonic \u2193 Weakly decreasing Countdown or reverse-sorted"},{"location":"stats/numeric/#badges","title":"Badges","text":"Badge Meaning <code>Numeric</code> Continuous or discrete numeric column Data type (e.g., <code>int64</code>) Source precision, range, and memory usage <code>approx</code> Statistics use sampling or approximation <p>Multiple flags can appear together. Address by priority: \ud83d\udd34 first \u2192 \ud83d\udfe0 next \u2192 \ud83d\udfe2 to inform approach.</p>"},{"location":"stats/numeric/#mathematical-definitions","title":"Mathematical Definitions","text":""},{"location":"stats/numeric/#notation","title":"Notation","text":"<p>Let \\(x_1, x_2, \\ldots, x_n\\) be the non-missing, finite observations for a column.</p>"},{"location":"stats/numeric/#central-tendency","title":"Central Tendency","text":"<p>Mean (arithmetic average):</p> \\[ \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i \\] <p>Median:</p> <p>The value \\(m\\) such that at least half the observations are \\(\\le m\\) and at least half are \\(\\ge m\\). For even \\(n\\), typically the average of the two middle values.</p> \\[ \\text{median} = \\begin{cases} x_{(n+1)/2} &amp; \\text{if } n \\text{ odd} \\\\ \\frac{x_{n/2} + x_{n/2+1}}{2} &amp; \\text{if } n \\text{ even} \\end{cases} \\] <p>where \\(x_{(k)}\\) denotes the \\(k\\)-th order statistic (sorted values).</p>"},{"location":"stats/numeric/#dispersion","title":"Dispersion","text":"<p>Sample variance (unbiased):</p> \\[ s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} (x_i-\\bar{x})^2 \\] <p>Standard deviation:</p> \\[ s = \\sqrt{s^2} \\] <p>Coefficient of variation (CV):</p> \\[ \\text{CV} = \\frac{s}{|\\bar{x}|} \\times 100\\% \\] <p>Measures relative variability; useful for comparing dispersion across variables with different scales.</p> <p>Interquartile range (IQR):</p> \\[ \\text{IQR} = Q_{0.75} - Q_{0.25} \\] <p>Robust measure of spread, resistant to outliers.</p> <p>Median absolute deviation (MAD):</p> \\[ \\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|) \\] <p>Highly robust measure of variability; often preferred over standard deviation for non-normal data.</p>"},{"location":"stats/numeric/#shape-statistics","title":"Shape Statistics","text":"<p>Skewness (Fisher-Pearson, \\(g_1\\)):</p> <p>Measures asymmetry of the distribution.</p> \\[ g_1 = \\frac{n}{(n-1)(n-2)} \\cdot \\frac{m_3}{s^3} \\] <p>where \\(m_3 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^3\\) is the third central moment.</p> <p>Interpretation: - \\(g_1 = 0\\): symmetric - \\(g_1 &gt; 0\\): right-skewed (long right tail) - \\(g_1 &lt; 0\\): left-skewed (long left tail)</p> <p>Excess kurtosis (\\(g_2\\)):</p> <p>Measures tail heaviness relative to normal distribution.</p> \\[ g_2 = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\cdot \\frac{m_4}{s^4} - \\frac{3(n-1)^2}{(n-2)(n-3)} \\] <p>where \\(m_4 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\bar{x})^4\\) is the fourth central moment.</p> <p>Interpretation: - \\(g_2 = 0\\): normal (mesokurtic) - \\(g_2 &gt; 0\\): heavy tails (leptokurtic) - \\(g_2 &lt; 0\\): light tails (platykurtic)</p>"},{"location":"stats/numeric/#quantiles","title":"Quantiles","text":"<p>Quantile function \\(Q(p)\\):</p> <p>For probability \\(p \\in [0,1]\\), the \\(p\\)-quantile is:</p> \\[ Q(p) = \\inf\\{x : F(x) \\ge p\\} \\] <p>where \\(F(x) = \\mathbb{P}(X \\le x)\\) is the cumulative distribution function.</p> <p>Common quantiles: - \\(Q(0.25)\\): first quartile (Q1) - \\(Q(0.50)\\): median (Q2) - \\(Q(0.75)\\): third quartile (Q3)</p>"},{"location":"stats/numeric/#confidence-intervals","title":"Confidence Intervals","text":"<p>95% confidence interval for mean:</p> <p>Assuming approximate normality (or large \\(n\\) by CLT):</p> \\[ \\text{CI}_{0.95}(\\bar{x}) = \\bar{x} \\pm t_{n-1,0.975} \\cdot \\frac{s}{\\sqrt{n}} \\] <p>where \\(t_{n-1,0.975}\\) is the 97.5th percentile of Student's t-distribution with \\(n-1\\) degrees of freedom.</p> <p>For large \\(n\\), \\(t_{n-1,0.975} \\approx 1.96\\).</p>"},{"location":"stats/numeric/#outlier-detection","title":"Outlier Detection","text":"<p>IQR fences (Tukey's method):</p> <p>Lower fence:</p> \\[ L = Q_{0.25} - 1.5 \\cdot \\text{IQR} \\] <p>Upper fence:</p> \\[ U = Q_{0.75} + 1.5 \\cdot \\text{IQR} \\] <p>Values outside \\([L, U]\\) are flagged as outliers. For \"extreme\" outliers, use multiplier 3.0 instead of 1.5.</p> <p>Modified z-score (robust):</p> \\[ M_i = 0.6745 \\cdot \\frac{x_i - \\text{median}(x)}{\\text{MAD}} \\] <p>Flag if \\(|M_i| &gt; 3.5\\). The constant 0.6745 makes MAD consistent with standard deviation under normality.</p> <p>Classical z-score:</p> \\[ Z_i = \\frac{x_i - \\bar{x}}{s} \\] <p>Flag if \\(|Z_i| &gt; 3\\). Sensitive to outliers themselves (not robust).</p>"},{"location":"stats/numeric/#streaming-algorithms","title":"Streaming Algorithms","text":""},{"location":"stats/numeric/#welfords-online-algorithm","title":"Welford's Online Algorithm","text":"<p>For computing mean and variance in a single pass with O(1) memory and numerical stability.</p> <p>Initialization:</p> \\[ n = 0, \\quad \\mu = 0, \\quad M_2 = 0 \\] <p>Update step: for each new value \\(x\\):</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\mu &amp;\\leftarrow \\mu + \\frac{\\delta}{n} \\\\ \\delta_2 &amp;= x - \\mu \\\\ M_2 &amp;\\leftarrow M_2 + \\delta \\cdot \\delta_2 \\end{aligned} \\] <p>Finalize:</p> \\[ \\text{mean} = \\mu, \\quad \\text{variance} = \\frac{M_2}{n-1} \\] <p>Properties: - Numerical stability: avoids catastrophic cancellation in \\(\\sum x_i^2 - n\\bar{x}^2\\) - Exact: produces same result as two-pass method (up to FP rounding) - Online: updates in O(1) time per value</p> <p>Reference: Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p>"},{"location":"stats/numeric/#higher-moments-skewness-kurtosis","title":"Higher Moments (Skewness, Kurtosis)","text":"<p>Extending Welford to track \\(M_3\\) and \\(M_4\\):</p> <p>Update step:</p> \\[ \\begin{aligned} n &amp;\\leftarrow n + 1 \\\\ \\delta &amp;= x - \\mu \\\\ \\delta_n &amp;= \\frac{\\delta}{n} \\\\ \\delta_n^2 &amp;= \\delta_n^2 \\\\ \\mu &amp;\\leftarrow \\mu + \\delta_n \\\\ M_4 &amp;\\leftarrow M_4 + \\delta \\left(\\delta^3 \\frac{n(n-1)}{n^3} + 6\\delta_n M_2 - 4\\delta_n M_3\\right) \\\\ M_3 &amp;\\leftarrow M_3 + \\delta \\left(\\delta^2 \\frac{n(n-1)}{n^2} - 3\\delta_n M_2\\right) \\\\ M_2 &amp;\\leftarrow M_2 + \\delta(\\delta - \\delta_n) \\end{aligned} \\] <p>Finalize:</p> \\[ \\begin{aligned} s^2 &amp;= \\frac{M_2}{n-1} \\\\ g_1 &amp;= \\frac{n}{(n-1)(n-2)} \\cdot \\frac{M_3/n}{(s^2)^{3/2}} \\\\ g_2 &amp;= \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\cdot \\frac{M_4/n}{(s^2)^2} - \\frac{3(n-1)^2}{(n-2)(n-3)} \\end{aligned} \\]"},{"location":"stats/numeric/#pebays-parallel-merge-formulas","title":"P\u00e9bay's Parallel Merge Formulas","text":"<p>To combine results from multiple chunks or parallel threads, P\u00e9bay's formulas enable exact merging of moments.</p> <p>Given two partial states: - State A: \\((n_a, \\mu_a, M_{2a}, M_{3a}, M_{4a})\\) - State B: \\((n_b, \\mu_b, M_{2b}, M_{3b}, M_{4b})\\)</p> <p>Define:</p> \\[ \\delta = \\mu_b - \\mu_a, \\quad n = n_a + n_b \\] <p>Merged state:</p> \\[ \\begin{aligned} \\mu &amp;= \\mu_a + \\delta \\cdot \\frac{n_b}{n} \\\\ M_2 &amp;= M_{2a} + M_{2b} + \\delta^2 \\cdot \\frac{n_a n_b}{n} \\\\ M_3 &amp;= M_{3a} + M_{3b} + \\delta^3 \\cdot \\frac{n_a n_b (n_a - n_b)}{n^2} + 3\\delta \\cdot \\frac{n_a M_{2b} - n_b M_{2a}}{n} \\\\ M_4 &amp;= M_{4a} + M_{4b} + \\delta^4 \\cdot \\frac{n_a n_b (n_a^2 - n_a n_b + n_b^2)}{n^3} \\\\ &amp;\\quad + 6\\delta^2 \\cdot \\frac{n_a n_b}{n^2}(n_a M_{2b} + n_b M_{2a}) + 4\\delta \\cdot \\frac{n_a M_{3b} - n_b M_{3a}}{n} \\end{aligned} \\] <p>Properties: - Associative: order of merging doesn't matter (up to FP rounding) - Exact: same result as single-pass over concatenated data - Parallelizable: enables multi-core and distributed computation</p> <p>Reference: P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212.</p>"},{"location":"stats/numeric/#quantile-estimation","title":"Quantile Estimation","text":""},{"location":"stats/numeric/#exact-quantiles-reservoir-sampling","title":"Exact Quantiles (Reservoir Sampling)","text":"<p>For exact quantiles, maintain a reservoir sample of size \\(k\\) (default 20,000):</p> <ol> <li>First \\(k\\) values: keep all</li> <li>For value \\(i &gt; k\\): include with probability \\(k/i\\), replacing random existing sample</li> </ol> <p>Uniform guarantee: Every subset of size \\(k\\) has equal probability.</p> <p>Quantile computation: Sort sample and compute using linear interpolation:</p> \\[ Q(p) \\approx x_{(\\lceil p \\cdot k \\rceil)} \\] <p>Pros: Exact for small datasets, unbiased estimator Cons: \\(O(k)\\) memory, \\(O(k \\log k)\\) sort cost</p>"},{"location":"stats/numeric/#approximate-quantiles-kll-sketch","title":"Approximate Quantiles (KLL Sketch)","text":"<p>For massive datasets, use KLL sketch (Karnin, Lang, Liberty):</p> <p>Properties: - Space: \\(O(\\frac{1}{\\epsilon} \\log \\log \\frac{1}{\\delta})\\) - Error bound: \\(\\epsilon\\)-approximate with probability \\(1-\\delta\\) - Mergeable: Combine sketches from multiple streams</p> <p>Example: With \\(\\epsilon=0.01\\), quantiles accurate to \u00b11 percentile using ~1 KB memory.</p> <p>Reference: Karnin, Z., Lang, K., Liberty, E. (2016), \"Optimal Quantile Approximation in Streams\", arXiv:1603.05346.</p>"},{"location":"stats/numeric/#t-digest-alternative","title":"T-Digest (Alternative)","text":"<p>T-digest (Dunning &amp; Ertl) provides excellent tail accuracy:</p> <p>Properties: - Better accuracy for extreme quantiles (P99, P99.9) - Adaptive compression - Mergeable</p> <p>Reference: Dunning, T., Ertl, O. (2019), \"Computing Extremely Accurate Quantiles Using t-Digests\", arXiv:1902.04023.</p>"},{"location":"stats/numeric/#histogram-construction","title":"Histogram Construction","text":""},{"location":"stats/numeric/#freedman-diaconis-rule","title":"Freedman-Diaconis Rule","text":"<p>Optimal bin width for histograms:</p> \\[ h = 2 \\cdot \\frac{\\text{IQR}}{n^{1/3}} \\] <p>Number of bins:</p> \\[ k = \\left\\lceil \\frac{\\max - \\min}{h} \\right\\rceil \\] <p>Rationale: Balances bias and variance; works well for wide variety of distributions.</p>"},{"location":"stats/numeric/#sturges-rule-alternative","title":"Sturges' Rule (Alternative)","text":"\\[ k = \\lceil \\log_2 n \\rceil + 1 \\] <p>Simpler but may undersmooth for large \\(n\\).</p>"},{"location":"stats/numeric/#scotts-rule-alternative","title":"Scott's Rule (Alternative)","text":"\\[ h = 3.5 \\cdot \\frac{s}{n^{1/3}} \\] <p>Assumes normal distribution; similar to Freedman-Diaconis.</p>"},{"location":"stats/numeric/#distinct-count-estimation","title":"Distinct Count Estimation","text":"<p>For numeric columns with many repeated values (e.g., categorical-like integers):</p>"},{"location":"stats/numeric/#kmv-k-minimum-values","title":"KMV (K-Minimum Values)","text":"<p>Maintain the \\(k\\) smallest hash values from column.</p> <p>Estimator:</p> \\[ \\hat{n}_{\\text{distinct}} = \\frac{k-1}{x_k} \\] <p>where \\(x_k\\) is the \\(k\\)-th smallest hash (normalized to [0,1]).</p> <p>Error bound:</p> \\[ \\text{Relative error} \\approx \\frac{1}{\\sqrt{k}} \\] <p>Example: \\(k=2048\\) \u2192 ~2.2% error (95% confidence)</p> <p>Properties: - Mergeable: Union of two KMV sketches - Space: \\(O(k)\\) per column - Update: \\(O(\\log k)\\) per value</p>"},{"location":"stats/numeric/#hyperloglog-alternative","title":"HyperLogLog (Alternative)","text":"<p>Space: \\(O(\\epsilon^{-2})\\) for relative error \\(\\epsilon\\) Error: Typical 2% with 1.5 KB Standard: Redis, BigQuery, many production systems</p>"},{"location":"stats/numeric/#missing-nan-and-inf-handling","title":"Missing, NaN, and Inf Handling","text":"<ul> <li>Missing (NULL): Excluded from moment calculations; counted separately</li> <li>NaN (Not-a-Number): Treated as missing</li> <li>\u00b1Inf: Excluded from moments; counted under <code>inf_count</code> and surfaced in warnings</li> <li>Type coercion: Strings parsing to numbers counted only if parsing enabled</li> </ul> <p>Edge Cases</p> <ul> <li>All-missing columns: statistics undefined (reported as <code>null</code>)</li> <li>\\(n &lt; 2\\): variance/shape undefined</li> <li>\\(n &lt; 4\\): kurtosis undefined</li> </ul>"},{"location":"stats/numeric/#computational-complexity","title":"Computational Complexity","text":"Operation Time Space Notes Moments \\(O(n)\\) total, \\(O(1)\\) per value \\(O(1)\\) Welford/P\u00e9bay Reservoir sampling \\(O(n)\\) total, \\(O(1)\\) amortized \\(O(k)\\) \\(k\\) = sample size Quantiles (exact) \\(O(k \\log k)\\) \\(O(k)\\) Sort sample KLL sketch \\(O(n \\log \\log n)\\) \\(O(\\epsilon^{-1} \\log \\log n)\\) Approximate KMV distinct \\(O(n \\log k)\\) \\(O(k)\\) Heap operations Histogram \\(O(n + k)\\) \\(O(b)\\) \\(b\\) bins"},{"location":"stats/numeric/#configuration","title":"Configuration","text":"<p>Control numeric analysis via <code>ReportConfig</code>:</p> <pre><code>from pysuricata import profile, ReportConfig\n\nconfig = ReportConfig()\n\n# Sample size for quantiles/histograms\nconfig.compute.numeric_sample_size = 20_000  # Default\n\n# Sketch sizes\nconfig.compute.uniques_sketch_size = 2_048  # KMV (default)\nconfig.compute.top_k_size = 50  # Top values (if tracking)\n\n# Quantiles to compute\n# (Not yet configurable, default: [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n\n# Histogram bins\n# (Automatic via Freedman-Diaconis, max 256)\n\n# Outlier detection method\n# (Always computed: IQR, z-score, MAD)\n\n# Random seed for reproducibility\nconfig.compute.random_seed = 42\n\nreport = profile(df, config=config)\n</code></pre>"},{"location":"stats/numeric/#implementation-details","title":"Implementation Details","text":""},{"location":"stats/numeric/#numericaccumulator-class","title":"NumericAccumulator Class","text":"<pre><code>class NumericAccumulator:\n    def __init__(self, name: str, config: NumericConfig):\n        self.name = name\n        self.count = 0\n        self.missing = 0\n        self.zeros = 0\n        self.negatives = 0\n        self.inf = 0\n\n        # Streaming moments\n        self._moments = StreamingMoments()\n\n        # Reservoir sample for quantiles\n        self._sample = ReservoirSampler(config.sample_size)\n\n        # KMV for distinct count\n        self._uniques = KMV(config.uniques_sketch_size)\n\n        # Min/max tracking\n        self._extremes = ExtremeTracker()\n\n    def update(self, values: np.ndarray):\n        \"\"\"Update with chunk of values\"\"\"\n        # Filter out missing/NaN/Inf\n        # Update moments\n        # Update sample\n        # Update KMV\n        # Track extremes\n        pass\n\n    def finalize(self) -&gt; NumericSummary:\n        \"\"\"Compute final statistics\"\"\"\n        # Compute mean, variance, skewness, kurtosis from moments\n        # Compute quantiles from sample\n        # Estimate distinct count from KMV\n        # Build histogram\n        # Detect outliers\n        return NumericSummary(...)\n</code></pre>"},{"location":"stats/numeric/#validation","title":"Validation","text":"<p>PySuricata validates numeric algorithms against reference implementations:</p> <ul> <li>NumPy/SciPy: Cross-check mean, variance, skewness, kurtosis on small datasets</li> <li>Property-based tests: Invariants under concatenation (merge = single pass)</li> <li>Scaling laws: \\(\\text{Var}(aX) = a^2 \\text{Var}(X)\\)</li> <li>Translation laws: \\(\\text{Mean}(X+c) = \\text{Mean}(X) + c\\)</li> <li>Numerical stability: Test with extreme values, large cancellations</li> </ul>"},{"location":"stats/numeric/#examples","title":"Examples","text":""},{"location":"stats/numeric/#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom pysuricata import profile\n\ndf = pd.DataFrame({\"amount\": [10, 20, 30, None, 50]})\nreport = profile(df)\nreport.save_html(\"report.html\")\n</code></pre>"},{"location":"stats/numeric/#streaming-large-dataset","title":"Streaming Large Dataset","text":"<pre><code>from pysuricata import profile, ReportConfig\n\ndef read_chunks():\n    for i in range(100):\n        yield pd.read_parquet(f\"data/part-{i}.parquet\")\n\nconfig = ReportConfig()\nconfig.compute.numeric_sample_size = 50_000\nconfig.compute.random_seed = 42\n\nreport = profile(read_chunks(), config=config)\n</code></pre>"},{"location":"stats/numeric/#access-statistics-programmatically","title":"Access Statistics Programmatically","text":"<pre><code>from pysuricata import summarize\n\nstats = summarize(df)\namount_stats = stats[\"columns\"][\"amount\"]\n\nprint(f\"Mean: {amount_stats['mean']}\")\nprint(f\"Std: {amount_stats['std']}\")\nprint(f\"Skewness: {amount_stats['skewness']}\")\n</code></pre>"},{"location":"stats/numeric/#references","title":"References","text":"<ol> <li> <p>Welford, B.P. (1962), \"Note on a Method for Calculating Corrected Sums of Squares and Products\", Technometrics, 4(3): 419\u2013420.</p> </li> <li> <p>P\u00e9bay, P. (2008), \"Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments\", Sandia Report SAND2008-6212. PDF</p> </li> <li> <p>Karnin, Z., Lang, K., Liberty, E. (2016), \"Optimal Quantile Approximation in Streams\", IEEE FOCS. arXiv:1603.05346</p> </li> <li> <p>Dunning, T., Ertl, O. (2019), \"Computing Extremely Accurate Quantiles Using t-Digests\", arXiv:1902.04023</p> </li> <li> <p>Tukey, J.W. (1977), Exploratory Data Analysis, Addison-Wesley.</p> </li> <li> <p>Freedman, D., Diaconis, P. (1981), \"On the histogram as a density estimator\", Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete, 57: 453\u2013476.</p> </li> <li> <p>Wikipedia: Algorithms for calculating variance - Link</p> </li> <li> <p>Wikipedia: Skewness - Link</p> </li> <li> <p>Wikipedia: Kurtosis - Link</p> </li> <li> <p>Wikipedia: Median absolute deviation - Link</p> </li> </ol>"},{"location":"stats/numeric/#see-also","title":"See Also","text":"<ul> <li>Categorical Analysis - String/categorical variables</li> <li>DateTime Analysis - Temporal variables</li> <li>Streaming Algorithms - Welford/P\u00e9bay deep dive</li> <li>Sketch Algorithms - KMV, HyperLogLog, KLL</li> <li>Configuration Guide - All parameters</li> </ul>"},{"location":"stats/overview/","title":"Statistical Methods Overview","text":"<p>PySuricata analyzes four variable types with specialized algorithms for each.</p>"},{"location":"stats/overview/#analysis-by-variable-type","title":"Analysis by Variable Type","text":""},{"location":"stats/overview/#numeric-variables","title":"Numeric Variables","text":"<p>Exact statistics using Welford/P\u00e9bay streaming algorithms:</p> <ul> <li>Mean, variance, standard deviation</li> <li>Skewness, kurtosis</li> <li>Min, max, range</li> </ul> <p>Approximate statistics using probabilistic data structures:</p> <ul> <li>Quantiles (reservoir sampling)</li> <li>Distinct count (KMV sketch)</li> <li>Histograms (adaptive binning)</li> </ul> <p>Key formulas:</p> \\[ \\bar{x} = \\frac{1}{n}\\sum x_i, \\quad s^2 = \\frac{1}{n-1}\\sum (x_i - \\bar{x})^2 \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#categorical-variables","title":"Categorical Variables","text":"<p>Analysis includes:</p> <ul> <li>Top-k values (Misra-Gries algorithm)</li> <li>Distinct count (KMV sketch)</li> <li>Entropy and Gini impurity</li> <li>String statistics</li> </ul> <p>Key formulas:</p> \\[ H(X) = -\\sum p(x) \\log_2 p(x), \\quad \\text{Gini}(X) = 1 - \\sum p(x)^2 \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#datetime-variables","title":"DateTime Variables","text":"<p>Temporal analysis:</p> <ul> <li>Hour, day-of-week, month distributions</li> <li>Monotonicity detection</li> <li>Time span and sampling rate</li> </ul> <p>Key formulas:</p> \\[ M = \\frac{n_{\\uparrow}}{n - 1}, \\quad r = \\frac{n}{\\Delta t} \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#boolean-variables","title":"Boolean Variables","text":"<p>Binary analysis:</p> <ul> <li>True/False proportions</li> <li>Entropy and balance metrics</li> <li>Imbalance detection</li> </ul> <p>Key formulas:</p> \\[ H = -p \\log_2(p) - (1-p) \\log_2(1-p) \\] <p>\u2192 Full Documentation</p>"},{"location":"stats/overview/#advanced-analytics","title":"Advanced Analytics","text":""},{"location":"stats/overview/#correlations","title":"Correlations","text":"<p>Streaming Pearson correlation between numeric columns, using pairwise co-moment tracking. Correlations above a configurable threshold are reported.</p> \\[ r_{xy} = \\frac{\\text{Cov}(X, Y)}{s_X \\cdot s_Y} \\]"},{"location":"stats/overview/#missing-values","title":"Missing Values","text":"<p>Per-column and dataset-wide missing value analysis:</p> <ul> <li>Missing count and percentage per column</li> <li>Top missing columns visualization</li> <li>Missing pattern detection</li> </ul>"},{"location":"stats/overview/#algorithms","title":"Algorithms","text":"<p>All statistics use single-pass streaming algorithms with bounded memory:</p> Algorithm Used for Space Welford/P\u00e9bay Mean, variance, skewness, kurtosis O(1) Reservoir sampling Quantiles, histograms O(s) KMV sketch Distinct count O(k) Misra-Gries Top-k frequent values O(k)"},{"location":"stats/overview/#guarantees","title":"Guarantees","text":"<ul> <li>Exact: moments (mean, variance, skewness, kurtosis), min/max, counts</li> <li>Approximate: quantiles (within \u00b11 percentile), distinct count (~2.2% error with default k=2048)</li> <li>Deterministic: set <code>random_seed</code> for reproducible reservoir sampling</li> </ul>"},{"location":"stats/overview/#see-also","title":"See Also","text":"<ul> <li>Streaming Algorithms \u2014 Algorithm details</li> <li>Sketch Algorithms \u2014 Probabilistic structures</li> </ul>"}]}